[
  {
    "objectID": "projects/predictive_maintenance.html",
    "href": "projects/predictive_maintenance.html",
    "title": "Predictive Maintenance for NTORQ",
    "section": "",
    "text": "Tools : python\nMachine learning : Regression\nRole : Lead Data Scientist\nStatus : In progress"
  },
  {
    "objectID": "london.html",
    "href": "london.html",
    "title": "Namaste London üôè",
    "section": "",
    "text": "My short stay at London | 2019-2020"
  },
  {
    "objectID": "london.html#ucl-x-deepmind-lecture-series",
    "href": "london.html#ucl-x-deepmind-lecture-series",
    "title": "Namaste London üôè",
    "section": "UCL x DeepMind Lecture Series",
    "text": "UCL x DeepMind Lecture Series\nAttended amazing lecture series on deep learning by Deep Mind in early 2020 at UCL London."
  },
  {
    "objectID": "london.html#machine-learning-at-imperial-college-london",
    "href": "london.html#machine-learning-at-imperial-college-london",
    "title": "Namaste London üôè",
    "section": "Machine Learning at Imperial College London",
    "text": "Machine Learning at Imperial College London\nAttended few ml and nlp classes conducted by AI Core at Imperial College."
  },
  {
    "objectID": "london.html#r-conference-by-max-kuhn",
    "href": "london.html#r-conference-by-max-kuhn",
    "title": "Namaste London üôè",
    "section": "R-Conference by Max Kuhn",
    "text": "R-Conference by Max Kuhn\nGot the chance to attend live conference by Max Kuhn the developer famous R packages like Caret & tidymodels."
  },
  {
    "objectID": "london.html#somewhere-in-london",
    "href": "london.html#somewhere-in-london",
    "title": "Namaste London üôè",
    "section": "Somewhere in London",
    "text": "Somewhere in London"
  },
  {
    "objectID": "london.html#london-eye",
    "href": "london.html#london-eye",
    "title": "Namaste London üôè",
    "section": "London Eye",
    "text": "London Eye"
  },
  {
    "objectID": "london.html#london-bridge",
    "href": "london.html#london-bridge",
    "title": "Namaste London üôè",
    "section": "London Bridge",
    "text": "London Bridge\nA Mandatory London Bridge Picture"
  },
  {
    "objectID": "london.html#cricket-at-canary-wharf",
    "href": "london.html#cricket-at-canary-wharf",
    "title": "Namaste London üôè",
    "section": "Cricket at Canary Wharf",
    "text": "Cricket at Canary Wharf\nWatching cricket here during office breaks was fun."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects & Clients",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vidyasagar Bhargava",
    "section": "",
    "text": "I‚Äôm a Lead Data Scientist at TVS Motor, working for connected mobility solutions department. My interests include Machine Learning, Computer Vision, and Generative AI. I have 10+ years of experience in the field of Machine learning and Data Science where I worked with various clients in developing prescriptive as well as predictive solutions to address core business problems.\n\n\n\n\n\n\nRoot Cause Analysis for Low Range Trips in EVs\n\nPredictive Maintenance of Brakes in two wheelers\n\nRange Prediction and Analysis for EVs\nChurn Prediction in Telecom Industry\nPrice optimization Engine for Scientific Pricing of Fashion Products\nNPS Improvement using explainable AI\nRetail store segmentation\nAnomaly Detection for machine parts failure\n\n\n\n\n\nB.Tech in Computer Science and Engineering\nXII in Computer Science\n\n\n\n\n\nNeural Networks and Deep Learning\nImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization\nBig Data Foundation\nIntroduction to Python\n\n\n\n\n\n\nThinking Hat at Wipro Limited | 2018\nBull‚Äôs Eye Award at Snapdeal | 2015\n\n\n\n\n\nIntroduction to Deep learning at Wipro Technologies on June 2017. [PPT]"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Work Experience & Major Projects\n\nTVS Motor :- Lead Data Scientist (2021 - Present)\n\nPredictive Maintenace\nRange Prediction for Electric Vehicle\n\nWipro Technologies :- Specialist Data Science (2017 - 2021)\n\nChurn Prediction for Telecom Company\nNPS Improvement for US based client\nBenchmarking of trained sentence embedding models\nScaling Machine Learning models using Spark\nDashboard for Time Series forecasting using Dash & Plotly\n\nCognizant Techonology Solution :- Data Scientist (2016 - 2017)\n\nRetail store segmentation\nAnomaly Detection for machine parts failure\n\nSnapdeal :- Analyst Advanced Analytics (2014 - 2016)\n\nPrice optimization Engine\n\nPrice Elasticity Models\n\nLadyblush E-Commerce Pvt. Ltd (Start up) :- Engineer (2014 - 2014)\n\nE-commerce AI Chatbot\nData Analytics and Dashboarding\n\nShopclues :- Analyst (2013 - 2014)\n\nData Analytics and Dashboarding\n\n\n\nEducation\n\nB.Tech in Computer Science and Engineering\n\n\n\nCourses\n\nMachine Learning Course by Andrew NG\nDeep learning Course by Andrew NG\nBig Data Foundation by IBM\nIntroduction to Python by Kaggle\n\n\n\nTalks & Webinars\n\nIntroduction to Deep learning at Wipro Technologies on June 2017. [PPT]"
  },
  {
    "objectID": "myblog/posts/Fbeta-Measure/index.html",
    "href": "myblog/posts/Fbeta-Measure/index.html",
    "title": "Fbeta-Measure",
    "section": "",
    "text": "The F-measure or F score, also called as F1 score is calculated as the harmonic mean of precision and recall, giving each the same weighting. It allows a model to be evaluated taking both the precision and recall into account using a single score, which is helpful when describing the performance of the model and in comparing models.\n\n\n\\[\nF_{1}=2.\\frac{{precision} \\times {recall}}{{precision} + {recall}}\n\\]\nThe Fbeta-measure is a generalization of the F-measure that adds a configuration parameter called beta. A default beta value is 1.0, which is the same as the F-measure. A smaller beta value, such as 0.5, gives more weight to precision and less to recall, whereas a larger beta value, such as 2.0, gives less weight to precision and more weight to recall in the calculation of the score.\n\n\n\\[\nF_{{\\beta}} = \\frac{(1 + {\\beta}^2). (precision.recall)}{({\\beta}^2.precision+recall)}\n\\]\nSummary\n\nPrecision and recall provide two ways to summarize the errors made for the positive class in a binary classification problem.\n\nF-measure provides a single score that summarizes the precision and recall.\n\nFbeta-measure provides a configurable version of the F-measure to give more or less attention to the precision and recall measure when calculating a single score.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "myblog/posts/Successful-delivering-of-machine-learning-models/index.html",
    "href": "myblog/posts/Successful-delivering-of-machine-learning-models/index.html",
    "title": "Successful delivering of Machine Learning Projects",
    "section": "",
    "text": "1. Easy access to required data and a comprehensive data strategy\nThere is saying in computer science world ‚Äúgarbage in, garbage out‚Äù which means nonsense input data produces nonsense output. Therefore your machine learning model is only as good as the data it‚Äôs trained on.If there is problem with data, machine learning scientists will end up spending their time in doing data cleanup and management.So we need a strong data strategy to make efficient use of ML scientist‚Äôs time and talent.\nWhat makes a strong data strategy ?\n\nData should be viewed as organizational asset rather than property of individual department that created or collected that data.\nData should be available easily, securely and in compliance with legal and regulatory requirements.\nData is put to work through analytics and machine learning to make better decisions, create efficiencies and drive new innovations.\n\nData related questions to be asked before the start of ML project\n\nWhat data is available to me today?\nWhat data is not quite available, but with some effort could become available?\nWhat data I don‚Äôt have today, but I might have in next few months or year? And what steps can be taken to begin gathering that data?\nIs there any potential bias in data or data sources?\n\n\n\n2. Selecting machine learning use cases and setting success metrics\nWe should aim to use machine learning where it is actually needed and not where it might be interesting. Some times simple analytics or rules get you 10-40% of business impact.Things to keep in mind include data readiness, business impact and machine learning applicability.\n\nA high impact use case without data or machine learing applicability ‚ùå\nA use case with lots of data and high machine learning applicability but low business impact ‚ùå\n\nBefore working on a project the team needs estimate its potential impact as well (Opportunity Sizing). So once we define business problem which can be solved with machine learning and done with opportunity sizing the next step is to outlining clear metrics to measure success.\nThe data science projects needs to have clear goal which is typically a target value for a clearly defined metric. In real world data science projects there are not just one but multiple metrics that model will evaluated against. Some of these evaluation metric won‚Äôt even be related to how your prediction performs against the ground truth. Other such metrics are like :\n\nOverall memory usage\nlatency of the prediction process\ncomplexity of predictive model\n\nReal world problems are indeed dominated by business and tech infrastructure concerns.\n\n\n3. Technical experts and domain experts should work together\nWe need to make sure that domain experts and technical experts or stakeholders work side by side. If relevant stakeholders are the part of entire process, everyone is most likely to accept, adopt and implement the solution. If a data scientist is working in silos then its very much unlikely that their models get implemented.\n\n\n4. Exploratory data analysis\nBefore building the model,we need to interrogate the data to see if there is any predictive power in the feature set. Read more about EDA here.\n\n\n5. A quick MVP\nIts good practice to build a minimum viable product which is build quickly and cheaply to validate the hypothesis before we commit extensive time and resource.\n\n\n6. Experiment Metrics\nWe should look for more than one metric to look when an experiment concludes.\n\n\n7. Regular Check-ins\nRather than meeting at the start and end of a model build, it is better to check-in frequently (e.g.¬†once or twice a week) to discuss latest findings and align on if any course corrections are necessary.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "myblog/posts/Nearest-Neighbour-Classifier/index.html#strength",
    "href": "myblog/posts/Nearest-Neighbour-Classifier/index.html#strength",
    "title": "Nearest Neighbour Classifier",
    "section": "Strength",
    "text": "Strength\n\nSimple and effective\nMakes no assumption about data\nFast Training Process"
  },
  {
    "objectID": "myblog/posts/Nearest-Neighbour-Classifier/index.html#weakness",
    "href": "myblog/posts/Nearest-Neighbour-Classifier/index.html#weakness",
    "title": "Nearest Neighbour Classifier",
    "section": "Weakness",
    "text": "Weakness\n\nDoesn‚Äôt produce model, limiting the ability to understand how features are related to class\nRequires selection of k\nSlow classification phase\nCategorical features and missing data require pre processing\n\nThe k-Nearest Neighbor algorithm uses nearest k number of neighbors for labeling of an unlabeled example. The unlabeled test example is assigned the class of majority of the k-Nearest Neighbors.\nFor finding the distance k-NN algorithm uses Euclidean distance."
  },
  {
    "objectID": "myblog/posts/Nearest-Neighbour-Classifier/index.html#defining-the-dataset",
    "href": "myblog/posts/Nearest-Neighbour-Classifier/index.html#defining-the-dataset",
    "title": "Nearest Neighbour Classifier",
    "section": "Defining the dataset",
    "text": "Defining the dataset\n\n# First Feature\nweather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\n'Rainy','Sunny','Overcast','Overcast','Rainy']\n# Second Feature\ntemp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\n\n# Label or target varible\nplay=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']\n\nWe have two features weather and temperature and one label play."
  },
  {
    "objectID": "myblog/posts/Nearest-Neighbour-Classifier/index.html#encoding-data-columns",
    "href": "myblog/posts/Nearest-Neighbour-Classifier/index.html#encoding-data-columns",
    "title": "Nearest Neighbour Classifier",
    "section": "Encoding data columns",
    "text": "Encoding data columns\n\nfrom sklearn import preprocessing\n\n#creating labelEncoder\nle = preprocessing.LabelEncoder()\n\n# Converting string labels into numbers.\nweather_encoded=le.fit_transform(weather)\n\nSimilarly, you can encode temperature and label into numeric columns.\n\n# converting string labels into numbers\ntemp_encoded=le.fit_transform(temp)\nlabel=le.fit_transform(play)"
  },
  {
    "objectID": "myblog/posts/Nearest-Neighbour-Classifier/index.html#combining-features",
    "href": "myblog/posts/Nearest-Neighbour-Classifier/index.html#combining-features",
    "title": "Nearest Neighbour Classifier",
    "section": "Combining Features",
    "text": "Combining Features\n\n#combinig weather and temp into single listof tuples\nfeatures=list(zip(weather_encoded,temp_encoded))"
  },
  {
    "objectID": "myblog/posts/Nearest-Neighbour-Classifier/index.html#generating-models",
    "href": "myblog/posts/Nearest-Neighbour-Classifier/index.html#generating-models",
    "title": "Nearest Neighbour Classifier",
    "section": "Generating Models",
    "text": "Generating Models\n\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=3)\n\n# Train the model using the training sets\nmodel.fit(features,label)\n\nKNeighborsClassifier(n_neighbors=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=3)"
  },
  {
    "objectID": "myblog/posts/Nearest-Neighbour-Classifier/index.html#predict-output",
    "href": "myblog/posts/Nearest-Neighbour-Classifier/index.html#predict-output",
    "title": "Nearest Neighbour Classifier",
    "section": "Predict Output",
    "text": "Predict Output\n\npredicted= model.predict([[0,2]]) # 0:Overcast, 2:Mild\nprint(predicted)\n\n[1]"
  },
  {
    "objectID": "myblog/posts/best-datascience-books/index.html",
    "href": "myblog/posts/best-datascience-books/index.html",
    "title": "The Best Data Science Books",
    "section": "",
    "text": "I have seen many aspiring data scientists who wants to start their career in the field of data science but they are confused about the path and resources (because there are so many). Every individual has their own way of learning any topic. Some prefer books, some youtube video tutorial for understanding and some wants to learn from university or online certificate courses platform like coursera, Edx udemy etc..\nSo I decided to write this post to those aspirants who are interested in learning data science through books. I have listed the best books for different categories."
  },
  {
    "objectID": "myblog/posts/best-datascience-books/index.html#python",
    "href": "myblog/posts/best-datascience-books/index.html#python",
    "title": "The Best Data Science Books",
    "section": "1. Python",
    "text": "1. Python\n\nFluent Python\n\nPython Data Science Handbook"
  },
  {
    "objectID": "myblog/posts/best-datascience-books/index.html#statistics-and-mathematics",
    "href": "myblog/posts/best-datascience-books/index.html#statistics-and-mathematics",
    "title": "The Best Data Science Books",
    "section": "2. Statistics and Mathematics",
    "text": "2. Statistics and Mathematics\n\nPractical Statistics for Data Scientists\n\nMathematics for Machine Learning"
  },
  {
    "objectID": "myblog/posts/best-datascience-books/index.html#machine-learning",
    "href": "myblog/posts/best-datascience-books/index.html#machine-learning",
    "title": "The Best Data Science Books",
    "section": "3. Machine Learning",
    "text": "3. Machine Learning\n\nMachine Learning with PyTorch and Scikit-Learn"
  },
  {
    "objectID": "myblog/posts/best-datascience-books/index.html#competitive-data-science",
    "href": "myblog/posts/best-datascience-books/index.html#competitive-data-science",
    "title": "The Best Data Science Books",
    "section": "4. Competitive Data Science",
    "text": "4. Competitive Data Science\n\nThe Kaggle Book"
  },
  {
    "objectID": "myblog/posts/best-datascience-books/index.html#productionize-models",
    "href": "myblog/posts/best-datascience-books/index.html#productionize-models",
    "title": "The Best Data Science Books",
    "section": "5. Productionize models",
    "text": "5. Productionize models\n\nDesigning Machine Learning Systems"
  },
  {
    "objectID": "myblog/posts/gradient-descent/index.html",
    "href": "myblog/posts/gradient-descent/index.html",
    "title": "Linear regression using gradient descent",
    "section": "",
    "text": "A simple linear regression equation with one feature is defined as :\n\\[y = b + w * x + \\epsilon\\]\nHere w is coefficient and b is intercept term and \\(\\epsilon\\) is the noise."
  },
  {
    "objectID": "myblog/posts/top-languages/index.html",
    "href": "myblog/posts/top-languages/index.html",
    "title": "Most Popular programming languages 2004-2021",
    "section": "",
    "text": "chart\n\n\n\n\n\n\n\ndata = FileAttachment(\"pypl.csv\").csv({typed: true})\n\nformatNumber = d3.format(\".1%\")\n\n\nformatDate = d3.utcFormat(\"%b %Y\")\n\ntickFormat = \"%\"\n\nk = 2\n\nimport {chart, viewof replay, d3} with {k, data, formatNumber, formatDate, tickFormat} from \"@d3/bar-chart-race\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "myblog/posts/hypothesis-testing/index.html",
    "href": "myblog/posts/hypothesis-testing/index.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "During hypothesis testing we confirm whether results we got is by chance ? If yes then experiment won‚Äôt be repeatable and so has little use.\nTwo ways of doing hypothesis testing are :-\nLet‚Äôs start with business case problem.\nQ :- Suppose that a PM claims that users on average spend about Rs. 50 per month on Amazon. However, you doubt this claim, and you believe that the average should be higher. So you sample 100 users and learn that the sample mean is Rs. 85. Would you reject the PM‚Äôs claim? Assume population standard deviation is 20."
  },
  {
    "objectID": "myblog/posts/hypothesis-testing/index.html#p-value-approach",
    "href": "myblog/posts/hypothesis-testing/index.html#p-value-approach",
    "title": "Hypothesis Testing",
    "section": "P-Value approach",
    "text": "P-Value approach\n\nHypothesis\nSignificance level\nTest Statistics\n\nProbability of observing test statistics\nStatistical Decision\n\n1. Hypothesis\nHo : the average spend per user is Rs. 50\nHa : the average spend per user is greater than Rs. 50\n2. Significance level\n\\(\\alpha = 0.05\\)\n3. Test Statistics\n\\[\nZ-statistics  = \\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}\n\\]\n\\[\n= \\frac{85-50}{20/\\sqrt{100}} =\\frac{35}{2} = 17.5\n\\]\n4. Probability of observing test statistics\nLooking into standard normal distribution table.\n\\(P-value &lt; 0.0001 &lt; \\alpha = 0.05\\)\n5. Statistical Decision\nAt alpha = 0.05 there is statistical significance to reject PM‚Äôs claim and conclude that the average spend per user is greater than Rs. 50.\nBusiness Case problem\nQ :- A Principal claims that the student in his school are above average intelligence. A random sample of 30 IQ scores have a mean of 112.5. Is there sufficient evidence to support the principal‚Äôs claim? The mean population IQ is 100 with standard deviation is 15."
  },
  {
    "objectID": "myblog/posts/hypothesis-testing/index.html#critical-value-approach",
    "href": "myblog/posts/hypothesis-testing/index.html#critical-value-approach",
    "title": "Hypothesis Testing",
    "section": "Critical Value Approach",
    "text": "Critical Value Approach\n\nHypothesis\nSignificance level\nTest Statistics\n\nCritical Value\nStatistical Decision\n\n\n\n\n\n\n\n\n\n\n1. Hypothesis\nHo : the average IQ score is 100\nHa : the average IQ score is greater than Rs. 100\n2. Significance level\n\\(\\alpha = 0.05\\)\n3. Test Statistics\n\\[\nZ-statistics  = \\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}}\n\\]\n\\[\n= \\frac{112.5-100}{15/\\sqrt{30}} = 4.56\n\\]\n4. Z Critical Value\nLooking into standard normal distribution table.\n\\(Z Critical-value = 1.645\\)\n5. Statistical Decision\nAt alpha = 0.05 test statistics is greater than Z Critical value hence we can reject null hypothesis."
  },
  {
    "objectID": "myblog/blog.html",
    "href": "myblog/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Accelerate computation on Mac using PyTorch and gpu support\n\n\n\n\n\nRunning a experiment using pytorch tensors on cpu vs leveraging gpu support on M1 mac and see how much gain we get in terms of speed.\n\n\n\n\n\nMay 1, 2024\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nPolars for Feature Engineering\n\n\n\n\n\nPolars is a high-performance DataFrame library, designed to provide fast and efficient data processing capabilities. Inspired by the reigning pandas library, Polars takes things to another level, offering a seamless experience for working with large datasets that might not fit into memory.\n\n\n\n\n\nJan 3, 2024\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Apple‚Äôs Machine learning Framework- MLX\n\n\n\n\n\nApple‚Äôs machine learning research team recently released a Machine Learning framework called MLX, a NumPy-like array framework designed for efficient and flexible machine learning on Apple silicon.\n\n\n\n\n\nDec 24, 2023\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing Activation Functions\n\n\n\n\n\nAn activation function decides whether a neuron should be activated or not which helps neural network to use important information while suppressing the irrelevant data points.\n\n\n\n\n\nJun 8, 2023\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Learning Theory\n\n\n\n\n\nA framework for analysing the inside of blackbox of machine learning algorithms.\n\n\n\n\n\nFeb 1, 2023\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nThe Best Data Science Books\n\n\n\n\n\nListing the best books available in the market right now for data science. \n\n\n\n\n\nJan 16, 2023\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Confidence Interval\n\n\n\n\n\nConfidence Intervals are useful tool for expressing the uncertainity around an estimate.\n\n\n\n\n\nDec 5, 2022\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nSuccessful delivering of Machine Learning Projects\n\n\n\n\n\nPrinciples and process for democratizing ML projects\n\n\n\n\n\nSep 7, 2022\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\n\n\n\n\nA hypothesis testing is a way to test an assumption about a population parameter.\n\n\n\n\n\nApr 4, 2022\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nMost Popular programming languages 2004-2021\n\n\n\n\n\nUsing animation lets see how different programmming language rise in last couple of decades.\n\n\n\n\n\nFeb 10, 2022\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression using gradient descent\n\n\n\n\n\nIn this post we will implement gradient descent algorithm from scratch using numpy for linear regression.\n\n\n\n\n\nDec 10, 2021\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nFbeta-Measure\n\n\n\n\n\nA generalization of the F-measure that adds a configuration parameter called beta\n\n\n\n\n\nMar 12, 2021\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nNearest Neighbour Classifier\n\n\n\n\n\nNearest neighbor classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples.\n\n\n\n\n\nMar 6, 2020\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "myblog/posts/gradient-descent/index.html#gradient-descent-algorithm",
    "href": "myblog/posts/gradient-descent/index.html#gradient-descent-algorithm",
    "title": "Solving linear regression problem using gradient descent",
    "section": "Gradient Descent Algorithm",
    "text": "Gradient Descent Algorithm\nStep 1 - Compute the loss\nStep 2 - Compute the gradient\nStep 3 - Update the parameters\nStep 4 - Repeat Step 1 to 3\n\nStep 1 :- Compute the loss\nFor regression problem loss is given by mean squared error (MSE).\n\\[ MSE  = \\frac{1}{N} \\sum\\limits_{i=1}^{N} (y-\\hat{y_i})^2 \\]\n\\[ MSE  = \\frac{1}{N} \\sum\\limits_{i=1}^{N} (y-b-wx_i)^2 \\]\n\n\nStep 2 :- Compute the gradient\nGradient is just partial derivative. It also tell the direction of steepest ascent.Now we have two parameters so we will calculate two partial derivatives.\n\n\n\n\n\n\nNote\n\n\n\nA derivative tells us how much a given quantity changes when we change slightly some other quantity.\n\n\nIn our case we are interested how much our MSE loss change when we vary one of our two parameters.\n\\[ \\frac{\\partial MSE}{\\partial w} = \\frac{\\partial MSE}{\\partial \\hat{y}}.\\frac{\\partial \\hat{y}}{\\partial w} = \\frac{1}{N} \\sum\\limits_{i=1}^{N} 2(y-b-wx_i).(-x_i) = -2\\frac{1}{N} \\sum\\limits_{i=1}^{N}(x_i)(y-\\hat{y_i}) \\]\n\\[ \\frac{\\partial MSE}{\\partial b} = \\frac{\\partial MSE}{\\partial \\hat{y}}.\\frac{\\partial \\hat{y}}{\\partial b} = \\frac{1}{N} \\sum\\limits_{i=1}^{N} 2(y-b-wx_i).(-1) = -2\\frac{1}{N} \\sum\\limits_{i=1}^{N}(y-\\hat{y_i}) \\]\n\n\nStep 3 :- Update the Parameters\nNow we will update the parameters by using gradients to minimize the loss. But gradient tells the direction of steepest ascent so we will multiply by -1.\n\\[ w  = w - \\eta\\frac{\\partial MSE}{\\partial w} \\]\n\\[ b  = b - \\eta\\frac{\\partial MSE}{\\partial b} \\]\n\n\nStep 4:- Repeat\nNow we will use updated parameters and start with step 1 again. We will repeat this process for multiple epochs. This is also known as training the model.\n\nAn epoch is completed when all the data points has been used for calculating the loss."
  },
  {
    "objectID": "myblog/posts/gradient-descent/index.html#implementing-linear-regression-using-numpy",
    "href": "myblog/posts/gradient-descent/index.html#implementing-linear-regression-using-numpy",
    "title": "Solving linear regression problem using gradient descent",
    "section": "Implementing Linear Regression using Numpy",
    "text": "Implementing Linear Regression using Numpy\nNow its time to implement our linear regression model using gradient descent using numpy.\n\nGenerate Synthetic data\nUsing the above equation we will generate some synthetic data with w = 2 and b = 1 and some random noise.\n\nnp.random.seed(42)\nimport numpy as np\nx = np.random.randn(100,1)\ny = 1 + 2*x + np.random.randn(100,1)\n\nOur goal will be to accurately predict the value of w (i.e.¬†2) and b (i.e.¬†1).\n\n\nInitialize parameters (w and b) and hyperparameter (learning_rate)\nSo we will initialize the learnable parameters w and b with some random values and try to find right value by minimizing the loss function. The value of the hyperparameter i.e.¬†learning rate is fixed.\n\nw = np.random.randn(1)\nb = np.random.randn(1)\nlearning_rate = 0.001\n\n\n\nGradient Descent algorithm\n\nCompute the loss\n\nCompute the gradients\n\nUpdate the parameters\n\nRepeat\n\n\ndef gradient_descent(x,y,w,b,learning_rate):\n    dldw = 0.0 \n    dldb = 0.0\n    N = x.shape[0]\n    \n    for xi, yi in zip(x,y):\n        dldw += -2*xi*(yi-(w*xi+b))\n        dldb += -2*(yi-(w*xi+b))\n    #make an update to the parameters\n    w = w - learning_rate*(1/N)*dldw\n    b = b - learning_rate*(1/N)*dldb\n    \n    return w,b\n\n\n\nIterate gradient descent function and update parameters to minimize loss.\n\nfor epoch in range(300):\n    w, b = gradient_descent(x,y,w,b,learning_rate) \n    yhat =  w*x + b\n    loss =  np.divide(np.sum((y-yhat)**2, axis=0), x.shape[0])\n    print(f'{epoch} loss is {loss}, parameters w:{w}, b:{b}')\n\n0 loss is [3.31211004], parameters w:[0.03394664], b:[-0.00241723]\n1 loss is [3.2011369], parameters w:[0.06731399], b:[-0.00483577]\n2 loss is [3.09389844], parameters w:[0.100112], b:[-0.00725474]\n3 loss is [2.99026833], parameters w:[0.13235046], b:[-0.0096733]\n4 loss is [2.8901245], parameters w:[0.16403896], b:[-0.01209065]\n5 loss is [2.79334907], parameters w:[0.19518695], b:[-0.01450599]\n6 loss is [2.69982816], parameters w:[0.2258037], b:[-0.01691859]\n7 loss is [2.60945176], parameters w:[0.25589833], b:[-0.01932772]\n8 loss is [2.52211359], parameters w:[0.2854798], b:[-0.02173268]\n9 loss is [2.43771102], parameters w:[0.31455692], b:[-0.02413281]\n10 loss is [2.35614487], parameters w:[0.34313833], b:[-0.02652747]\n11 loss is [2.27731934], parameters w:[0.37123253], b:[-0.02891603]\n12 loss is [2.20114189], parameters w:[0.39884789], b:[-0.03129792]\n13 loss is [2.12752313], parameters w:[0.42599262], b:[-0.03367256]\n14 loss is [2.05637669], parameters w:[0.45267477], b:[-0.03603941]\n15 loss is [1.98761913], parameters w:[0.4789023], b:[-0.03839795]\n16 loss is [1.92116987], parameters w:[0.50468298], b:[-0.04074769]\n17 loss is [1.85695103], parameters w:[0.53002448], b:[-0.04308813]\n18 loss is [1.79488739], parameters w:[0.55493432], b:[-0.04541883]\n19 loss is [1.73490628], parameters w:[0.5794199], b:[-0.04773935]\n20 loss is [1.67693749], parameters w:[0.60348849], b:[-0.05004928]\n21 loss is [1.6209132], parameters w:[0.62714724], b:[-0.0523482]\n22 loss is [1.56676786], parameters w:[0.65040315], b:[-0.05463576]\n23 loss is [1.51443818], parameters w:[0.67326314], b:[-0.05691158]\n24 loss is [1.463863], parameters w:[0.69573398], b:[-0.05917532]\n25 loss is [1.41498321], parameters w:[0.71782234], b:[-0.06142665]\n26 loss is [1.36774173], parameters w:[0.73953477], b:[-0.06366526]\n27 loss is [1.32208339], parameters w:[0.76087769], b:[-0.06589085]\n28 loss is [1.27795492], parameters w:[0.78185744], b:[-0.06810315]\n29 loss is [1.23530481], parameters w:[0.80248024], b:[-0.07030189]\n30 loss is [1.19408333], parameters w:[0.82275218], b:[-0.07248682]\n31 loss is [1.1542424], parameters w:[0.84267927], b:[-0.0746577]\n32 loss is [1.11573559], parameters w:[0.86226742], b:[-0.07681431]\n33 loss is [1.07851804], parameters w:[0.88152242], b:[-0.07895644]\n34 loss is [1.04254639], parameters w:[0.90044997], b:[-0.08108389]\n35 loss is [1.00777875], parameters w:[0.91905568], b:[-0.08319647]\n36 loss is [0.97417466], parameters w:[0.93734503], b:[-0.08529402]\n37 loss is [0.94169501], parameters w:[0.95532345], b:[-0.08737636]\n38 loss is [0.91030202], parameters w:[0.97299626], b:[-0.08944336]\n39 loss is [0.87995918], parameters w:[0.99036866], b:[-0.09149486]\n40 loss is [0.85063121], parameters w:[1.00744581], b:[-0.09353074]\n41 loss is [0.82228404], parameters w:[1.02423274], b:[-0.09555088]\n42 loss is [0.79488472], parameters w:[1.04073441], b:[-0.09755518]\n43 loss is [0.76840144], parameters w:[1.05695571], b:[-0.09954352]\n44 loss is [0.74280345], parameters w:[1.07290142], b:[-0.10151582]\n45 loss is [0.71806103], parameters w:[1.08857624], b:[-0.103472]\n46 loss is [0.69414548], parameters w:[1.10398481], b:[-0.10541198]\n47 loss is [0.67102905], parameters w:[1.11913167], b:[-0.1073357]\n48 loss is [0.64868494], parameters w:[1.1340213], b:[-0.1092431]\n49 loss is [0.62708725], parameters w:[1.14865808], b:[-0.11113413]\n50 loss is [0.60621094], parameters w:[1.16304633], b:[-0.11300875]\n51 loss is [0.58603182], parameters w:[1.17719029], b:[-0.11486691]\n52 loss is [0.56652652], parameters w:[1.19109414], b:[-0.11670861]\n53 loss is [0.54767247], parameters w:[1.20476198], b:[-0.1185338]\n54 loss is [0.52944783], parameters w:[1.21819782], b:[-0.12034249]\n55 loss is [0.5118315], parameters w:[1.23140564], b:[-0.12213465]\n56 loss is [0.49480313], parameters w:[1.24438931], b:[-0.12391028]\n57 loss is [0.47834299], parameters w:[1.25715267], b:[-0.1256694]\n58 loss is [0.46243208], parameters w:[1.26969948], b:[-0.127412]\n59 loss is [0.44705198], parameters w:[1.28203342], b:[-0.1291381]\n60 loss is [0.43218494], parameters w:[1.29415812], b:[-0.13084771]\n61 loss is [0.41781377], parameters w:[1.30607717], b:[-0.13254087]\n62 loss is [0.40392188], parameters w:[1.31779405], b:[-0.1342176]\n63 loss is [0.39049322], parameters w:[1.32931223], b:[-0.13587793]\n64 loss is [0.3775123], parameters w:[1.34063508], b:[-0.13752191]\n65 loss is [0.36496413], parameters w:[1.35176593], b:[-0.13914956]\n66 loss is [0.35283424], parameters w:[1.36270807], b:[-0.14076094]\n67 loss is [0.34110864], parameters w:[1.3734647], b:[-0.1423561]\n68 loss is [0.3297738], parameters w:[1.38403899], b:[-0.14393509]\n69 loss is [0.31881666], parameters w:[1.39443404], b:[-0.14549796]\n70 loss is [0.3082246], parameters w:[1.4046529], b:[-0.14704478]\n71 loss is [0.2979854], parameters w:[1.41469859], b:[-0.14857561]\n72 loss is [0.28808727], parameters w:[1.42457404], b:[-0.15009052]\n73 loss is [0.27851882], parameters w:[1.43428216], b:[-0.15158957]\n74 loss is [0.26926903], parameters w:[1.4438258], b:[-0.15307285]\n75 loss is [0.26032724], parameters w:[1.45320775], b:[-0.15454041]\n76 loss is [0.25168318], parameters w:[1.46243078], b:[-0.15599235]\n77 loss is [0.2433269], parameters w:[1.47149758], b:[-0.15742874]\n78 loss is [0.23524878], parameters w:[1.48041082], b:[-0.15884966]\n79 loss is [0.22743954], parameters w:[1.48917311], b:[-0.1602552]\n80 loss is [0.2198902], parameters w:[1.49778701], b:[-0.16164544]\n81 loss is [0.21259208], parameters w:[1.50625506], b:[-0.16302048]\n82 loss is [0.2055368], parameters w:[1.51457974], b:[-0.16438041]\n83 loss is [0.19871625], parameters w:[1.52276348], b:[-0.16572531]\n84 loss is [0.1921226], parameters w:[1.53080869], b:[-0.16705528]\n85 loss is [0.18574828], parameters w:[1.53871771], b:[-0.16837042]\n86 loss is [0.17958597], parameters w:[1.54649288], b:[-0.16967083]\n87 loss is [0.17362859], parameters w:[1.55413645], b:[-0.1709566]\n88 loss is [0.16786932], parameters w:[1.56165068], b:[-0.17222784]\n89 loss is [0.16230154], parameters w:[1.56903775], b:[-0.17348464]\n90 loss is [0.15691888], parameters w:[1.57629984], b:[-0.17472711]\n91 loss is [0.15171514], parameters w:[1.58343906], b:[-0.17595535]\n92 loss is [0.14668439], parameters w:[1.59045751], b:[-0.17716947]\n93 loss is [0.14182083], parameters w:[1.59735724], b:[-0.17836957]\n94 loss is [0.13711891], parameters w:[1.60414026], b:[-0.17955576]\n95 loss is [0.13257324], parameters w:[1.61080857], b:[-0.18072815]\n96 loss is [0.1281786], parameters w:[1.6173641], b:[-0.18188684]\n97 loss is [0.12392998], parameters w:[1.62380878], b:[-0.18303195]\n98 loss is [0.11982249], parameters w:[1.63014448], b:[-0.18416359]\n99 loss is [0.11585145], parameters w:[1.63637307], b:[-0.18528185]\n100 loss is [0.1120123], parameters w:[1.64249635], b:[-0.18638687]\n101 loss is [0.10830065], parameters w:[1.64851612], b:[-0.18747873]\n102 loss is [0.10471227], parameters w:[1.65443414], b:[-0.18855757]\n103 loss is [0.10124303], parameters w:[1.66025213], b:[-0.18962348]\n104 loss is [0.09788899], parameters w:[1.66597179], b:[-0.19067659]\n105 loss is [0.09464629], parameters w:[1.6715948], b:[-0.191717]\n106 loss is [0.09151125], parameters w:[1.67712278], b:[-0.19274483]\n107 loss is [0.08848026], parameters w:[1.68255737], b:[-0.19376018]\n108 loss is [0.08554988], parameters w:[1.68790013], b:[-0.19476318]\n109 loss is [0.08271675], parameters w:[1.69315263], b:[-0.19575393]\n110 loss is [0.07997763], parameters w:[1.6983164], b:[-0.19673255]\n111 loss is [0.07732941], parameters w:[1.70339295], b:[-0.19769914]\n112 loss is [0.07476905], parameters w:[1.70838375], b:[-0.19865384]\n113 loss is [0.07229363], parameters w:[1.71329027], b:[-0.19959673]\n114 loss is [0.06990033], parameters w:[1.71811392], b:[-0.20052795]\n115 loss is [0.06758642], parameters w:[1.72285612], b:[-0.2014476]\n116 loss is [0.06534926], parameters w:[1.72751825], b:[-0.20235579]\n117 loss is [0.06318629], parameters w:[1.73210167], b:[-0.20325263]\n118 loss is [0.06109506], parameters w:[1.7366077], b:[-0.20413825]\n119 loss is [0.05907317], parameters w:[1.74103767], b:[-0.20501274]\n120 loss is [0.05711831], parameters w:[1.74539286], b:[-0.20587622]\n121 loss is [0.05522828], parameters w:[1.74967455], b:[-0.2067288]\n122 loss is [0.0534009], parameters w:[1.75388396], b:[-0.20757059]\n123 loss is [0.05163409], parameters w:[1.75802234], b:[-0.20840171]\n124 loss is [0.04992585], parameters w:[1.76209089], b:[-0.20922225]\n125 loss is [0.04827423], parameters w:[1.76609078], b:[-0.21003233]\n126 loss is [0.04667735], parameters w:[1.77002318], b:[-0.21083207]\n127 loss is [0.04513339], parameters w:[1.77388924], b:[-0.21162155]\n128 loss is [0.04364058], parameters w:[1.77769008], b:[-0.21240091]\n129 loss is [0.04219724], parameters w:[1.78142681], b:[-0.21317024]\n130 loss is [0.04080172], parameters w:[1.7851005], b:[-0.21392964]\n131 loss is [0.03945244], parameters w:[1.78871223], b:[-0.21467923]\n132 loss is [0.03814785], parameters w:[1.79226305], b:[-0.21541911]\n133 loss is [0.03688647], parameters w:[1.79575399], b:[-0.21614939]\n134 loss is [0.03566688], parameters w:[1.79918607], b:[-0.21687017]\n135 loss is [0.03448768], parameters w:[1.80256027], b:[-0.21758155]\n136 loss is [0.03334753], parameters w:[1.80587759], b:[-0.21828364]\n137 loss is [0.03224513], parameters w:[1.80913897], b:[-0.21897654]\n138 loss is [0.03117924], parameters w:[1.81234538], b:[-0.21966035]\n139 loss is [0.03014864], parameters w:[1.81549773], b:[-0.22033518]\n140 loss is [0.02915216], parameters w:[1.81859696], b:[-0.22100112]\n141 loss is [0.02818867], parameters w:[1.82164394], b:[-0.22165827]\n142 loss is [0.02725708], parameters w:[1.82463958], b:[-0.22230674]\n143 loss is [0.02635632], parameters w:[1.82758473], b:[-0.22294662]\n144 loss is [0.02548538], parameters w:[1.83048025], b:[-0.22357801]\n145 loss is [0.02464326], parameters w:[1.83332699], b:[-0.22420101]\n146 loss is [0.02382901], parameters w:[1.83612576], b:[-0.22481571]\n147 loss is [0.02304171], parameters w:[1.83887738], b:[-0.22542221]\n148 loss is [0.02228046], parameters w:[1.84158265], b:[-0.2260206]\n149 loss is [0.0215444], parameters w:[1.84424234], b:[-0.22661099]\n150 loss is [0.02083269], parameters w:[1.84685723], b:[-0.22719345]\n151 loss is [0.02014453], parameters w:[1.84942809], b:[-0.2277681]\n152 loss is [0.01947913], parameters w:[1.85195564], b:[-0.22833501]\n153 loss is [0.01883575], parameters w:[1.85444063], b:[-0.22889427]\n154 loss is [0.01821365], parameters w:[1.85688377], b:[-0.22944599]\n155 loss is [0.01761212], parameters w:[1.85928578], b:[-0.22999025]\n156 loss is [0.01703049], parameters w:[1.86164734], b:[-0.23052713]\n157 loss is [0.01646809], parameters w:[1.86396914], b:[-0.23105673]\n158 loss is [0.0159243], parameters w:[1.86625186], b:[-0.23157914]\n159 loss is [0.01539848], parameters w:[1.86849614], b:[-0.23209443]\n160 loss is [0.01489005], parameters w:[1.87070265], b:[-0.23260271]\n161 loss is [0.01439843], parameters w:[1.87287202], b:[-0.23310404]\n162 loss is [0.01392307], parameters w:[1.87500488], b:[-0.23359852]\n163 loss is [0.01346342], parameters w:[1.87710184], b:[-0.23408623]\n164 loss is [0.01301897], parameters w:[1.87916352], b:[-0.23456725]\n165 loss is [0.01258921], parameters w:[1.8811905], b:[-0.23504167]\n166 loss is [0.01217365], parameters w:[1.88318337], b:[-0.23550957]\n167 loss is [0.01177183], parameters w:[1.88514272], b:[-0.23597102]\n168 loss is [0.01138329], parameters w:[1.8870691], b:[-0.23642611]\n169 loss is [0.01100759], parameters w:[1.88896307], b:[-0.23687491]\n170 loss is [0.01064431], parameters w:[1.89082518], b:[-0.23731751]\n171 loss is [0.01029303], parameters w:[1.89265597], b:[-0.23775398]\n172 loss is [0.00995336], parameters w:[1.89445596], b:[-0.2381844]\n173 loss is [0.00962491], parameters w:[1.89622568], b:[-0.23860884]\n174 loss is [0.00930732], parameters w:[1.89796564], b:[-0.23902738]\n175 loss is [0.00900021], parameters w:[1.89967634], b:[-0.2394401]\n176 loss is [0.00870326], parameters w:[1.90135827], b:[-0.23984706]\n177 loss is [0.00841611], parameters w:[1.90301192], b:[-0.24024835]\n178 loss is [0.00813845], parameters w:[1.90463777], b:[-0.24064403]\n179 loss is [0.00786996], parameters w:[1.90623628], b:[-0.24103417]\n180 loss is [0.00761034], parameters w:[1.90780791], b:[-0.24141885]\n181 loss is [0.00735929], parameters w:[1.90935313], b:[-0.24179813]\n182 loss is [0.00711653], parameters w:[1.91087237], b:[-0.24217209]\n183 loss is [0.00688179], parameters w:[1.91236608], b:[-0.24254079]\n184 loss is [0.00665481], parameters w:[1.91383468], b:[-0.2429043]\n185 loss is [0.00643531], parameters w:[1.9152786], b:[-0.24326269]\n186 loss is [0.00622307], parameters w:[1.91669825], b:[-0.24361602]\n187 loss is [0.00601783], parameters w:[1.91809404], b:[-0.24396436]\n188 loss is [0.00581937], parameters w:[1.91946638], b:[-0.24430778]\n189 loss is [0.00562747], parameters w:[1.92081566], b:[-0.24464633]\n190 loss is [0.0054419], parameters w:[1.92214228], b:[-0.24498009]\n191 loss is [0.00526245], parameters w:[1.9234466], b:[-0.24530912]\n192 loss is [0.00508893], parameters w:[1.92472901], b:[-0.24563347]\n193 loss is [0.00492113], parameters w:[1.92598988], b:[-0.24595321]\n194 loss is [0.00475888], parameters w:[1.92722957], b:[-0.2462684]\n195 loss is [0.00460198], parameters w:[1.92844843], b:[-0.2465791]\n196 loss is [0.00445026], parameters w:[1.92964683], b:[-0.24688536]\n197 loss is [0.00430354], parameters w:[1.93082509], b:[-0.24718726]\n198 loss is [0.00416167], parameters w:[1.93198357], b:[-0.24748484]\n199 loss is [0.00402448], parameters w:[1.9331226], b:[-0.24777816]\n200 loss is [0.00389181], parameters w:[1.9342425], b:[-0.24806728]\n201 loss is [0.00376353], parameters w:[1.93534359], b:[-0.24835226]\n202 loss is [0.00363948], parameters w:[1.9364262], b:[-0.24863315]\n203 loss is [0.00351952], parameters w:[1.93749063], b:[-0.24891001]\n204 loss is [0.00340351], parameters w:[1.93853719], b:[-0.24918288]\n205 loss is [0.00329134], parameters w:[1.93956618], b:[-0.24945183]\n206 loss is [0.00318286], parameters w:[1.9405779], b:[-0.2497169]\n207 loss is [0.00307797], parameters w:[1.94157264], b:[-0.24997815]\n208 loss is [0.00297653], parameters w:[1.94255068], b:[-0.25023564]\n209 loss is [0.00287844], parameters w:[1.9435123], b:[-0.2504894]\n210 loss is [0.00278359], parameters w:[1.94445779], b:[-0.25073949]\n211 loss is [0.00269186], parameters w:[1.94538741], b:[-0.25098597]\n212 loss is [0.00260316], parameters w:[1.94630143], b:[-0.25122888]\n213 loss is [0.00251739], parameters w:[1.94720011], b:[-0.25146826]\n214 loss is [0.00243444], parameters w:[1.94808371], b:[-0.25170417]\n215 loss is [0.00235423], parameters w:[1.94895249], b:[-0.25193666]\n216 loss is [0.00227667], parameters w:[1.9498067], b:[-0.25216576]\n217 loss is [0.00220166], parameters w:[1.95064657], b:[-0.25239154]\n218 loss is [0.00212913], parameters w:[1.95147235], b:[-0.25261402]\n219 loss is [0.00205899], parameters w:[1.95228428], b:[-0.25283327]\n220 loss is [0.00199116], parameters w:[1.95308259], b:[-0.25304931]\n221 loss is [0.00192556], parameters w:[1.95386751], b:[-0.25326221]\n222 loss is [0.00186213], parameters w:[1.95463927], b:[-0.25347199]\n223 loss is [0.0018008], parameters w:[1.95539809], b:[-0.25367871]\n224 loss is [0.00174148], parameters w:[1.95614417], b:[-0.2538824]\n225 loss is [0.00168412], parameters w:[1.95687775], b:[-0.25408311]\n226 loss is [0.00162865], parameters w:[1.95759903], b:[-0.25428088]\n227 loss is [0.00157501], parameters w:[1.95830821], b:[-0.25447575]\n228 loss is [0.00152313], parameters w:[1.9590055], b:[-0.25466776]\n229 loss is [0.00147297], parameters w:[1.9596911], b:[-0.25485694]\n230 loss is [0.00142446], parameters w:[1.96036521], b:[-0.25504335]\n231 loss is [0.00137755], parameters w:[1.96102801], b:[-0.25522702]\n232 loss is [0.00133218], parameters w:[1.96167971], b:[-0.25540798]\n233 loss is [0.00128831], parameters w:[1.96232048], b:[-0.25558627]\n234 loss is [0.00124589], parameters w:[1.96295051], b:[-0.25576194]\n235 loss is [0.00120486], parameters w:[1.96356998], b:[-0.25593501]\n236 loss is [0.00116519], parameters w:[1.96417907], b:[-0.25610553]\n237 loss is [0.00112682], parameters w:[1.96477795], b:[-0.25627353]\n238 loss is [0.00108972], parameters w:[1.96536679], b:[-0.25643905]\n239 loss is [0.00105384], parameters w:[1.96594577], b:[-0.25660211]\n240 loss is [0.00101914], parameters w:[1.96651504], b:[-0.25676277]\n241 loss is [0.00098559], parameters w:[1.96707478], b:[-0.25692104]\n242 loss is [0.00095314], parameters w:[1.96762513], b:[-0.25707696]\n243 loss is [0.00092176], parameters w:[1.96816627], b:[-0.25723057]\n244 loss is [0.00089141], parameters w:[1.96869834], b:[-0.2573819]\n245 loss is [0.00086207], parameters w:[1.9692215], b:[-0.25753099]\n246 loss is [0.00083369], parameters w:[1.96973589], b:[-0.25767785]\n247 loss is [0.00080624], parameters w:[1.97024167], b:[-0.25782253]\n248 loss is [0.0007797], parameters w:[1.97073897], b:[-0.25796506]\n249 loss is [0.00075404], parameters w:[1.97122795], b:[-0.25810546]\n250 loss is [0.00072921], parameters w:[1.97170873], b:[-0.25824377]\n251 loss is [0.00070521], parameters w:[1.97218147], b:[-0.25838002]\n252 loss is [0.000682], parameters w:[1.97264628], b:[-0.25851424]\n253 loss is [0.00065955], parameters w:[1.97310331], b:[-0.25864645]\n254 loss is [0.00063784], parameters w:[1.97355269], b:[-0.25877668]\n255 loss is [0.00061685], parameters w:[1.97399455], b:[-0.25890497]\n256 loss is [0.00059655], parameters w:[1.974429], b:[-0.25903134]\n257 loss is [0.00057691], parameters w:[1.97485619], b:[-0.25915581]\n258 loss is [0.00055793], parameters w:[1.97527621], b:[-0.25927842]\n259 loss is [0.00053957], parameters w:[1.97568921], b:[-0.25939919]\n260 loss is [0.00052181], parameters w:[1.9760953], b:[-0.25951816]\n261 loss is [0.00050464], parameters w:[1.97649458], b:[-0.25963533]\n262 loss is [0.00048803], parameters w:[1.97688718], b:[-0.25975075]\n263 loss is [0.00047197], parameters w:[1.97727321], b:[-0.25986443]\n264 loss is [0.00045644], parameters w:[1.97765278], b:[-0.2599764]\n265 loss is [0.00044142], parameters w:[1.978026], b:[-0.26008669]\n266 loss is [0.00042689], parameters w:[1.97839297], b:[-0.26019532]\n267 loss is [0.00041285], parameters w:[1.9787538], b:[-0.26030232]\n268 loss is [0.00039926], parameters w:[1.97910859], b:[-0.2604077]\n269 loss is [0.00038613], parameters w:[1.97945744], b:[-0.26051149]\n270 loss is [0.00037342], parameters w:[1.97980045], b:[-0.26061372]\n271 loss is [0.00036113], parameters w:[1.98013773], b:[-0.2607144]\n272 loss is [0.00034925], parameters w:[1.98046936], b:[-0.26081357]\n273 loss is [0.00033776], parameters w:[1.98079545], b:[-0.26091123]\n274 loss is [0.00032665], parameters w:[1.98111607], b:[-0.26100742]\n275 loss is [0.0003159], parameters w:[1.98143134], b:[-0.26110216]\n276 loss is [0.00030551], parameters w:[1.98174133], b:[-0.26119546]\n277 loss is [0.00029546], parameters w:[1.98204613], b:[-0.26128734]\n278 loss is [0.00028574], parameters w:[1.98234584], b:[-0.26137784]\n279 loss is [0.00027634], parameters w:[1.98264053], b:[-0.26146697]\n280 loss is [0.00026725], parameters w:[1.98293029], b:[-0.26155474]\n281 loss is [0.00025846], parameters w:[1.98321521], b:[-0.26164118]\n282 loss is [0.00024995], parameters w:[1.98349536], b:[-0.26172631]\n283 loss is [0.00024173], parameters w:[1.98377083], b:[-0.26181015]\n284 loss is [0.00023378], parameters w:[1.98404169], b:[-0.26189271]\n285 loss is [0.00022609], parameters w:[1.98430802], b:[-0.26197402]\n286 loss is [0.00021865], parameters w:[1.98456989], b:[-0.26205409]\n287 loss is [0.00021146], parameters w:[1.98482739], b:[-0.26213294]\n288 loss is [0.00020451], parameters w:[1.98508058], b:[-0.26221059]\n289 loss is [0.00019778], parameters w:[1.98532954], b:[-0.26228706]\n290 loss is [0.00019127], parameters w:[1.98557434], b:[-0.26236237]\n291 loss is [0.00018498], parameters w:[1.98581504], b:[-0.26243652]\n292 loss is [0.0001789], parameters w:[1.98605172], b:[-0.26250955]\n293 loss is [0.00017302], parameters w:[1.98628444], b:[-0.26258146]\n294 loss is [0.00016733], parameters w:[1.98651327], b:[-0.26265227]\n295 loss is [0.00016182], parameters w:[1.98673828], b:[-0.26272201]\n296 loss is [0.0001565], parameters w:[1.98695953], b:[-0.26279067]\n297 loss is [0.00015135], parameters w:[1.98717707], b:[-0.26285829]\n298 loss is [0.00014638], parameters w:[1.98739098], b:[-0.26292487]\n299 loss is [0.00014156], parameters w:[1.98760132], b:[-0.26299043]"
  },
  {
    "objectID": "myblog/posts/gradient-descent/index.html#gradient-descent",
    "href": "myblog/posts/gradient-descent/index.html#gradient-descent",
    "title": "Linear regression using gradient descent",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nGradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.\nSteps to implement gradient descent\nStep 1 - Compute the loss\nStep 2 - Compute the gradient\nStep 3 - Update the parameters\nStep 4 - Repeat Step 1 to 3\n\nStep 1 :- Compute the loss\nFor regression problem loss is given by mean squared error (MSE).\n\\[ MSE  = \\frac{1}{N} \\sum\\limits_{i=1}^{N} (y-\\hat{y_i})^2 \\]\n\\[ MSE  = \\frac{1}{N} \\sum\\limits_{i=1}^{N} (y-b-wx_i)^2 \\]\n\n\nStep 2 :- Compute the gradient\nGradient is a vector with function‚Äôs partial derivatives for components.\n\\[ \\Delta (MSE)  = [\\frac{\\partial MSE}{\\partial w}, \\frac{\\partial MSE}{\\partial b}] \\]\nIt also tell the direction of steepest ascent which means in which direction one should step to increase the function most quickly.\n\n\n\n\n\n\nNote\n\n\n\nA derivative tells us how much a given quantity changes when we change slightly some other quantity.\n\n\nIn our case we are interested how much our MSE loss change when we vary one of our two parameters.\n\\[ \\frac{\\partial MSE}{\\partial w} = \\frac{\\partial MSE}{\\partial \\hat{y}}.\\frac{\\partial \\hat{y}}{\\partial w} = \\frac{1}{N} \\sum\\limits_{i=1}^{N} 2(y-b-wx_i).(-x_i) = -2\\frac{1}{N} \\sum\\limits_{i=1}^{N}(x_i)(y-\\hat{y_i}) \\]\n\\[ \\frac{\\partial MSE}{\\partial b} = \\frac{\\partial MSE}{\\partial \\hat{y}}.\\frac{\\partial \\hat{y}}{\\partial b} = \\frac{1}{N} \\sum\\limits_{i=1}^{N} 2(y-b-wx_i).(-1) = -2\\frac{1}{N} \\sum\\limits_{i=1}^{N}(y-\\hat{y_i}) \\]\n\n\nStep 3 :- Update the Parameters\nNow we will update the parameters by using gradients to minimize the loss. But gradient tells the direction of steepest ascent so we will multiply by -1.\n\\[ w  = w - \\eta\\frac{\\partial MSE}{\\partial w} \\]\n\\[ b  = b - \\eta\\frac{\\partial MSE}{\\partial b} \\]\n\n\nStep 4:- Repeat\nNow we will use updated parameters and start with step 1 again. We will repeat this process for multiple epochs. This is also known as training the model.\n\n\nAn epoch is completed when all the data points has been used for calculating the loss.\n\nClick below to add some new points and see how algorithm adjust the slope of line over time to meet best fit.\n\np5(sketch =&gt; {\n  let system;\n  \n  //set up some default points\n  \n  /*    \n  */\n  \n  let data = [\n    sketch.createVector(.105, .423333),\n    sketch.createVector(.58, .526666),\n    sketch.createVector(.75, .82666),\n    sketch.createVector(.3475, .183333),\n    sketch.createVector(.3195, .543353),\n  ];\n  \n  let m = 1;\n  let b = 0;\n  \n  sketch.setup = function() {\n    sketch.createCanvas(800, 350);\n    sketch.background(51);\n  };\n  \n  const gradientDescent = () =&gt; {\n    var learning_rate = 0.05;\n    \n    for(var i = 0; i &lt; data.length; i++) {\n      var x = data[i].x;\n      var y = data[i].y;\n      \n      var guess = m * x + b;\n      var error = y - guess;\n      \n      m = m + (error * x) * learning_rate;\n      b = b + (error) * learning_rate;\n    }\n  }\n  \n  const drawLine = () =&gt; {\n    var x1 = 0;\n    var y1 = m * x1 + b;\n    var x2 = 1;\n    var y2 = m * x2 + b;\n    \n    x1 = sketch.map(x1, 0, 1, 0, sketch.width);\n    y1 = sketch.map(y1, 0, 1, sketch.height, 0);\n    x2 = sketch.map(x2, 0, 1, 0, sketch.width);\n    y2 = sketch.map(y2, 0, 1, sketch.height, 0);\n    \n    sketch.stroke(0, 0, 0);\n    sketch.strokeWeight(2);\n    sketch.line(x1, y1, x2, y2);\n  }\n \n  sketch.mousePressed = () =&gt; {\n    var x = sketch.map(sketch.mouseX, 0, sketch.width, 0 , 1);\n    var y = sketch.map(sketch.mouseY, 0, sketch.height, 1, 0);\n    var point = sketch.createVector(x, y);\n    data.push(point);\n  }\n  \n  sketch.draw = () =&gt; {\n    sketch.background(255, 255, 255)\n    for (var i = 0; i &lt; data.length; i++) {\n      var x = sketch.map(data[i].x, 0, 1, 0, sketch.width);\n      var y = sketch.map(data[i].y, 0, 1, sketch.height, 0);\n      sketch.fill(32, 178, 170);\n      sketch.stroke(32, 178, 170);\n      sketch.ellipse(x, y, 8, 8);\n    }\n    \n    if(data.length &gt; 1) {\n      gradientDescent();\n      drawLine();\n    }\n  }\n})\n\n\n\n\n\n\n\nimport {p5} from \"@tmcw/p5\""
  },
  {
    "objectID": "myblog/posts/gradient-descent/index.html#implementing-linear-regression",
    "href": "myblog/posts/gradient-descent/index.html#implementing-linear-regression",
    "title": "A Brief introduction to linear regression using gradient descent",
    "section": "Implementing Linear Regression",
    "text": "Implementing Linear Regression\nNow its time to implement our linear regression model using gradient descent.\n\nGenerate Synthetic data\nUsing the above equation we will generate some synthetic data with w = 2 and b = 1 and some random noise.\n\nnp.random.seed(42)\nimport numpy as np\nx = np.random.randn(100,1)\ny = 1 + 2*x + np.random.randn(100,1)\n\nOur goal will be to accurately predict the value of w (i.e.¬†2) and b (i.e.¬†1).\n\n\nInitialize parameters (w and b) and hyperparameter (learning_rate)\nSo we will initialize the learnable parameters w and b with some random values and try to find right value by minimizing the loss function. The value of the hyperparameter i.e.¬†learning rate is fixed.\n\nw = np.random.randn(1)\nb = np.random.randn(1)\nlearning_rate = 0.001\n\n\n\nGradient Descent algorithm\n\nCompute the loss\n\nCompute the gradients\n\nUpdate the parameters\n\nRepeat\n\n\ndef gradient_descent(x,y,w,b,learning_rate):\n    dldw = 0.0 \n    dldb = 0.0\n    N = x.shape[0]\n    \n    \n    for xi, yi in zip(x,y):\n        dldw += -2*xi*(yi-(w*xi+b))\n        dldb += -2*(yi-(w*xi+b))\n    \n    #make an update to the parameters\n    w = w - learning_rate*(1/N)*dldw\n    b = b - learning_rate*(1/N)*dldb\n    return w,b\n\n\n\nIterate gradient descent function and update parameters to minimize loss.\n\nfor epoch in range(3000):\n    w, b = gradient_descent(x,y,w,b,learning_rate) \n    yhat =  w*x + b\n    loss =  np.divide(np.sum((y-yhat)**2, axis=0), x.shape[0])\n    #print(f'{epoch} loss is {loss}, parameters w:{w}, b:{b}')\nprint(w, b)\n\n[1.84846592] [1.00379672]"
  },
  {
    "objectID": "myblog/posts/gradient-descent/index.html#simple-linear-regression",
    "href": "myblog/posts/gradient-descent/index.html#simple-linear-regression",
    "title": "Linear regression using gradient descent",
    "section": "",
    "text": "A simple linear regression equation with one feature is defined as :\n\\[y = b + w * x + \\epsilon\\]\nHere w is coefficient and b is intercept term and \\(\\epsilon\\) is the noise."
  },
  {
    "objectID": "myblog/posts/gradient-descent/index.html#gradient-descent-variants",
    "href": "myblog/posts/gradient-descent/index.html#gradient-descent-variants",
    "title": "Linear regression using gradient descent",
    "section": "Gradient Descent Variants",
    "text": "Gradient Descent Variants\n\nBatch Gradient Descent\nStochastic Gradient Descent\nMini Batch Gradient Descent\n\nIf we use all the points in the training set to compute loss then it is batch gradient descent. If we use single data point and update our parameters it stochastic gradient descent. Anything between 1 and N is mini batch gradient descent."
  },
  {
    "objectID": "myblog/posts/gradient-descent/index.html#implementing-linear-regression-using-batch-gradient-descent",
    "href": "myblog/posts/gradient-descent/index.html#implementing-linear-regression-using-batch-gradient-descent",
    "title": "Linear regression using gradient descent",
    "section": "Implementing Linear Regression using batch gradient descent",
    "text": "Implementing Linear Regression using batch gradient descent\nNow its time to implement our linear regression model using batch gradient descent.\n\nGenerate Synthetic data\nUsing the above equation we will generate some synthetic data with w = 2 and b = 1 and some random noise.\n\nimport numpy as np\nnp.random.seed(42)\n\nx = np.random.randn(100,1)\ny = 1 + 2*x + np.random.randn(100,1)\n\nOur goal will be to accurately predict the value of w (i.e.¬†2) and b (i.e.¬†1).\n\n\nInitialize parameters (w and b) and hyperparameter (learning_rate)\nSo we will initialize the learnable parameters w and b with some random values and try to find right value by minimizing the loss function. The value of the hyperparameter i.e.¬†learning rate is fixed.\n\nw = np.random.randn(1)\nb = np.random.randn(1)\nlr = 0.001\n\n\n\nGradient Descent algorithm\n\ndef gradient_descent(x, y, w, b, lr):\n    \n    #compute the loss\n    yhat =  b + w*x\n    error = (y-yhat)\n    loss  = (error*2).mean()\n    \n    #compute the gradients\n    b_grad = -2*error.mean()\n    w_grad = -2*(x*error).mean()\n    \n    #update the parameters\n    b = b - lr * b_grad\n    w = w - lr * w_grad\n    return w,b\n\n\n# implementing multiple epochs \n\nfor epoch in range(5000):\n    w,b = gradient_descent(x, y, w, b, lr)\nprint(b,w)\n\n[1.00716235] [1.85616176]\n\n\nLet‚Äôs compare our output with scikit-learn‚Äôs Linear Regression\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x, y)\nprint(lr.intercept_, lr.coef_[0])\n\n[1.00742783] [1.85674284]\n\n\nOur result matches upto few decimal places that means we have correctly implemented our batch gradient descent algorithm."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Vidyasagar Bhargava",
    "section": "Education",
    "text": "Education\nB.Tech in Computer Science & Engineering | University of Uttar Pradesh"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Vidyasagar Bhargava",
    "section": "Experience",
    "text": "Experience\nTVS Motors (2021-Present) Predictive maintenance Snapdeal (2015-2019) Price Optimization"
  },
  {
    "objectID": "myblog/posts/introduction-to-manim/Untitled.html",
    "href": "myblog/posts/introduction-to-manim/Untitled.html",
    "title": "Introduction to Manim",
    "section": "",
    "text": "Introduction\nA community maintained Python library for creating mathematical animations.\n\n\n%%manim -qm SimpleExample\n\nclass SimpleExample(Scene):\n    def construct(self):\n        blue_circle = Circle(color='#654a4e', fill_opacity=0.5)\n        green_square = Square(color='#20B2AA', fill_opacity=0.5)\n        green_square.next_to(blue_circle, RIGHT)\n        self.add(blue_circle, green_square)\n\n\n\n\n\n%%manim -qm AnimationExample\nclass AnimationExample(Scene):\n    def construct(self):\n        ax = Axes(x_range =(-3,3), y_range=(-3,3))\n        curve = ax.plot(lambda x: (x+2)*x*(x-2)/2, color='#654a4e')\n        self.add(ax,curve)\n       # self.play(Create(ax))\n\n\n\n\nGetting the area\n\n%%manim -qm AnimationExample\nclass AnimationExample(Scene):\n    def construct(self):\n        ax = Axes(x_range =(-3,3), y_range=(-3,3))\n        curve = ax.plot(lambda x: (x+2)*x*(x-2)/2, color='#654a4e')\n        area = ax.get_area(curve, x_range=(-2,0))\n        self.add(ax,curve, area)\n\n\n\n\nNow lets see some animation\n\n\n%%manim -v WARNING  --progress_bar None   --disable_caching AnimationExample\nconfig.media_embed = True\n\n\nclass AnimationExample(Scene):\n    def construct(self):\n        self.camera.background_color = '#e6ddde'\n        ax = Axes(x_range =(-3,3), y_range=(-3,3))\n        curve = ax.plot(lambda x: (x+2)*x*(x-2)/2, color='#654a4e')\n        area = ax.get_area(curve, x_range=(-2,0))\n        self.add(ax,curve, area)\n        self.play(Create(ax), Create(curve))\n        self.play(FadeIn(area))\n\n\n \n Your browser does not support the video tag.\n \n\n\n\nTo be continued‚Ä¶"
  },
  {
    "objectID": "projects/predictive_maintenance.html#section",
    "href": "projects/predictive_maintenance.html#section",
    "title": "Predictive Maintenance for NTORQ",
    "section": "",
    "text": "Tools : python\nMachine learning : Regression\nRole : Lead Data Scientist\nStatus : In progress"
  },
  {
    "objectID": "myblog/posts/Generative-AI/index.html",
    "href": "myblog/posts/Generative-AI/index.html",
    "title": "Introduction to Diffusion based Models",
    "section": "",
    "text": "Coming Soon"
  },
  {
    "objectID": "myblog/posts/Generative-AI/index.html#generative-model-evaluation",
    "href": "myblog/posts/Generative-AI/index.html#generative-model-evaluation",
    "title": "Introduction to Diffusion based Models",
    "section": "",
    "text": "A generative model is evaluated using FID scores (Fr√©chet Inception Distance). It measures how closely generated samples match real-world samples by comparing statistics between feature maps extracted from both sets of data using a pre-trained neural network. The lower the score, the better the quality and realism of generated images produced by a given model.\nLibrary like clean-fid is used to standardize the FID calculation.\n\nimport numpy as np"
  },
  {
    "objectID": "myblog/posts/Generative-AI/index.html#diffusion-models",
    "href": "myblog/posts/Generative-AI/index.html#diffusion-models",
    "title": "Introduction to Diffusion based Models",
    "section": "",
    "text": "Coming Soon"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Vidyasagar Bhargava",
    "section": "",
    "text": "I‚Äôm a Lead Data Scientist at TVS Motor, working for connected mobility solutions department. My interests include Machine Learning, Computer Vision, and Generative AI. I have 10+ years of experience in the field of Machine learning and Data Science where I worked with various clients in developing prescriptive as well as predictive solutions to address core business problems.\n\n\n\n\n\n\nRoot Cause Analysis for Low Range Trips in EVs\n\nPredictive Maintenance of Brakes in two wheelers\n\nRange Prediction and Analysis for EVs\nChurn Prediction in Telecom Industry\nPrice optimization Engine for Scientific Pricing of Fashion Products\nNPS Improvement using explainable AI\nRetail store segmentation\nAnomaly Detection for machine parts failure\n\n\n\n\n\nB.Tech in Computer Science and Engineering\nXII in Computer Science\n\n\n\n\n\nNeural Networks and Deep Learning\nImproving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization\nBig Data Foundation\nIntroduction to Python"
  },
  {
    "objectID": "index.html#selected-projects",
    "href": "index.html#selected-projects",
    "title": "Vidyasagar Bhargava",
    "section": "",
    "text": "Generative AI | Ergonomic Methods for Assessing Spatial Models | 2023\nMachine Learning | Spatial Resampling Infrastructure | 2022\nunifir | A Unifying API for Working with Unity in R | 2022\nterrainr | Retrieve Data from the USGS National Map and Transform it for 3D Landscape Visualizations | 2021"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Vidyasagar Bhargava",
    "section": "",
    "text": "Generative AI | Ergonomic Methods for Assessing Spatial Models | 2023\nComputer Vision | Spatial Resampling Infrastructure | 2022\nXAI"
  },
  {
    "objectID": "index.html#technologies",
    "href": "index.html#technologies",
    "title": "Vidyasagar Bhargava",
    "section": "",
    "text": "Programming - Python, SQL, R\nDeep Learning Framworks - PyTorch\nOther tools and Platforms - Databricks"
  },
  {
    "objectID": "index.html#programming",
    "href": "index.html#programming",
    "title": "Vidyasagar Bhargava",
    "section": "",
    "text": "Python, SQL, R"
  },
  {
    "objectID": "index.html#major-projects",
    "href": "index.html#major-projects",
    "title": "Vidyasagar Bhargava",
    "section": "",
    "text": "Root Cause Analysis for Low Range Trips in EVs.\nPredictive Maintenance for ICE Vehicles\nRange Prediction for Electric Vehicle\nChurn Prediction for Telecom Company\nNPS Improvement for US based client\nBenchmarking of trained sentence embedding models\nScaling Machine Learning models using Spark\nDashboard for Time Series forecasting using Dash & Plotly\nRetail store segmentation\nAnomaly Detection for machine parts failure\nPrice optimization Engine based on price elasticity models\n\n\n\n\nB.Tech in Computer Science and Engineering\n\n\n\n\n\nMachine Learning Course by Andrew NG\nDeep learning Course by Andrew NG\nBig Data Foundation by IBM\nIntroduction to Python by Kaggle\n\n\n\n\n\nIntroduction to Deep learning at Wipro Technologies on June 2017. [PPT]"
  },
  {
    "objectID": "index.html#talks-webinars",
    "href": "index.html#talks-webinars",
    "title": "Vidyasagar Bhargava",
    "section": "",
    "text": "Introduction to Deep learning at Wipro Technologies on June 2017. [PPT]"
  },
  {
    "objectID": "index.html#honors-awards",
    "href": "index.html#honors-awards",
    "title": "Vidyasagar Bhargava",
    "section": "",
    "text": "Thinking Hat at Wipro Limited | 2018\nBull‚Äôs Eye Award at Snapdeal | 2015"
  },
  {
    "objectID": "research/research.html",
    "href": "research/research.html",
    "title": "Research Papers",
    "section": "",
    "text": "Implementation of top research papers in the field of Deep learning and understanding the architecture and maths behind the same.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlexnet\n\n\n\n\n\nAlexNet is the first deep architecture which was introduced by Alex Krizhevsky and his colleagues in 2012. It was designed to classify images for the ImageNet LSVRC-2010 competition where it achieved state of the art results. It is a simple yet powerful network architecture, which helped pave the way for groundbreaking research in Deep Learning as it is now.You can read more about the model in original research paper here\n\n\n\n\n\nMay 12, 2022\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\n\n\n\n\n\n\nLeNet-5\n\n\n\n\n\nLeNet-5 is introduced by Yann LeCun, Leon Bottou, Yoshua Bengio and Patrick Haffner in the year 1998 in the paper Gradient-Based Learning Applied to Document Recognition. LeNet is a classic convolutional neural network employing the use of convolutions, pooling and fully connected layers. It was used for the handwritten digit recognition task with the MNIST dataset.\n\n\n\n\n\nMay 8, 2022\n\n\nVidyasagar Bhargava\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "research/papers/alexnet/index.html",
    "href": "research/papers/alexnet/index.html",
    "title": "Lenet-5",
    "section": "",
    "text": "img"
  },
  {
    "objectID": "research/posts/lenet/index.html",
    "href": "research/posts/lenet/index.html",
    "title": "LeNet-5",
    "section": "",
    "text": "Here we will be using famous MNIST dataset which contains hand written digits. These are greyscale with size of 28x28 composed of 60,000 training images and 10,000 testing images."
  },
  {
    "objectID": "research/posts/lenet/index.html#testing",
    "href": "research/posts/lenet/index.html#testing",
    "title": "LeNet-5",
    "section": "Testing",
    "text": "Testing\n\n# Test the model\n# In test phase, we don't need to compute gradients (for memory efficiency)\n  \nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n     \n\nAccuracy of the network on the 10000 test images: 98.95 %\n\n\nUsing this model, we get around 98.95% accuracy which is quite good."
  },
  {
    "objectID": "research/posts/lenet/index.html#conclusions",
    "href": "research/posts/lenet/index.html#conclusions",
    "title": "LeNet-5",
    "section": "Conclusions",
    "text": "Conclusions\n\nWe started by learning the architecture of LeNet5 and the different kinds of layers in that.\n\nThen we built LeNet5 from scratch along with defining hyperparameters for the model.\n\nFinally, we trained and tested our model on the MNIST dataset, and the model seemed to perform well on the test dataset."
  },
  {
    "objectID": "research/posts/lenet/index.html#dataset",
    "href": "research/posts/lenet/index.html#dataset",
    "title": "LeNet-5",
    "section": "",
    "text": "Here we will be using famous MNIST dataset which contains hand written digits. These are greyscale with size of 28x28 composed of 60,000 training images and 10,000 testing images."
  },
  {
    "objectID": "research/posts/lenet/index.html#dataset-understanding",
    "href": "research/posts/lenet/index.html#dataset-understanding",
    "title": "LeNet-5",
    "section": "Dataset Understanding",
    "text": "Dataset Understanding\nHere we will be using famous MNIST dataset which contains hand written digits. These are greyscale with size of 28x28 composed of 60,000 training images and 10,000 testing images.\n\nImporting Libraries\n\n# loading the relevant libraries\nimport torch\nimport torch.nn as nn \nimport torchvision\nimport torchvision.transforms as transforms\n\n# Define the relevant variables\nbatch_size = 64\nnum_classes = 10\nlearning_rate = 0.001\nnum_epochs = 10\n\n# Device will determine whether to run the training on GPU or CPU.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n\nLoading the Dataset\nUsing the torchvision library we will load the dataset.\n\n\n\n\n\n\nImportant\n\n\n\nThe MNIST data can‚Äôt be used as it is for the LeNet5 architecture. The LeNet5 architecture accepts the input to be 32x32 and the MNIST images are 28x28. We can fix this by resizing the images, normalizing them using the pre-calculated mean and standard deviation (available online), and finally storing them as tensors.\n\n\n\n\n# Loading the dataset and preprocessing\ntrain_dataset = torchvision.datasets.MNIST(root = './data',\n                                           train = True,\n                                           transform = transforms.Compose([\n                                                  transforms.Resize((32,32)),\n                                                  transforms.ToTensor(),\n                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n                                           download = True)\n\n\ntest_dataset = torchvision.datasets.MNIST(root = './data',\n                                          train = False,\n                                          transform = transforms.Compose([\n                                                  transforms.Resize((32,32)),\n                                                  transforms.ToTensor(),\n                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n                                          download=True)\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                           batch_size = batch_size,\n                                           shuffle = True)\n\n\ntest_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n                                           batch_size = batch_size,\n                                           shuffle = True)\n\n\n\n\nLeNet5 from Scratch\n\n# Defining the convolutional neural network\nclass LeNet5(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n            nn.BatchNorm2d(6),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 2, stride = 2))\n        self.fc = nn.Linear(400, 120)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(120, 84)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(84, num_classes)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        out = self.relu(out)\n        out = self.fc1(out)\n        out = self.relu1(out)\n        out = self.fc2(out)\n        return out\n\n\n\nSetting up hyperparameters\n\nmodel = LeNet5(num_classes).to(device)\n\n# Setting the loss function\ncost = nn.CrossEntropyLoss()\n\n# Setting the optimizer with the model parameters and learning rate\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# this is defined to print how many steps are remaining when training\ntotal_step = len(train_loader)\n\n\n\nModel Training\n\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        #Forward pass\n        outputs = model(images)\n        loss = cost(outputs, labels)\n            \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n                \n        if (i+1) % 400 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\nEpoch [1/10], Step [400/938], Loss: 0.1018\nEpoch [1/10], Step [800/938], Loss: 0.0178\nEpoch [2/10], Step [400/938], Loss: 0.0299\nEpoch [2/10], Step [800/938], Loss: 0.0044\nEpoch [3/10], Step [400/938], Loss: 0.0335\nEpoch [3/10], Step [800/938], Loss: 0.0365\nEpoch [4/10], Step [400/938], Loss: 0.0217\nEpoch [4/10], Step [800/938], Loss: 0.0110\nEpoch [5/10], Step [400/938], Loss: 0.0063\nEpoch [5/10], Step [800/938], Loss: 0.0620\nEpoch [6/10], Step [400/938], Loss: 0.0178\nEpoch [6/10], Step [800/938], Loss: 0.0588\nEpoch [7/10], Step [400/938], Loss: 0.0092\nEpoch [7/10], Step [800/938], Loss: 0.0120\nEpoch [8/10], Step [400/938], Loss: 0.0048\nEpoch [8/10], Step [800/938], Loss: 0.0455\nEpoch [9/10], Step [400/938], Loss: 0.0067\nEpoch [9/10], Step [800/938], Loss: 0.0589\nEpoch [10/10], Step [400/938], Loss: 0.0015\nEpoch [10/10], Step [800/938], Loss: 0.0030\n\n\nAs we can see, the loss is decreasing with every epoch which shows that our model is indeed learning.\n\n\nModel Testing\n\n# Test the model\n# In test phase, we don't need to compute gradients (for memory efficiency)\n  \nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n     \n\nAccuracy of the network on the 10000 test images: 98.97 %\n\n\nUsing this model, we get around 98.97% accuracy which is quite good."
  },
  {
    "objectID": "research/posts/alexnet/index.html",
    "href": "research/posts/alexnet/index.html",
    "title": "Alexnet",
    "section": "",
    "text": "Unlike LeNet-5 the famous Alexnet network was operated with 3-channel images which were (224x224x3) in size. It also used max pooling with ReLU activations when subsampling. The kernels used for convolutions were either 11x11, 5x5, or 3x3 while kernels used for max pooling were 3x3 in size. It classified images into 1000 classes. It also utilized multiple GPUs."
  },
  {
    "objectID": "research/posts/alexnet.html",
    "href": "research/posts/alexnet.html",
    "title": "Alexnet",
    "section": "",
    "text": "import pandas as pd \nimport numpy as np"
  },
  {
    "objectID": "research/posts/alexnet/index.html#architecture",
    "href": "research/posts/alexnet/index.html#architecture",
    "title": "Alexnet",
    "section": "",
    "text": "Unlike LeNet-5 the famous Alexnet network was operated with 3-channel images which were (224x224x3) in size. It also used max pooling with ReLU activations when subsampling. The kernels used for convolutions were either 11x11, 5x5, or 3x3 while kernels used for max pooling were 3x3 in size. It classified images into 1000 classes. It also utilized multiple GPUs."
  },
  {
    "objectID": "research/posts/alexnet/index.html#data-loading",
    "href": "research/posts/alexnet/index.html#data-loading",
    "title": "Alexnet",
    "section": "Data Loading",
    "text": "Data Loading\n\nDataset\nLet‚Äôs start by loading and then pre-processing the data. For our purposes, we will be using the CIFAR10 dataset. The dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nClasses in the dataset are completely mututally exclusive. There is no overlap.\n\n\n\nimporting the libraries\nLet‚Äôs start by importing the required libraries along with defining a variable device, so that the Notebook knows to use a GPU to train the model if it‚Äôs available.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n\nLoading the dataset\nUsing torchvision (a helper library for computer vision tasks), we will load our dataset. This method has some helper functions that makes pre-processing pretty easy and straight-forward. Let‚Äôs define the functions get_train_valid_loader and get_test_loader, and then call them to load in and process our CIFAR-10 data\n\ndef get_train_valid_loader(data_dir,\n                           batch_size,\n                           augment,\n                           random_seed,\n                           valid_size=0.1,\n                           shuffle=True):\n    normalize = transforms.Normalize(\n        mean=[0.4914, 0.4822, 0.4465],\n        std=[0.2023, 0.1994, 0.2010],\n    )\n\n    # define transforms\n    valid_transform = transforms.Compose([\n            transforms.Resize((227,227)),\n            transforms.ToTensor(),\n            normalize,\n    ])\n    if augment:\n        train_transform = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ])\n    else:\n        train_transform = transforms.Compose([\n            transforms.Resize((227,227)),\n            transforms.ToTensor(),\n            normalize,\n        ])\n\n    # load the dataset\n    train_dataset = datasets.CIFAR10(\n        root=data_dir, train=True,\n        download=True, transform=train_transform,\n    )\n\n    valid_dataset = datasets.CIFAR10(\n        root=data_dir, train=True,\n        download=True, transform=valid_transform,\n    )\n\n    num_train = len(train_dataset)\n    indices = list(range(num_train))\n    split = int(np.floor(valid_size * num_train))\n\n    if shuffle:\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n\n    train_idx, valid_idx = indices[split:], indices[:split]\n    train_sampler = SubsetRandomSampler(train_idx)\n    valid_sampler = SubsetRandomSampler(valid_idx)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, sampler=train_sampler)\n \n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n\n    return (train_loader, valid_loader)\n\n\ndef get_test_loader(data_dir,\n                    batch_size,\n                    shuffle=True):\n    normalize = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n    )\n\n    # define transform\n    transform = transforms.Compose([\n        transforms.Resize((227,227)),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n    dataset = datasets.CIFAR10(\n        root=data_dir, train=False,\n        download=True, transform=transform,\n    )\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, shuffle=shuffle\n    )\n\n    return data_loader\n\n\n# CIFAR10 dataset \ntrain_loader, valid_loader = get_train_valid_loader(data_dir = './data',                                      batch_size = 64,\n                       augment = False,                                          random_seed = 1)\n\ntest_loader = get_test_loader(data_dir = './data',\n                              batch_size = 64)\n\n\n\nAlexNet from Scratch\n\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(AlexNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n            nn.BatchNorm2d(96),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 3, stride = 2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 3, stride = 2))\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU())\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(384),\n            nn.ReLU())\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size = 3, stride = 2))\n        self.fc = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(9216, 4096),\n            nn.ReLU())\n        self.fc1 = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU())\n        self.fc2= nn.Sequential(\n            nn.Linear(4096, num_classes))\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out\n\n\n\nSetting Hyperparameters\n\nnum_classes = 10\nnum_epochs = 20\nbatch_size = 64\nlearning_rate = 0.005\n\nmodel = AlexNet(num_classes).to(device)\n\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n\n\n# Train the model\ntotal_step = len(train_loader)\n\n\n\nTraining\n\n\ntotal_step = len(train_loader)\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Move tensors to the configured device\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n            \n    # Validation\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in valid_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            del images, labels, outputs\n    \n        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))"
  },
  {
    "objectID": "myblog/posts/introduction-to-apple-machine-learning-framework-mlx/index.html",
    "href": "myblog/posts/introduction-to-apple-machine-learning-framework-mlx/index.html",
    "title": "Introduction to Apple‚Äôs Machine learning Framework- MLX",
    "section": "",
    "text": "Installation\nMLX is available on PyPI. You need an Apple silicon based computer.\npip install mlx\n\n\nKey Features of MLX\n1. Familiar APIs\nMLX has a Python API that closely follows NumPy. MLX also has a fully featured C++ API, which closely mirrors the Python API. MLX has higher-level packages like mlx.nn and mlx.optimizers with APIs that closely follow PyTorch to simplify building more complex models.\n\nimport mlx.core as mx\na = mx.array([1,2,3,4])\nprint(a)\n\narray([1, 2, 3, 4], dtype=int32)\n\n\n\nprint(a.dtype)\n\nint32\n\n\n\nb = mx.array([1.0, 2.0, 3.0, 4.0])\nprint(b.dtype)\n\nfloat32\n\n\n2. Lazy computation\nComputations in MLX are lazy. That means outputs of MLX operations are not computed untill they are needed.\n\nc = a + b   # c not yet evaluated\nmx.eval(c)  # evaluates c\nc = a + b\nprint(c)    # also evaluates c\n\narray([2, 4, 6, 8], dtype=float32)\n\n\n3. Composable function transformations & Dynamic graph construction\nMLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.\nComputation graphs in MLX are constructed dynamically. Changing the shapes of function arguments does not trigger slow compilations, and debugging is simple and intuitive.\n\n# MLX has standard function transformations like grad() and vmap()\n\nx = mx.array(0.0)\nmx.sin(x)\n\narray(0, dtype=float32)\n\n\n\nmx.grad(mx.sin)(x)\n\narray(1, dtype=float32)\n\n\n\nmx.grad(mx.grad(mx.sin))(x)\n\narray(-0, dtype=float32)\n\n\n4. Unified memory Architecture\nA notable difference from MLX and other frameworks is the unified memory model. Arrays in MLX live in shared memory. Operations on MLX arrays can be performed on any of the supported device types without transferring data.\nLet‚Äôs see an example\n\na = mx.random.normal((100,))\nb = mx.random.normal((100,))\n\nboth a and b lives in unified memory.\nIn MLX, you don‚Äôt need to move arrays between different memory locations for different devices (like CPU or GPU). Instead of moving data, you specify the device (like CPU or GPU) when you perform an operation on the arrays.\n\nmx.add(a, b, stream=mx.cpu)\n\narray([2, 4, 6, 8], dtype=float32)\n\n\n\nmx.add(a, b, stream=mx.gpu)\n\narray([2, 4, 6, 8], dtype=float32)\n\n\nIf you perform operations that don‚Äôt depend on each other (like adding ‚Äòa‚Äô and ‚Äòb‚Äô in example), MLX can run them in parallel. So, the CPU and GPU can both work on the same task simultaneously because there are no dependencies between them.\n\nc = mx.add(a, b, stream=mx.cpu)\nd = mx.add(a, c, stream=mx.gpu)\n\nIf there are dependencies (meaning one operation depends on the result of another), MLX takes care of managing them. For instance, if you add ‚Äòa‚Äô and ‚Äòb‚Äô on the CPU and then perform another addition on the GPU that depends on the result from the CPU, MLX ensures that the GPU operation waits for the CPU operation to finish before it starts.\nExample\n\ndef fun(a, b, d1, d2):\n  x = mx.matmul(a, b, stream=d1)\n  for _ in range(500):\n      b = mx.exp(b, stream=d2)\n  return x, b\n\n\na = mx.random.uniform(shape=(4096, 512))\nb = mx.random.uniform(shape=(512, 4))\n\nThe first matmul operation is good fit for the GPU since it is more compute dense. The second sequence of operations are better fit for the CPU, since they are very small and would be probably overhead bound on GPU.\n5. Multi-device\nOperations can run on any of the supported devices (currently the CPU and the GPU). The framework is intended to be user-friendly, but still efficient to train and deploy models. The design of the framework itself is also conceptually simple.\n\nmx.default_stream(mx.default_device())\n\nStream(Device(gpu, 0), 0)\n\n\n\n\nLinear Regression implementation\nLet‚Äôs implement simple linear regression example as starting point\n\nimport mlx.core as mx\n\nnum_features = 100\nnum_examples = 1_000\nnum_iters = 10_000  # iterations of SGD\nlr = 0.01  # learning rate for SGD\n\nInitialize parameters (w and b) and hyperparameter (learning_rate)\n\n# True parameters\nw_star = mx.random.normal((num_features,))\n\n# Input examples (design matrix)\nX = mx.random.normal((num_examples, num_features))\n\n# Noisy labels\neps = 1e-2 * mx.random.normal((num_examples,))\ny = X @ w_star + eps\n\n\ndef loss_fn(w):\n    return 0.5 * mx.mean(mx.square(X @ w - y))\n\ngrad_fn = mx.grad(loss_fn)\n\n\nw = 1e-2 * mx.random.normal((num_features,))\n\nfor _ in range(num_iters):\n    grad = grad_fn(w)\n    w = w - lr * grad\n    mx.eval(w)\n\n\nloss = loss_fn(w)\nerror_norm = mx.sum(mx.square(w - w_star)).item() ** 0.5\n\nprint(\n    f\"Loss {loss.item():.5f}, |w-w*| = {error_norm:.5f}, \"\n)\n\nLoss 0.00004, |w-w*| = 0.00354, \n\n\n\n\nLogistic Regression\nLet‚Äôs implement logistic regression now\n\nimport time\n\nimport mlx.core as mx\n\nnum_features = 100\nnum_examples = 1_000\nnum_iters = 10_000\nlr = 0.1\n\n# True parameters\nw_star = mx.random.normal((num_features,))\n\n# Input examples\nX = mx.random.normal((num_examples, num_features))\n\n# Labels\ny = (X @ w_star) &gt; 0\n\n\n# Initialize random parameters\nw = 1e-2 * mx.random.normal((num_features,))\n\n\ndef loss_fn(w):\n    logits = X @ w\n    return mx.mean(mx.logaddexp(0.0, logits) - y * logits)\n\n\ngrad_fn = mx.grad(loss_fn)\n\ntic = time.time()\nfor _ in range(num_iters):\n    grad = grad_fn(w)\n    w = w - lr * grad\n    mx.eval(w)\n\ntoc = time.time()\n\nloss = loss_fn(w)\nfinal_preds = (X @ w) &gt; 0\nacc = mx.mean(final_preds == y)\n\nthroughput = num_iters / (toc - tic)\nprint(\n    f\"Loss {loss.item():.5f}, Accuracy {acc.item():.5f} \"\n    f\"Throughput {throughput:.5f} (it/s)\"\n)\n\nLoss 0.02796, Accuracy 1.00000 Throughput 2555.69304 (it/s)\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "myblog/posts/introduction-to-apple-machine-learning-framework-mlx/index.html#installation",
    "href": "myblog/posts/introduction-to-apple-machine-learning-framework-mlx/index.html#installation",
    "title": "Introduction to Apple‚Äôs Machine learning Framework- MLX",
    "section": "",
    "text": "MLX is available on PyPI. You need an Apple silicon based computer.\npip install mlx\n\n\n\n\nMLX has a Python API that closely follows NumPy. MLX also has a fully featured C++ API, which closely mirrors the Python API. MLX has higher-level packages like mlx.nn and mlx.optimizers with APIs that closely follow PyTorch to simplify building more complex models.\n\nimport mlx.core as mx\na = mx.array([1,2,3,4])\nprint(a)\n\narray([1, 2, 3, 4], dtype=int32)\n\n\n\nprint(a.dtype)\n\nint32\n\n\n\nb = mx.array([1.0, 2.0, 3.0, 4.0])\nprint(b.dtype)\n\nfloat32\n\n\n\n\n\nComputations in MLX are lazy. That means outputs of MLX operations are not computed untill they are needed.\n\nc = a + b   # c not yet evaluated\nmx.eval(c)  # evaluates c\nc = a + b\nprint(c)    # also evaluates c\n\narray([2, 4, 6, 8], dtype=float32)\n\n\n\n\n\nMLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.\nComputation graphs in MLX are constructed dynamically. Changing the shapes of function arguments does not trigger slow compilations, and debugging is simple and intuitive.\n\n# MLX has standard function transformations like grad() and vmap()\n\nx = mx.array(0.0)\nmx.sin(x)\n\narray(0, dtype=float32)\n\n\n\nmx.grad(mx.sin)(x)\n\narray(1, dtype=float32)\n\n\n\nmx.grad(mx.grad(mx.sin))(x)\n\narray(-0, dtype=float32)\n\n\n\n\n\nA notable difference from MLX and other frameworks is the unified memory model. Arrays in MLX live in shared memory. Operations on MLX arrays can be performed on any of the supported device types without transferring data.\n\n\n\nOperations can run on any of the supported devices (currently the CPU and the GPU). The framework is intended to be user-friendly, but still efficient to train and deploy models. The design of the framework itself is also conceptually simple."
  },
  {
    "objectID": "myblog/posts/introduction-to-apple-machine-learning-framework-mlx/index.html#top-features-of-mlx",
    "href": "myblog/posts/introduction-to-apple-machine-learning-framework-mlx/index.html#top-features-of-mlx",
    "title": "Introduction to Apple‚Äôs Machine learning Framework- MLX",
    "section": "Top Features of MLX",
    "text": "Top Features of MLX\n\nNumPy like APIs\nMLX API closely aligned with NumPy which makes some sense of familiarity to users.\n\nimport mlx.core as mx\na = mx.array([1,2,3,4])\na.shape\n\n[4]"
  },
  {
    "objectID": "myblog/posts/introduction-to-apple-machine-learning-framework-mlx/index.html#key-features-of-mlx",
    "href": "myblog/posts/introduction-to-apple-machine-learning-framework-mlx/index.html#key-features-of-mlx",
    "title": "Introduction to Apple‚Äôs Machine learning Framework- MLX",
    "section": "Key Features of MLX",
    "text": "Key Features of MLX\n\nFamiliar APIs\nMLX has a Python API that closely follows NumPy. MLX also has a fully featured C++ API, which closely mirrors the Python API. MLX has higher-level packages like mlx.nn and mlx.optimizers with APIs that closely follow PyTorch to simplify building more complex models.\n\nimport mlx.core as mx\na = mx.array([1,2,3,4])\nprint(a)\n\narray([1, 2, 3, 4], dtype=int32)\n\n\n\nprint(a.dtype)\n\nint32\n\n\n\nb = mx.array([1.0, 2.0, 3.0, 4.0])\nprint(b.dtype)\n\nfloat32\n\n\n\n\nLazy computation:\nComputations in MLX are lazy. That means outputs of MLX operations are not computed untill they are needed.\n\nc = a + b   # c not yet evaluated\nmx.eval(c)  # evaluates c\nc = a + b\nprint(c)    # also evaluates c\n\narray([2, 4, 6, 8], dtype=float32)\n\n\n\n\nComposable function transformations & Dynamic graph construction :\nMLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.\nComputation graphs in MLX are constructed dynamically. Changing the shapes of function arguments does not trigger slow compilations, and debugging is simple and intuitive.\n\n# MLX has standard function transformations like grad() and vmap()\n\nx = mx.array(0.0)\nmx.sin(x)\n\narray(0, dtype=float32)\n\n\n\nmx.grad(mx.sin)(x)\n\narray(1, dtype=float32)\n\n\n\nmx.grad(mx.grad(mx.sin))(x)\n\narray(-0, dtype=float32)\n\n\n\n\nUnified memory\nA notable difference from MLX and other frameworks is the unified memory model. Arrays in MLX live in shared memory. Operations on MLX arrays can be performed on any of the supported device types without transferring data.\n\n\nMulti-device\nOperations can run on any of the supported devices (currently the CPU and the GPU). The framework is intended to be user-friendly, but still efficient to train and deploy models. The design of the framework itself is also conceptually simple."
  },
  {
    "objectID": "myblog/posts/polars-for-featureengineering/index.html",
    "href": "myblog/posts/polars-for-featureengineering/index.html",
    "title": "Polars for Feature Engineering",
    "section": "",
    "text": "Polars API are\nMost of the feature engineering task based on below 7 verbs\nimport polars as pl\ndf = pl.read_csv(\"StudentsPerformance.csv\")\ndf.head()\n\n\nshape: (5, 9)\n\n\n\nid\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\n\n\ni64\nstr\nstr\nstr\nstr\nstr\ni64\ni64\ni64\n\n\n\n\n1\n\"female\"\n\"group B\"\n\"bachelor's deg‚Ä¶\n\"standard\"\n\"none\"\n72\n72\n74\n\n\n2\n\"female\"\n\"group C\"\n\"some college\"\n\"standard\"\n\"completed\"\n69\n90\n88\n\n\n3\n\"female\"\n\"group B\"\n\"master's degre‚Ä¶\n\"standard\"\n\"none\"\n90\n95\n93\n\n\n4\n\"male\"\n\"group A\"\n\"associate's de‚Ä¶\n\"free/reduced\"\n\"none\"\n47\n57\n44\n\n\n5\n\"male\"\n\"group C\"\n\"some college\"\n\"standard\"\n\"none\"\n76\n78\n75"
  },
  {
    "objectID": "myblog/posts/polars-for-featureengineering/index.html#select-columns",
    "href": "myblog/posts/polars-for-featureengineering/index.html#select-columns",
    "title": "Polars for Feature Engineering",
    "section": "1. Select Columns",
    "text": "1. Select Columns\n\nSelecting 1 column\n\n\ndf.select(pl.col('gender')).head()\n\n\nshape: (5, 1)\n\n\n\ngender\n\n\nstr\n\n\n\n\n\"female\"\n\n\n\"female\"\n\n\n\"female\"\n\n\n\"male\"\n\n\n\"male\"\n\n\n\n\n\n\n\nSelecting two or more columns\n\n\ndf.select(pl.col(['gender', 'math score'])).head()\n\n\nshape: (5, 2)\n\n\n\ngender\nmath score\n\n\nstr\ni64\n\n\n\n\n\"female\"\n72\n\n\n\"female\"\n69\n\n\n\"female\"\n90\n\n\n\"male\"\n47\n\n\n\"male\"\n76\n\n\n\n\n\n\n\nSelecting all the columns\n\n\ndf.select(pl.col('*')).head()\n\n\nshape: (5, 9)\n\n\n\nid\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\n\n\ni64\nstr\nstr\nstr\nstr\nstr\ni64\ni64\ni64\n\n\n\n\n1\n\"female\"\n\"group B\"\n\"bachelor's deg‚Ä¶\n\"standard\"\n\"none\"\n72\n72\n74\n\n\n2\n\"female\"\n\"group C\"\n\"some college\"\n\"standard\"\n\"completed\"\n69\n90\n88\n\n\n3\n\"female\"\n\"group B\"\n\"master's degre‚Ä¶\n\"standard\"\n\"none\"\n90\n95\n93\n\n\n4\n\"male\"\n\"group A\"\n\"associate's de‚Ä¶\n\"free/reduced\"\n\"none\"\n47\n57\n44\n\n\n5\n\"male\"\n\"group C\"\n\"some college\"\n\"standard\"\n\"none\"\n76\n78\n75"
  },
  {
    "objectID": "myblog/posts/polars-for-featureengineering/index.html#create-columns",
    "href": "myblog/posts/polars-for-featureengineering/index.html#create-columns",
    "title": "Polars for Feature Engineering",
    "section": "2. Create Columns",
    "text": "2. Create Columns\n\nCreating a new column ‚Äúsum‚Äù by summing math score and reading score\n\n\ndf.with_columns(\n    (pl.col('math score') + pl.col('reading score')).alias('sum')\n).head()\n\n\nshape: (5, 10)\n\n\n\nid\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\nsum\n\n\ni64\nstr\nstr\nstr\nstr\nstr\ni64\ni64\ni64\ni64\n\n\n\n\n1\n\"female\"\n\"group B\"\n\"bachelor's deg‚Ä¶\n\"standard\"\n\"none\"\n72\n72\n74\n144\n\n\n2\n\"female\"\n\"group C\"\n\"some college\"\n\"standard\"\n\"completed\"\n69\n90\n88\n159\n\n\n3\n\"female\"\n\"group B\"\n\"master's degre‚Ä¶\n\"standard\"\n\"none\"\n90\n95\n93\n185\n\n\n4\n\"male\"\n\"group A\"\n\"associate's de‚Ä¶\n\"free/reduced\"\n\"none\"\n47\n57\n44\n104\n\n\n5\n\"male\"\n\"group C\"\n\"some college\"\n\"standard\"\n\"none\"\n76\n78\n75\n154"
  },
  {
    "objectID": "myblog/posts/polars-for-featureengineering/index.html#filter",
    "href": "myblog/posts/polars-for-featureengineering/index.html#filter",
    "title": "Polars for Feature Engineering",
    "section": "3. Filter",
    "text": "3. Filter\n\nSimple filtering\nselecting female students\n\n\ndf.filter(pl.col('gender')=='female').head()\n\n\nshape: (5, 9)\n\n\n\nid\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\n\n\ni64\nstr\nstr\nstr\nstr\nstr\ni64\ni64\ni64\n\n\n\n\n1\n\"female\"\n\"group B\"\n\"bachelor's deg‚Ä¶\n\"standard\"\n\"none\"\n72\n72\n74\n\n\n2\n\"female\"\n\"group C\"\n\"some college\"\n\"standard\"\n\"completed\"\n69\n90\n88\n\n\n3\n\"female\"\n\"group B\"\n\"master's degre‚Ä¶\n\"standard\"\n\"none\"\n90\n95\n93\n\n\n6\n\"female\"\n\"group B\"\n\"associate's de‚Ä¶\n\"standard\"\n\"none\"\n71\n83\n78\n\n\n7\n\"female\"\n\"group B\"\n\"some college\"\n\"standard\"\n\"completed\"\n88\n95\n92\n\n\n\n\n\n\n\nMultiple filtering\nselecting female students those belong to group B\n\n\ndf.filter(\n    (pl.col('gender')=='female') & \n    (pl.col('race/ethnicity')=='group B')\n).head()\n\n\nshape: (5, 9)\n\n\n\nid\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\n\n\ni64\nstr\nstr\nstr\nstr\nstr\ni64\ni64\ni64\n\n\n\n\n1\n\"female\"\n\"group B\"\n\"bachelor's deg‚Ä¶\n\"standard\"\n\"none\"\n72\n72\n74\n\n\n3\n\"female\"\n\"group B\"\n\"master's degre‚Ä¶\n\"standard\"\n\"none\"\n90\n95\n93\n\n\n6\n\"female\"\n\"group B\"\n\"associate's de‚Ä¶\n\"standard\"\n\"none\"\n71\n83\n78\n\n\n7\n\"female\"\n\"group B\"\n\"some college\"\n\"standard\"\n\"completed\"\n88\n95\n92\n\n\n10\n\"female\"\n\"group B\"\n\"high school\"\n\"free/reduced\"\n\"none\"\n38\n60\n50"
  },
  {
    "objectID": "myblog/posts/polars-for-featureengineering/index.html#join",
    "href": "myblog/posts/polars-for-featureengineering/index.html#join",
    "title": "Polars for Feature Engineering",
    "section": "4. Join",
    "text": "4. Join\n\ndf2 = pl.read_csv('LanguageScore.csv')\ndf.join(df2, on=\"id\").head()\n\n\nshape: (5, 10)\n\n\n\nid\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\nlanguage score\n\n\ni64\nstr\nstr\nstr\nstr\nstr\ni64\ni64\ni64\ni64\n\n\n\n\n1\n\"female\"\n\"group B\"\n\"bachelor's deg‚Ä¶\n\"standard\"\n\"none\"\n72\n72\n74\n74\n\n\n2\n\"female\"\n\"group C\"\n\"some college\"\n\"standard\"\n\"completed\"\n69\n90\n88\n67\n\n\n3\n\"female\"\n\"group B\"\n\"master's degre‚Ä¶\n\"standard\"\n\"none\"\n90\n95\n93\n34\n\n\n4\n\"male\"\n\"group A\"\n\"associate's de‚Ä¶\n\"free/reduced\"\n\"none\"\n47\n57\n44\n33\n\n\n5\n\"male\"\n\"group C\"\n\"some college\"\n\"standard\"\n\"none\"\n76\n78\n75\n75"
  },
  {
    "objectID": "myblog/posts/polars-for-featureengineering/index.html#concat",
    "href": "myblog/posts/polars-for-featureengineering/index.html#concat",
    "title": "Polars for Feature Engineering",
    "section": "Concat",
    "text": "Concat\n\ndf2 = df2.drop(\"id\")\npl.concat([df, df2], how=\"horizontal\").head()\n\n\nshape: (5, 10)\n\n\n\nid\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\nlanguage score\n\n\ni64\nstr\nstr\nstr\nstr\nstr\ni64\ni64\ni64\ni64\n\n\n\n\n1\n\"female\"\n\"group B\"\n\"bachelor's deg‚Ä¶\n\"standard\"\n\"none\"\n72\n72\n74\n74\n\n\n2\n\"female\"\n\"group C\"\n\"some college\"\n\"standard\"\n\"completed\"\n69\n90\n88\n67\n\n\n3\n\"female\"\n\"group B\"\n\"master's degre‚Ä¶\n\"standard\"\n\"none\"\n90\n95\n93\n34\n\n\n4\n\"male\"\n\"group A\"\n\"associate's de‚Ä¶\n\"free/reduced\"\n\"none\"\n47\n57\n44\n33\n\n\n5\n\"male\"\n\"group C\"\n\"some college\"\n\"standard\"\n\"none\"\n76\n78\n75\n75"
  },
  {
    "objectID": "myblog/posts/polars-for-featureengineering/index.html#group-by",
    "href": "myblog/posts/polars-for-featureengineering/index.html#group-by",
    "title": "Polars for Feature Engineering",
    "section": "5. Group By",
    "text": "5. Group By\nCount total elements for each race/ethnicity\n\ndf.group_by('race/ethnicity').count()\n\n\nshape: (5, 2)\n\n\n\nrace/ethnicity\ncount\n\n\nstr\nu32\n\n\n\n\n\"group B\"\n190\n\n\n\"group A\"\n89\n\n\n\"group D\"\n262\n\n\n\"group C\"\n319\n\n\n\"group E\"\n140"
  },
  {
    "objectID": "myblog/posts/polars-for-featureengineering/index.html#aggregate",
    "href": "myblog/posts/polars-for-featureengineering/index.html#aggregate",
    "title": "Polars for Feature Engineering",
    "section": "6. Aggregate",
    "text": "6. Aggregate\naverage math score for females and males\n\ndf.group_by('gender').agg(pl.col('math score').mean().alias('mean_score'))\n\n\nshape: (2, 2)\n\n\n\ngender\nmean_score\n\n\nstr\nf64\n\n\n\n\n\"male\"\n68.728216\n\n\n\"female\"\n63.633205"
  },
  {
    "objectID": "myblog/posts/polars-for-featureengineering/index.html#sort",
    "href": "myblog/posts/polars-for-featureengineering/index.html#sort",
    "title": "Polars for Feature Engineering",
    "section": "7. Sort",
    "text": "7. Sort\nsort the dataframe by math score\n\ndf.sort('math score',descending=True).head()\n\n\nshape: (5, 9)\n\n\n\nid\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\n\n\ni64\nstr\nstr\nstr\nstr\nstr\ni64\ni64\ni64\n\n\n\n\n150\n\"male\"\n\"group E\"\n\"associate's de‚Ä¶\n\"free/reduced\"\n\"completed\"\n100\n100\n93\n\n\n452\n\"female\"\n\"group E\"\n\"some college\"\n\"standard\"\n\"none\"\n100\n92\n97\n\n\n459\n\"female\"\n\"group E\"\n\"bachelor's deg‚Ä¶\n\"standard\"\n\"none\"\n100\n100\n100\n\n\n624\n\"male\"\n\"group A\"\n\"some college\"\n\"standard\"\n\"completed\"\n100\n96\n86\n\n\n626\n\"male\"\n\"group D\"\n\"some college\"\n\"standard\"\n\"completed\"\n100\n97\n99"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\n Back to top"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "No matching items\n\n Back to top"
  },
  {
    "objectID": "myblog/posts/activation-functions/index.html",
    "href": "myblog/posts/activation-functions/index.html",
    "title": "Choosing Activation Functions",
    "section": "",
    "text": "Dubey Singh Chaudhary(2022). Activation Functions in Deep Learning : A Comprehensive Survey and Benchmark"
  },
  {
    "objectID": "myblog/posts/activation-functions/index.html#most-common-activation-functions",
    "href": "myblog/posts/activation-functions/index.html#most-common-activation-functions",
    "title": "Choosing Activation Functions",
    "section": "Most Common Activation functions",
    "text": "Most Common Activation functions\nBelow are the most common activation function used in deep learning field :\n\nSigmoid\nTanh\nReLU\nLeaky ReLU\nSoftmax\nBinary Step\nIdentity\nGELU\nSwish\nMish\n\n\nSigmoid\nSigmoid activation function is also called the logistic function. It is a non-linear function which converts its input into a probability value between 0 and 1. Large negative values are converted towards 0 while large positive values are converted towards 1.\n\\(f(z) = \\frac{1}{1+e^{-z}}\\)\n\n\n\nTanh\n\n\n\nReLU\nReLU is a simple and robust choice\n\n\n\nSoftmax\n\n\nUnfortunately there is no right solution it all comes down to trying it out in practice and seeing what works."
  },
  {
    "objectID": "myblog/posts/activation-functions/index.html#activation-functions",
    "href": "myblog/posts/activation-functions/index.html#activation-functions",
    "title": "Choosing Activation Functions",
    "section": "Activation Functions",
    "text": "Activation Functions\nMost of activation functions are non-linear however we also use linear activation functions in neural networks. For example, we use linear activation function in the output layer of neural network model that solves a regression problem.\n\n\n\n\n\n\nLinear vs Non Linear Functions\n\n\n\nA linear function (called f) takes the input, z and returns the output, cz which is the multiplication of the input by the constant, c. Mathematically, this can be expressed as f(z) = cz. When c=1, the function returns the input as it is and no change is made to the input. The graph of a linear function is a single straight line.\nAny function that is not linear can be classified as a non-linear function. The graph of a non-linear function is not a single straight line. It can be a complex pattern or a combination of two or more linear components.\n\n\nThere is no universally best activation function. But there are some activation functions which are usually better than others.\n\n\n\nDubey Singh Chaudhary(2022). Activation Functions in Deep Learning : A Comprehensive Survey and Benchmark"
  },
  {
    "objectID": "myblog/posts/activation-functions/index.html#introduction",
    "href": "myblog/posts/activation-functions/index.html#introduction",
    "title": "Choosing Activation Functions",
    "section": "Introduction",
    "text": "Introduction\nA neural network has three types of layers : input layer that take raw input , hidden layer that take input from another layer and pass output to another layer, and finally output layer that make a prediction.\nInput layer has no computation performed so there is no activation function required. All hidden layers typically use the same activation function. The output layer will typically use different activation function from the hidden layer depending on the type of prediction required by the model.\nAn activation function should also be differentiable which means their first order derivative can be calculated for a given input value. This is required since neural network are trained using backpropagation algorithm which requires derivative of loss function in order to updates the weight of model."
  },
  {
    "objectID": "myblog/posts/activation-functions/index.html#contents",
    "href": "myblog/posts/activation-functions/index.html#contents",
    "title": "Choosing Activation Functions",
    "section": "",
    "text": "Need of Activation Function in Neural Networks\nActivation Functions\nMost Common Activation functions"
  },
  {
    "objectID": "myblog/posts/activation-functions/index.html#need-of-activation-function-in-neural-networks",
    "href": "myblog/posts/activation-functions/index.html#need-of-activation-function-in-neural-networks",
    "title": "Choosing Activation Functions",
    "section": "Need of Activation Functions in Neural Networks",
    "text": "Need of Activation Functions in Neural Networks\nThe objective of activation function in neural network is to add non-linearity so that it can learn complex patterns. Activation function introduces an additional step at each layer during the forward propagation, but its computation is worth. Here it is why-\nLet‚Äôs suppose we have a neural network without activation functions. In that case every neuron will be performing linear transformation on the inputs using the weights and biases. It‚Äôs because it doesn‚Äôt matter how many hidden layers we attach in the neural network; all layers will behave in the same way because the composition of two linear functions is a linear function itself.\nAlthough the neural network becomes simpler, hence learning any complex task is impossible, and our model would be just a linear regression model."
  },
  {
    "objectID": "myblog/posts/activation-functions/index.html#types-of-activation-functions",
    "href": "myblog/posts/activation-functions/index.html#types-of-activation-functions",
    "title": "Choosing Activation Functions",
    "section": "Types of Activation Functions",
    "text": "Types of Activation Functions\nMost of activation functions are non-linear however we also use linear activation functions in neural networks. For example, we use linear activation function in the output layer of neural network model that solves a regression problem.\n\n\n\n\n\n\nLinear vs Non Linear Functions\n\n\n\nA linear function (called f) takes the input, z and returns the output, cz which is the multiplication of the input by the constant, c. Mathematically, this can be expressed as f(z) = cz. When c=1, the function returns the input as it is and no change is made to the input. The graph of a linear function is a single straight line.\nAny function that is not linear can be classified as a non-linear function. The graph of a non-linear function is not a single straight line. It can be a complex pattern or a combination of two or more linear components.\n\n\nBelow are the most common activation function used in hidden layers :\n\nBinary Step Function\nLinear Activation Function\nNon - Linear Activation Functions\n\nSigmoid\nTanh\nReLU\n\nLeaky ReLU\nParametric ReLU\n\nELU\nGELU\nSwish\nSELU\nSoftmax\n\n\n\nBinary Step Function\nIn binary step function a threshold value decides that a neuron should be activated or not. Here the input fed to activation function is compared with threshold and if its greater than threshold neuron is activated otherwise it is deactivated which means output is not passed to the next hidden layer.\nMathematically a binary step function can be represented as\n\\[\nf(x) = \\begin{cases}\n      0 & for \\ x\\lt 0 \\\\\n      1 & for \\ x\\geq 0\n   \\end{cases}\n\\]\ncode for binary step function\n\ndef binary_step_function(x):\n    return np.where(x &lt; 0, 0, 1)\n\nplot for binary step function\n\nimport numpy as np \nimport matplotlib.pyplot as plt\n\n# Generate a range of values for x\nx_values = np.linspace(-5, 5, 1000)\n\n# Apply the binary step function to each value of x\ny_values = binary_step_function(x_values)\n\n# Plot the binary step function\nplt.plot(x_values, y_values, label='Binary Step Function')\nplt.title('Binary Step Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe gradient of step function is zero which causes hindrance in the backpropagation process as well as it can‚Äôt be use for multi-class classification problems,\n\n\n\n\nLinear Activation Function\nThis is also known as identity or no activation function where the activation is proportional to input.\nMathematically it can be represented as:\n\\[\nf(x)= x\n\\]\ncode for linear activation function\n\ndef linear_activation(x):\n    return x\n\nplot for linear activation function\n\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 10)\ny_values = linear_activation(x_values)\n\n# Plot the linear activation function\nplt.plot(x_values, y_values, label='Linear Activation Function')\nplt.title('Linear Activation Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBackpropagation can‚Äôt be used with linear activation function as the derivative of function is constant and has no relation to the input x.¬†All layers of the neural network will collapse into one if a linear activation function is used.\n\n\n\n\nNon-Linear Activation Functions\n\nSigmoid\nSigmoid activation function is also called the logistic function. It is a non-linear function which converts its input into a probability value between 0 and 1. Large negative values are converted towards 0 while large positive values are converted towards 1.\nMathematically it can be represented as:\n\\[\nf(x) = \\frac{1}{1+e^{-x}}\n\\]\ncode for sigmoid activation function\n\nimport numpy as np\n\ndef sigmoid(x):\n    return 1/(1+ np.exp(-x))\n\nplot for sigmoid activation function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 100) \nz = sigmoid(x)\n\nplt.plot(x, z) \nplt.title('Sigmoid Function')\nplt.xlabel('Input (x)') \nplt.ylabel(\"Output\") \nplt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)  \nplt.show() \n\n\n\n\n\n\n\n\nSigmoid is right choice where we have to predict the probability as an output. The function is differentiable and provide smooth gradient which mean no jumps in output values\n\n\n\n\n\n\nWarning\n\n\n\nSigmoid function suffers from vanishing gradient problem which makes learning difficult.\n\n\n\n\nTanh\nTanh or hyperbolic tangent is very similar to sigmoid activation function and even has same S-shape with difference in output range of -1 to 1. In Tanh larger the input (more positive) , the closer the output will be to 1.0 , whereas the smaller the input (more negative) , the closer the output will be to -1.0.\nMathematically it can be represented as:\n\\[\nf(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\n\\]\ncan also be written as :\n\\[\nf(x) = \\frac{e^{2x}-1}{e^{2x}+1}\n\\]\ncode for Tanh activation function\n\nimport numpy\n\ndef tanh(x):\n    return (np.exp(2*x) - 1) / (np.exp(2*x) + 1)\n\nplot for Tanh activation function\n\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 1000)\ny_values = tanh(x_values)\n# Plot the tanh activation function\nplt.plot(x_values, y_values, label='tanh Activation Function')\nplt.title('tanh Activation Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIt also faces the vanishing gradient issue similar to sigmoid activation function.\n\n\n\n\nReLU\nReLU stands for rectified linear unit. ReLU gives an impression of linear activation function but it has derivative function and allows for backpropagation while simultaneously making it computationally efficient. ReLU function doesn‚Äôt activate all the neuron at same time. The neuron will be deactivated only when the output of the linear transformation is less than 0.\nReLU is a simple and robust choice.\nMathematically it can be represented as:\n\\[\nf(x) = max(0,x)\n\\]\ncode for ReLU activation function\n\nimport numpy as np\n\ndef relu(x):\n    return np.maximum(0, x)\n\nplot for ReLU activation function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 1000)\ny_values = relu(x_values)\n\n# Plot the ReLU activation function\nplt.plot(x_values, y_values, label='ReLU Activation Function')\nplt.title('Rectified Linear Unit (ReLU) Activation Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nReLU accelerates the convergence of gradient descent towards the global minimum of the loss function due to its linear, non-saturating property.\n\n\n\n\n\n\nNote\n\n\n\nThe ReLU is not differentiable at a singular point x = 0, but we can still use what are known as sub-derivatives in backpropagation algorithm. The usual derivative of a ReLU is actually a sub-derivative to be precise. We use what is called sub-gradient descent approach to optimize such functions.\n\n\nDying ReLU Problem\nThe derivative of ReLU activation is given as :\n\\[\nf'(x) = \\begin{cases}\n      1 & for \\ x\\geq 0 \\\\\n      0 & for \\ x\\lt 0\n   \\end{cases}\n\\]\nHere the negative values makes the gradient value zero. Due to this reason, during the backpropagation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated.¬†\n\n\nLeaky ReLU\nLeaky ReLU is improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area.\nMathematically it can be represented as:\n\\[\nf(x) = max(0.01x, x)\n\\]\ncode for Leaky ReLU activation function\n\nimport numpy as np\n\ndef leaky_relu(x):\n    return np.maximum(0.01*x, x)\n\nplot for Leaky ReLU activation function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 1000)\ny_values = leaky_relu(x_values)\n\n# Plot the ReLU activation function\nplt.plot(x_values, y_values, label='Leaky ReLU Activation Function')\nplt.title('Leaky Rectified Linear Unit (Leaky ReLU) Activation Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nThe advantages of Leaky ReLU are same as of ReLU, in addition to the fact that it does enable backpropagation, even for negative input values.¬†However it suffers from inconsistent predictions for negative input values.\n\n\nParametric ReLU\nParametric ReLU or PReLU is another variant of ReLU that aims to solve the problem of dying ReLU and Leaky ReLU (inconsistent predictions for negative input values). So the authors of the paper behind PReLU thought why not let the a in ax for x&lt;0 (in LeakyReLU) get learned.\nAnd here is the catch: if all the channels share the same a that gets learned, it is called channel-shared PReLU. But if each channel learn their own a, it is called channel-wise PReLU.\nSo what if ReLU or LeakyReLU was better for that problem? That is upto the model to learn:\n\nif a is/are learned as 0 -&gt; PReLU becomes ReLu\nif a is/are learned as small number -&gt; PReLU becomes LeakyReLU\n\nMathematically it can be represented as:\n\\[\nf(x) = max(ax, x)\n\\]Where ‚Äúa‚Äù is the slope parameter for negative values.\ncode for Parametric ReLU activation function\n\nimport numpy as np\n\nclass PReLU:\n    def __init__(self, alpha=0.01):\n        self.alpha = alpha\n\n    def __call__(self, x):\n        return np.maximum(self.alpha * x, x)\n\nplot for Parametric ReLU activation function\n\nx_values = np.linspace(-5, 5, 100)\n\n# Create a PReLU instance with an initial slope (alpha)\nprelu = PReLU(alpha=0.1)\n\n# Calculate corresponding output values using PReLU\nprelu_values = prelu(x_values)\n\n# Plot the PReLU function\nplt.plot(x_values, prelu_values, label='PReLU Function (alpha=0.1)')\nplt.title('Parametric ReLU (PReLU) Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nIn leaky ReLU alpha is hyper parameter where as in Parametric ReLU it is a parameter.\n\n\n\nELU\nExponential Linear Unit, or ELU for short, is also a variant of ReLU that modifies the slope of the negative part of the function. ELU uses a log curve to define the negative values unlike the leaky ReLU and Parametric ReLU functions with a straight line.¬†\nMathematically it can be represented as:\n\\[\nf(x) = \\begin{cases}\n      x & for \\ x\\geq 0 \\\\\n      \\alpha(e^x-1) & for \\ x\\lt 0\n   \\end{cases}\n\\]\ncode for ELU activation function\n\nimport numpy as np\n\ndef elu(x, alpha=1.0):\n    return np.where(x &gt;= 0, x, alpha * (np.exp(x) - 1))\n\nplot for ELU activation function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 100)\nelu_values = elu(x_values)\n\n# Plot the ELU function\nplt.plot(x_values, elu_values, label='ELU Function (alpha=1.0)')\nplt.title('Exponential Linear Unit (ELU) Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nELU becomes smooth slowly until its output equal to -Œ± whereas ReLU sharply smoothes. It also avoids dead ReLU problem by introducing log curve for negative values of input.\nHowever the computational time increases because of the exponential operation.\n\n\nGELU\nGaussian Error Linear Unit or GELU activation function is compatible with BERT, ROBERTa, ALBERT, and other top NLP models. This activation function is motivated by combining properties from dropout, zoneout, and ReLUs.¬†\nMathematically it can be represented as:\n\\[\nf(x) = x\\Phi(x)\n\\]\n\\[\nf(x) = 0.5x \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715x^3\\right)\\right)\\right)\n\\]\nwhere \\(\\Phi(x)\\) is the cumulative distribution function of Gaussian distribution.\ncode for GELU activation function\n\nimport numpy as np\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n\nplot for GELU activation function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 100)\ngelu_values = gelu(x_values)\n\n# Plot the GELU function\nplt.plot(x_values, gelu_values, label='GELU Function')\nplt.title('Gaussian Error Linear Unit (GELU) Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nGELU non linearity is better than ReLU and ELU activations and finds performance improvements across all tasks in domains of computer vision, natural language processing, and speech recognition.\n\n\nSwish\nIn 2018 the paper Searching for activation functions by researchers at Google Brain team proposes a novel activation function called Swish, which was discovered using a Neural Architecture Search (NAS) approach and showed significant improvement in performance compared to standard activation functions like ReLU or Leaky ReLU.\nSwish consistently matches or outperforms ReLU activation function on deep networks applied to various challenging domains such as image classification machine translation etc.¬†\nMathematically it can be represented as:\n\\[\nf(x) = \\frac{x}{1 + e^{-\\beta x}}\n\\]\nwhere \\(\\beta\\) is either a constant or trainable parameter depending on the model.\nit can also written in terms of sigmoid activation function\n\\[\nf(x) = x*sigmoid(\\beta x)\n\\]\nat \\(\\beta=1\\) the function becomes equivalent to sigmoid linear unit or SiLU.\n\\[\nf(x) = \\frac{x}{1+ e^{-x}}\n\\]\ncode for Swish activation function\n\nimport numpy as np\n\ndef swish(x,beta = 1.0):\n    return x*(1/(1+np.exp(-beta*x)))\n\nplot for Swish activation function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 100)\nswish_values = swish(x_values)\n\n# Plot the Swish function\nplt.plot(x_values, swish_values, label='Swish Function (beta=1.0)')\nplt.title('Swish Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSELU\nScaled Exponential Linear Unit or SELU was defined in self-normalizing networks and takes care of internal normalization which means each layer preserves the mean and variance from the previous layers. SELU enables this normalization by adjusting the mean and variance.¬†\nMathematically it can be represented as:\n\\[\n\\begin{equation} f(\\alpha, x) = \\lambda \\begin{cases} \\alpha(e^x - 1), & \\text{if}\\ x \\lt 0 \\\\\nx, & \\text{otherwise} \\\\ \\end{cases} \\end{equation}\n\\]\nSELU has values of Œ± and Œª predefined.\ncode for SELU activation function\n\nimport numpy as np\n\ndef selu(x, alpha=1.67326, lambda_=1.0507):\n    return lambda_ * np.where(x &gt; 0, x, alpha * (np.exp(x) - 1))\n\nplot for SELU activation function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 100)\nselu_values = selu(x_values)\n\n# Plot the SELU function\nplt.plot(x_values, selu_values, label='SELU Function (alpha=1.67326, lambda=1.0507)')\nplt.title('Scaled Exponential Linear Unit (SELU) Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nThe main advantage of SELU over ReLU is internal normalization is faster than external normalization, which means the network converges faster.\n\nSELU is a relatively newer activation function and needs more papers on architectures such as CNNs and RNNs, where it is comparatively explored.\n\n\n\nSoftmax\nSoftmax is generalization of sigmoid activation function which can be used for multi-class classification. The softmax function squashes the outputs of each unit to be between 0 and 1, just like a sigmoid function. But it also divides each output such that the total sum of the outputs is equal to 1.\nAssume that you have three classes, meaning that there would be three neurons in the output layer. Now, suppose that your output from the neurons is [1.8, 0.9, 0.68]. Applying the softmax function over these values to give a probabilistic view will result in the following outcome: [0.58, 0.23, 0.19].¬†\n\\[\nf(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K\n\\]\ncode for Softmax activation function\n\nimport numpy as np\n\ndef softmax(z):\n    exp_z = np.exp(z)\n    return exp_z / np.sum(exp_z)"
  },
  {
    "objectID": "myblog/posts/activation-functions/index.html#choosing-the-right-activation-function",
    "href": "myblog/posts/activation-functions/index.html#choosing-the-right-activation-function",
    "title": "Choosing Activation Functions",
    "section": "Choosing the right activation function",
    "text": "Choosing the right activation function\nBelow are some rule of thumb for choosing the right activation function :\n\nActivation function in output layer depends on type of prediction problem.\n\nBinary Classification - Sigmoid\nMulti-class Classification - Softmax\nRegression - Linear\nMultilabel Classification‚ÄîSigmoid\n\nActivation function in hidden layer\n\nStart with using ReLU function and then move over to other activation functions if ReLU doesn‚Äôt provide optimum results.\nDon‚Äôt use sigmoid and Tanh activation functions in hidden layer as they can cause vanishing gradient problem.\nSwish function is used in neural networks having a depth greater than 40 layers."
  },
  {
    "objectID": "myblog/posts/activation-functions/index.html#footnotes",
    "href": "myblog/posts/activation-functions/index.html#footnotes",
    "title": "Choosing Activation Functions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://arxiv.org/pdf/1710.05941.pdf?ref=blog.paperspace.com‚Ü©Ô∏é"
  },
  {
    "objectID": "myblog/posts/confidence-interval/index.html",
    "href": "myblog/posts/confidence-interval/index.html",
    "title": "Understanding Confidence Interval",
    "section": "",
    "text": "Imagine you want to estimate the average height of all the adults. It‚Äôs very impractical to measure average height of all adults. So you take a sample of 100 adults and measure their average height to be 5‚Äô7 inches. Now you know that this is just a estimate and if you took another sample, you might get different average. So how can you express the uncertainty around your estimate?"
  },
  {
    "objectID": "myblog/posts/confidence-interval/index.html#confidence-intervals",
    "href": "myblog/posts/confidence-interval/index.html#confidence-intervals",
    "title": "Understanding Confidence Interval",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA confidence interval is a range of values, derived from the sample data, that is likely to contain the true value of an unknown population parameter. For example you can say you are 95% confident that the average height of all adults is between 5‚Äô5 and 5‚Äô9 inches. This is 95% confidence interval."
  },
  {
    "objectID": "myblog/posts/confidence-interval/index.html#how-to-calculate",
    "href": "myblog/posts/confidence-interval/index.html#how-to-calculate",
    "title": "Understanding Confidence Interval",
    "section": "How to calculate",
    "text": "How to calculate\n\\[\n\\bar{x} \\pm z\\cdot \\frac{s}{\\sqrt{n}}\n\\]\nwhere \\(\\bar{x}\\) = sample mean, \\(s\\) = sample standard deviation, \\(z\\) = confidence level value and \\(n\\) =sample size\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Example data\ndata = np.random.normal(loc=5.8, scale=0.5, size=100) \n\n# Calculate mean and standard deviation of the sample\nmean_value = np.mean(data)\nstd_dev = np.std(data)\n\n# Calculate the standard error of the mean (SEM)\nsem = std_dev / np.sqrt(len(data))\n\n# Calculate the margin of error for a 95% confidence interval\nmargin_of_error = norm.ppf(0.975) * sem\n\n# Calculate the confidence interval\nconfidence_interval = (mean_value - margin_of_error, mean_value + margin_of_error)\n\n# Plotting the data\nplt.hist(data, bins=20, alpha=0.7, color='#39729E', edgecolor='black', label='Sample Data')\n\n# Plotting the confidence interval\nplt.axvline(x=confidence_interval[0], color='grey', linestyle='--', label='95% Confidence Interval')\nplt.axvline(x=confidence_interval[1], color='grey', linestyle='--')\n\n# Adding labels and title\nplt.xlabel('Height')\nplt.ylabel('Frequency')\nplt.title('95% Confidence Interval for Adult Height')\n\n# Adding legend\nplt.legend()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "myblog/posts/confidence-interval/index.html#interpretation",
    "href": "myblog/posts/confidence-interval/index.html#interpretation",
    "title": "Understanding Confidence Interval",
    "section": "Interpretation",
    "text": "Interpretation\nSaying ‚ÄúI am 95% confident that the average height is between 5‚Äô5 and 5‚Äô9 inches‚Äù doesn‚Äôt mean that there is 95% chance that the true average falls in this range. Instead it means that if you were to take many samples and compute a 95% confidence interval for each one, about 95% of these intervals would contain the true average height."
  },
  {
    "objectID": "myblog/posts/confidence-interval/index.html#choosing-ci",
    "href": "myblog/posts/confidence-interval/index.html#choosing-ci",
    "title": "Understanding Confidence Interval",
    "section": "Choosing CI",
    "text": "Choosing CI\nIn order to choose confidence interval you should look into consequences of being wrong, if consequences are severe you might choose 99% CI else 95% is very common.\n\nHigher CI level results in wider intervals and vice versa. So there is always a trade off between precision and confidence."
  },
  {
    "objectID": "myblog/posts/confidence-interval/index.html#limitations",
    "href": "myblog/posts/confidence-interval/index.html#limitations",
    "title": "Understanding Confidence Interval",
    "section": "Limitations",
    "text": "Limitations\nCI assumes that your data is normally distributed but which may not always be the case."
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html",
    "href": "myblog/posts/statistical-learning-theory/index.html",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "In data science problems , we usually need to :\n\nMake a decision\nTake an action\nProduce some output\n\nAlso we have some evaluation criterion\n\n\nAn action is the generic term for what is produced by our system.\nExamples of Actions\n\nProduce a 0/1 classification\nReject hypothesis\n\n\nTaking machine translation example the input could be an entire paragraph text in French and output could be corresponding translation in English is the action we will take producing the translation.\n\n\n\n\nWe need to evaluate the action in some way and the goal of decision theory is about finding ‚ÄúOptimal‚Äù actions under various definitions of optimality.\nExamples of Evaluation criteria\n\nIs classification correct?\nDoes text transcription exactly match the spoken words?\n\n\n\n\nFirst two steps to formalizing a problem :\n\nDefine the action space\nspecify the evaluation criterion\n\n\nFormalization may evolve gradually, as you understand the problem better.\n\n\n\n\nMost of problems have an extra piece which goes by various names :\n\nInputs in Machine Learning\nCovariates in Statistics\n\nExamples of inputs:\n\nA picture we get to determine whether its a cat or not.\nUser past history and profile whether he/she is going to churn or not.\nA search query\n\n\n\n\nAlso known by other name as Outcomes or Label. Its the right answer or what actually happens. Inputs are often paired with outputs or outcomes or labels.\nExamples of inputs:\n\nWhether or not the picture actually contains an animal.\nUser churned or not\n\n\nIn case of self driving car outcomes is not literally the right or wrong answer‚Ä¶ its just an action you want to take for example you press the accelerator and steer the steering wheel and outcomes is how it moved or where is the position of the car in the world.\n\n\n\n\nMany problems domain can be formalized as follows :\n\nObserve an input x\nTake an action a\nObserve the outcome y\nEvaluate action in relation to the outcome : l(a,y)\n\n\n\n\n\n\n\nNote\n\n\n\nOutcome y is often independent of action a but this is not always the case. For example in case of self driving the action you take affects the outcomes but in case of churn the outcome won‚Äôt change based on input. Basically you just predict and keep it to yourself and don‚Äôt act on it.\n\n\n\n\n\n\n\nThe three spaces :\n\nInput space \\(X\\)\nAction space \\(A\\)\nOutput space \\(Y\\)\n\n\n\n\n\n\n\nConcept Check\n\n\n\nWhats are the spaces for linear regression?\nInput space : \\(\\mathbb{R}^d\\) , Action space : \\(\\mathbb{R}\\) , Output space : \\(\\mathbb{R}\\)\nWhat are the spaces for logistic regression ?\nInput space: \\(\\mathbb{R}^d\\) , Action space: Probability between 0 & 1, Output space: [0,1]\n\n\n\n\n\nA decision function (or prediction function) gets input x \\(\\in X\\) and produces an action a \\(\\in A\\) :\n\\[\nf: X \\rightarrow A\n\\]\n\n\n\nA loss function evaluates an action in the context of the outcome \\(y\\).\n\\[\nl: A \\times Y \\rightarrow R\n(a, y) \\rightarrow l(a, y)\n\\]\n\n\n\nFirst two steps to formalizing the problem:\n\nDefining the action space (i.e.¬†set of all possible actions)\nspecifying the evaluation criterion\n\nFor example when a stakeholder ask the data scientist to solve a problem then she/he :\n\nmay have an opinion on what the action space should be, and\nhopefully has an opinion on the evaluation criterion, but\nhe/she really cares about your producing a good decision function\n\n\n\n\n\nLoss function \\(l\\) evaluates a single action\nHow to evaluate the decision function as whole?\n\nIn order to deploy a decision function stake holder needs to know the score or assessment of how well its going to perform. So the standard framework called Statistical Learning Theory gives the recipe for that.\n\n\n\n\nSLT helps in providing the average performance that your decision function will get.\n\n\n\nAssumption in SLT: Action has no effect on the output.\nAssume there is a data generating distribution \\(P_{X \\times Y}\\).\nAll input/output pairs \\((x,y)\\) are generated i.i.d from \\(P_{X \\times Y}\\).\ni.i.d means ‚Äúindependent and identically distributed‚Äù practically it means\n\nno covariate shift\nno concept drift\n\nWant decision function \\(f(x)\\) that generally ‚Äúdoes well on average‚Äù:\n\n\\(l(f(x),y)\\) is usually small\n\n\nHow do we are going to formalize this ?\n\n\n\n\nThe risk is just a fancy name for expected loss on a new sample \\((x,y)\\) drawn randomly from \\(P_{x \\times y}\\).\nDefinition:\nThe risk of a decision function \\(f : X \\rightarrow A\\) is\n\\[\nR(f) = \\mathbb{E}[l(f(x),y)]\n\\]\n\nRisk function cannot be computed since we don‚Äôt know \\(P_{x \\times y}\\), we cannot compute the expectation. But we can estimate it‚Ä¶\n\n\n\n\nThere is notion of Bayes Decision Function which is the decision function which minimizes the risk. So its the best possible decision function when evaluated in terms of expected loss on randomly chosen data point.\nDefinition :\nA Bayes decision function \\(f^* : X \\rightarrow A\\) is a function that achieves the minimal risk among all possible functions:\n\\[\nf^* = argmin R(f)\n\\]\nwhere the minimum is taken over all functions from \\(X\\) to \\(A\\).\n\n\n\n\n\n\nBayes Risk\n\n\n\nThe risk of a Bayes decision function is called the Bayes Risk.\n\n\nA Bayes decision function is often called the target function, since it is the best decision function we can possibly produce.\n\n\n\n\n\n\nspaces : \\(A = Y = R\\)\nsquare loss:\n\n\\[\nl(a,y) = (a-y)^2\n\\]\n\nRisk : (which is expected value of loss on some sample)\n\n\\[\nR(f) = \\mathbb{E}[(f(x),y)^2]\n\\]\n\ntarget function:\n\n\\[\nf^*(x) = \\mathbb{E}[y|x]\n\\]\n\n\n\n\nspaces : $A = Y = {0, 1,‚Ä¶.,K-1}\n0-1 loss\n\n\\[\nl(a,y) = 1(a \\neq y):=  \\begin{cases}\n  1 \\ if \\ a \\neq y \\\\\n  0 \\ otherwise\n\\end{cases}\n\\]\n\nrisk is misclassification error rate\n\n\\[\nR(f) = \\mathbb{E}[1(f(x) \\neq y] = 0.P(f(x) = y) + 1.P(f(x) \\neq y)\n= P(f(x) \\neq y)\n\\]\n\n1 in parentheis is indicator function that evaluates to 1 if thing inside parenthesis is true and 0 otherwise.\n\nThe risk of decision function is probability of error.\n\ntarget function is the assignment to the most likely class\n\n\\[\nf^*(x) = argmax P(y = k |x)\n\\]\nk is the prediction and y is the output and predict the class that has maximum probability.\n\n\n\n\n\n\nBut we can‚Äôt compute the risk !\n\n\n\n\nCan‚Äôt compute \\(R(f) = \\mathbb{E}l(f(x),y)\\) because we don‚Äôt know \\(P_{x \\times y}\\) because no one tells us the probability of distribution describing the real world.\nOne thing we can do in data science is assume we have some sample data.\n\nLet \\(D_n = ((x_1, y_1),....,(x_n, y_n))\\) be drawn i.i.d from \\(P_{x \\times y}\\)\n\nLet‚Äôs draw some inspiration from the Strong law of large numbers:\nif \\(z, z_1,...Z_n\\) are i.i.d with expected value \\(Ez\\) then\n\n\\[\n\\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{i=1}^{n} Z_{i} = \\mathbb{E}z\n\\]\nwith probability 1.\nBasically with enough data we can converge to property of distribution as whole.\n\n\n\n\n\n\nSo risk is expected loss, empirical risk is approximation of risk.\nLet \\(D_n = ((x_1, y_1),...,(x_n, y_n))\\) be drawn i.i.d from \\(P_{x \\times y}\\)\nDefinition: The empirical risk of \\(f: X \\rightarrow A\\) with respect to \\(D_n\\) is\n\\[\n\\hat{R}_{n}(f) = \\frac{1}{n} \\sum_{i=1}^{n} l(f(x_i), y_i)\n\\]\n\nThe loss are random because data points on which is calculated is random.\n\nBy Strong law of large numbers, the empirical risk converges to actual risk of the function.\n\\[\n\\lim_{n\\to\\infty} \\hat{R}_{n}(f) = R(f)\n\\] almost surely.\n\n\nWe want risk minimizer, is empirical risj minimizer close enough?\nDefinition:\nA function \\(\\hat{f}\\) is an empirical risk minimizer if\n\\[\n\\hat{f} = argmin \\hat{R}_{n}(f),\n\\]\nwhere the minimum is take over all functions\n\n\nERM led to a function \\(f\\) that just memorized the data. Then how can we spread information or generalize from training inputs to new inputs? We need to smooth things out somehow. So the one approach is Constrained ERM. In constrained ERM instead of minimizing empirical risk over all decision functions, constrain to particular subset called a Hypothesis Space.\n\n\n\n\n\nA hypothesis space \\(F\\) is a set of functions mapping \\(X \\rightarrow A\\). It is the collection of decision function we are considering.\n\n\n\nEmpirical Risk Minimization (ERM) in \\(F\\) is\n\\[\n\\hat{f}_{n} = argmin \\frac{1}{n} \\sum_{i=1}^{n} l(f(x_i), y_i)\n\\]"
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#actions",
    "href": "myblog/posts/statistical-learning-theory/index.html#actions",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "An action is the generic term for what is produced by our system.\nExamples of Actions\n\nProduce a 0/1 classification\nReject hypothesis\n\n\nTaking machine translation example the input could be an entire paragraph text in French and output could be corresponding translation in English is the action we will take producing the translation."
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#evaluation-criterion",
    "href": "myblog/posts/statistical-learning-theory/index.html#evaluation-criterion",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "We need to evaluate the action in some way and the goal of decision theory is about finding ‚ÄúOptimal‚Äù actions under various definitions of optimality.\nExamples of Evaluation criteria\n\nIs classification correct?\nDoes text transcription exactly match the spoken words?"
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#formalizing-a-business-problem",
    "href": "myblog/posts/statistical-learning-theory/index.html#formalizing-a-business-problem",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "First two steps to formalizing a problem :\n\nDefine the action space\nspecify the evaluation criterion\n\n\nFormalization may evolve gradually, as you understand the problem better."
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#input",
    "href": "myblog/posts/statistical-learning-theory/index.html#input",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "Most of problems have an extra piece which goes by various names :\n\nInputs in Machine Learning\nCovariates in Statistics\n\nExamples of inputs:\n\nA picture we get to determine whether its a cat or not.\nUser past history and profile whether he/she is going to churn or not.\nA search query"
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#output",
    "href": "myblog/posts/statistical-learning-theory/index.html#output",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "Also known by other name as Outcomes or Label. Its the right answer or what actually happens. Inputs are often paired with outputs or outcomes or labels.\nExamples of inputs:\n\nWhether or not the picture actually contains an animal.\nUser churned or not\n\n\nIn case of self driving car outcomes is not literally the right or wrong answer‚Ä¶ its just an action you want to take for example you press the accelerator and steer the steering wheel and outcomes is how it moved or where is the position of the car in the world."
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#typical-sequence-of-events",
    "href": "myblog/posts/statistical-learning-theory/index.html#typical-sequence-of-events",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "Many problems domain can be formalized as follows :\n\nObserve an input x\nTake an action a\nObserve the outcome y\nEvaluate action in relation to the outcome : l(a,y)\n\n\n\n\n\n\n\nNote\n\n\n\nOutcome y is often independent of action a but this is not always the case. For example in case of self driving the action you take affects the outcomes but in case of churn the outcome won‚Äôt change based on input. Basically you just predict and keep it to yourself and don‚Äôt act on it."
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#formalization",
    "href": "myblog/posts/statistical-learning-theory/index.html#formalization",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "The three spaces :\n\nInput space \\(X\\)\nAction space \\(A\\)\nOutput space \\(Y\\)\n\n\n\n\n\n\n\nConcept Check\n\n\n\nWhats are the spaces for linear regression?\nInput space : \\(\\mathbb{R}^d\\) , Action space : \\(\\mathbb{R}\\) , Output space : \\(\\mathbb{R}\\)\nWhat are the spaces for logistic regression ?\nInput space: \\(\\mathbb{R}^d\\) , Action space: Probability between 0 & 1, Output space: [0,1]\n\n\n\n\n\nA decision function (or prediction function) gets input x \\(\\in X\\) and produces an action a \\(\\in A\\) :\n\\[\nf: X \\rightarrow A\n\\]\n\n\n\nA loss function evaluates an action in the context of the outcome \\(y\\).\n\\[\nl: A \\times Y \\rightarrow R\n(a, y) \\rightarrow l(a, y)\n\\]\n\n\n\nFirst two steps to formalizing the problem:\n\nDefining the action space (i.e.¬†set of all possible actions)\nspecifying the evaluation criterion\n\nFor example when a stakeholder ask the data scientist to solve a problem then she/he :\n\nmay have an opinion on what the action space should be, and\nhopefully has an opinion on the evaluation criterion, but\nhe/she really cares about your producing a good decision function\n\n\n\n\n\nLoss function \\(l\\) evaluates a single action\nHow to evaluate the decision function as whole?\n\nIn order to deploy a decision function stake holder needs to know the score or assessment of how well its going to perform. So the standard framework called Statistical Learning Theory gives the recipe for that."
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#statistical-learning-theory",
    "href": "myblog/posts/statistical-learning-theory/index.html#statistical-learning-theory",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "SLT helps in providing the average performance that your decision function will get.\n\n\n\nAssumption in SLT: Action has no effect on the output.\nAssume there is a data generating distribution \\(P_{X \\times Y}\\).\nAll input/output pairs \\((x,y)\\) are generated i.i.d from \\(P_{X \\times Y}\\).\ni.i.d means ‚Äúindependent and identically distributed‚Äù practically it means\n\nno covariate shift\nno concept drift\n\nWant decision function \\(f(x)\\) that generally ‚Äúdoes well on average‚Äù:\n\n\\(l(f(x),y)\\) is usually small\n\n\nHow do we are going to formalize this ?"
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#the-risk-functional",
    "href": "myblog/posts/statistical-learning-theory/index.html#the-risk-functional",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "The risk is just a fancy name for expected loss on a new sample \\((x,y)\\) drawn randomly from \\(P_{x \\times y}\\).\nDefinition:\nThe risk of a decision function \\(f : X \\rightarrow A\\) is\n\\[\nR(f) = \\mathbb{E}[l(f(x),y)]\n\\]\n\nRisk function cannot be computed since we don‚Äôt know \\(P_{x \\times y}\\), we cannot compute the expectation. But we can estimate it‚Ä¶"
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#the-bayes-decision-function",
    "href": "myblog/posts/statistical-learning-theory/index.html#the-bayes-decision-function",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "There is notion of Bayes Decision Function which is the decision function which minimizes the risk. So its the best possible decision function when evaluated in terms of expected loss on randomly chosen data point.\nDefinition :\nA Bayes decision function \\(f^* : X \\rightarrow A\\) is a function that achieves the minimal risk among all possible functions:\n\\[\nf^* = argmin R(f)\n\\]\nwhere the minimum is taken over all functions from \\(X\\) to \\(A\\).\n\n\n\n\n\n\nBayes Risk\n\n\n\nThe risk of a Bayes decision function is called the Bayes Risk.\n\n\nA Bayes decision function is often called the target function, since it is the best decision function we can possibly produce."
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#examples",
    "href": "myblog/posts/statistical-learning-theory/index.html#examples",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "spaces : \\(A = Y = R\\)\nsquare loss:\n\n\\[\nl(a,y) = (a-y)^2\n\\]\n\nRisk : (which is expected value of loss on some sample)\n\n\\[\nR(f) = \\mathbb{E}[(f(x),y)^2]\n\\]\n\ntarget function:\n\n\\[\nf^*(x) = \\mathbb{E}[y|x]\n\\]\n\n\n\n\nspaces : $A = Y = {0, 1,‚Ä¶.,K-1}\n0-1 loss\n\n\\[\nl(a,y) = 1(a \\neq y):=  \\begin{cases}\n  1 \\ if \\ a \\neq y \\\\\n  0 \\ otherwise\n\\end{cases}\n\\]\n\nrisk is misclassification error rate\n\n\\[\nR(f) = \\mathbb{E}[1(f(x) \\neq y] = 0.P(f(x) = y) + 1.P(f(x) \\neq y)\n= P(f(x) \\neq y)\n\\]\n\n1 in parentheis is indicator function that evaluates to 1 if thing inside parenthesis is true and 0 otherwise.\n\nThe risk of decision function is probability of error.\n\ntarget function is the assignment to the most likely class\n\n\\[\nf^*(x) = argmax P(y = k |x)\n\\]\nk is the prediction and y is the output and predict the class that has maximum probability.\n\n\n\n\n\n\nBut we can‚Äôt compute the risk !\n\n\n\n\nCan‚Äôt compute \\(R(f) = \\mathbb{E}l(f(x),y)\\) because we don‚Äôt know \\(P_{x \\times y}\\) because no one tells us the probability of distribution describing the real world.\nOne thing we can do in data science is assume we have some sample data.\n\nLet \\(D_n = ((x_1, y_1),....,(x_n, y_n))\\) be drawn i.i.d from \\(P_{x \\times y}\\)\n\nLet‚Äôs draw some inspiration from the Strong law of large numbers:\nif \\(z, z_1,...Z_n\\) are i.i.d with expected value \\(Ez\\) then\n\n\\[\n\\lim_{n\\to\\infty} \\frac{1}{n} \\sum_{i=1}^{n} Z_{i} = \\mathbb{E}z\n\\]\nwith probability 1.\nBasically with enough data we can converge to property of distribution as whole."
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#the-empirical-risk-functional",
    "href": "myblog/posts/statistical-learning-theory/index.html#the-empirical-risk-functional",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "So risk is expected loss, empirical risk is approximation of risk.\nLet \\(D_n = ((x_1, y_1),...,(x_n, y_n))\\) be drawn i.i.d from \\(P_{x \\times y}\\)\nDefinition: The empirical risk of \\(f: X \\rightarrow A\\) with respect to \\(D_n\\) is\n\\[\n\\hat{R}_{n}(f) = \\frac{1}{n} \\sum_{i=1}^{n} l(f(x_i), y_i)\n\\]\n\nThe loss are random because data points on which is calculated is random.\n\nBy Strong law of large numbers, the empirical risk converges to actual risk of the function.\n\\[\n\\lim_{n\\to\\infty} \\hat{R}_{n}(f) = R(f)\n\\] almost surely.\n\n\nWe want risk minimizer, is empirical risj minimizer close enough?\nDefinition:\nA function \\(\\hat{f}\\) is an empirical risk minimizer if\n\\[\n\\hat{f} = argmin \\hat{R}_{n}(f),\n\\]\nwhere the minimum is take over all functions\n\n\nERM led to a function \\(f\\) that just memorized the data. Then how can we spread information or generalize from training inputs to new inputs? We need to smooth things out somehow. So the one approach is Constrained ERM. In constrained ERM instead of minimizing empirical risk over all decision functions, constrain to particular subset called a Hypothesis Space."
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#hypothesis-spaces",
    "href": "myblog/posts/statistical-learning-theory/index.html#hypothesis-spaces",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "A hypothesis space \\(F\\) is a set of functions mapping \\(X \\rightarrow A\\). It is the collection of decision function we are considering."
  },
  {
    "objectID": "myblog/posts/statistical-learning-theory/index.html#constrained-empirical-risk-minimization",
    "href": "myblog/posts/statistical-learning-theory/index.html#constrained-empirical-risk-minimization",
    "title": "Statistical Learning Theory",
    "section": "",
    "text": "Empirical Risk Minimization (ERM) in \\(F\\) is\n\\[\n\\hat{f}_{n} = argmin \\frac{1}{n} \\sum_{i=1}^{n} l(f(x_i), y_i)\n\\]"
  },
  {
    "objectID": "myblog/posts/survival-analysis/index.html",
    "href": "myblog/posts/survival-analysis/index.html",
    "title": "Survival Analysis",
    "section": "",
    "text": "Contents : -"
  },
  {
    "objectID": "myblog/posts/survival-analysis/index.html#definition",
    "href": "myblog/posts/survival-analysis/index.html#definition",
    "title": "Survival Analysis",
    "section": "Definition",
    "text": "Definition"
  },
  {
    "objectID": "myblog/posts/understanding-transformers/index.html",
    "href": "myblog/posts/understanding-transformers/index.html",
    "title": "Understanding Transformers",
    "section": "",
    "text": "Introduction\nLarge Language Models like GPT-4, Llama3 etc are based on a deep neural network based specific architecture known as Transformer. Prior transformer we solve sequence to sequence modeling problem using LSTM or GRU, but these architecture has some limitations like :\n\nIt takes alot of time in training since due sequential process.\n\nIt fails to keep track of past context.\n\nIn 2017 google brain and university of toronto released a paper Attention is all you need in which they proposed attention mechanism which is the core of working of transformer. In this paper researchers tried to solve the machine translation problem from english to french and introduced attenetion\n\n\nHow transformers works?\n\n\nHow to apply transformers\n\n\nCode\n\n\n\n\n Back to top"
  },
  {
    "objectID": "myblog/posts/pytorch-on-m1-mac/index.html",
    "href": "myblog/posts/pytorch-on-m1-mac/index.html",
    "title": "Accelerate computation on Mac using PyTorch and gpu support",
    "section": "",
    "text": "In year 2022 PyTorch and Metal engineering team at apple collaborated and announced support for GPU-accelerated pytorch operations on mac. Before that PyTorch operations on mac only leveraged CPU. But with PyTorch v1.12 release, developers and researchers can take advantage of Apple silicon GPUs for significantly faster model training as well. Here we will perform simple experiment to see the difference in doing tensor operations on CPU vs leveraging gpu support on M1 Mac."
  },
  {
    "objectID": "myblog/posts/pytorch-on-m1-mac/index.html#introduction",
    "href": "myblog/posts/pytorch-on-m1-mac/index.html#introduction",
    "title": "Accelerate computation on Mac using PyTorch and gpu support",
    "section": "",
    "text": "In year 2022 PyTorch and Metal engineering team at apple collaborated and announced support for GPU-accelerated pytorch operations on mac. Before that PyTorch operations on mac only leveraged CPU. But with PyTorch v1.12 release, developers and researchers can take advantage of Apple silicon GPUs for significantly faster model training as well. Here we will perform simple experiment to see the difference in doing tensor operations on CPU vs leveraging gpu support on M1 Mac."
  },
  {
    "objectID": "myblog/posts/pytorch-on-m1-mac/index.html#initial-setup",
    "href": "myblog/posts/pytorch-on-m1-mac/index.html#initial-setup",
    "title": "Accelerate computation on Mac using PyTorch and gpu support",
    "section": "Initial Setup",
    "text": "Initial Setup\nIn order to run the experiment we need to install below libraries.\n!pip install torch torchvision torchaudio"
  }
]