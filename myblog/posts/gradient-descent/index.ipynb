{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4a4f7f57-bb93-47c3-8d89-a480153df90a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Linear regression using gradient descent\"\n",
    "\n",
    "description: In this post we will implement gradient descent algorithm from scratch using numpy for linear regression.\n",
    "\n",
    "author: \"Vidyasagar Bhargava\"\n",
    "date: \"04/10/2017\"\n",
    "categories:\n",
    "  - gradient descent\n",
    "  - machine learning \n",
    "  \n",
    "image: \"https://i.imgur.com/J1w93Y9.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4dcafe-0b67-46a3-8621-88239dfb3292",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "A simple linear regression equation with one feature is defined as  :  \n",
    "\n",
    "$$y = b + w * x + \\epsilon$$\n",
    "\n",
    "Here w is coefficient and b is intercept term and $\\epsilon$  is the noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7920457-c856-4d57-a67f-680418fac2a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model. Parameters refer to coefficients in Linear Regression and weights in neural networks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daafe2b-4147-4ffd-a442-27f08f46047c",
   "metadata": {},
   "source": [
    "**Steps to implement gradient descent**  \n",
    "Step 1 - Compute the loss   \n",
    "Step 2 - Compute the gradient   \n",
    "Step 3 - Update the parameters   \n",
    "Step 4 - Repeat Step 1 to 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea6b5d-b36e-40ac-b6bd-c0df21f8f917",
   "metadata": {},
   "source": [
    "### Step 1 :-  Compute the loss \n",
    "\n",
    "For regression problem loss is given by mean squared error (MSE).  \n",
    "\n",
    "$$ MSE  = \\frac{1}{N} \\sum\\limits_{i=1}^{N} (y-\\hat{y_i})^2 $$\n",
    "\n",
    "$$ MSE  = \\frac{1}{N} \\sum\\limits_{i=1}^{N} (y-b-wx_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae3ed6-98e0-4b69-a4d6-0e74debb3741",
   "metadata": {},
   "source": [
    "### Step 2 :- Compute the gradient  \n",
    "\n",
    "Gradient is a vector with function's partial derivatives for components. \n",
    "\n",
    "$$ \\Delta (MSE)  = [\\frac{\\partial MSE}{\\partial w}, \\frac{\\partial MSE}{\\partial b}] $$\n",
    "\n",
    "It also tell the direction of steepest ascent which means in which direction one should step to increase the function most quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4745cb5-4827-469b-b642-1f50d8784a3c",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "A derivative tells us how much a given quantity changes when we change slightly some other quantity.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706f6de-24b1-4d73-9ba3-fd1b1245d76a",
   "metadata": {},
   "source": [
    "In our case we are interested how much our MSE loss change when we vary one of our two parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a4cbf2-0024-4a15-83eb-c72026909f2a",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial MSE}{\\partial w} = \\frac{\\partial MSE}{\\partial \\hat{y}}.\\frac{\\partial \\hat{y}}{\\partial w} = \\frac{1}{N} \\sum\\limits_{i=1}^{N} 2(y-b-wx_i).(-x_i) = -2\\frac{1}{N} \\sum\\limits_{i=1}^{N}(x_i)(y-\\hat{y_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad980-50d0-4b3d-9f61-650f5b51ad39",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial MSE}{\\partial b} = \\frac{\\partial MSE}{\\partial \\hat{y}}.\\frac{\\partial \\hat{y}}{\\partial b} = \\frac{1}{N} \\sum\\limits_{i=1}^{N} 2(y-b-wx_i).(-1) = -2\\frac{1}{N} \\sum\\limits_{i=1}^{N}(y-\\hat{y_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acfb2aa-8560-4df3-b2a3-fc1a2f809f21",
   "metadata": {},
   "source": [
    "### Step 3 :- Update the Parameters\n",
    "\n",
    "Now we will update the parameters by using gradients to minimize the loss. But gradient tells the direction of steepest ascent so we will multiply by -1.\n",
    "\n",
    "\n",
    "$$ w  = w - \\eta\\frac{\\partial MSE}{\\partial w} $$  \n",
    "$$ b  = b - \\eta\\frac{\\partial MSE}{\\partial b} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19860cac-bec6-4bb3-a915-fb8f0df11ab2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 4:- Repeat\n",
    "\n",
    "Now we will use updated parameters and start with step 1 again. We will repeat this process for multiple epochs. This is also known as training the model. \n",
    "\n",
    "\n",
    "::: {.column-margin}\n",
    "An epoch is completed when all the data points has been used for calculating the loss. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66efecc-354e-4847-a466-50cf397c21b5",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b5943-0a9a-4988-b60d-b1bf3ca2c3a5",
   "metadata": {},
   "source": [
    "Click below to add some new points and see how algorithm adjust the slope of line over time to meet best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215a927-3ef8-4521-af7f-a2c47cdebd26",
   "metadata": {},
   "source": [
    "```{ojs}\n",
    "//| echo: false\n",
    "\n",
    "p5(sketch => {\n",
    "  let system;\n",
    "  \n",
    "  //set up some default points\n",
    "  \n",
    "  /*    \n",
    "  */\n",
    "  \n",
    "  let data = [\n",
    "    sketch.createVector(.105, .423333),\n",
    "    sketch.createVector(.58, .526666),\n",
    "    sketch.createVector(.75, .82666),\n",
    "    sketch.createVector(.3475, .183333),\n",
    "    sketch.createVector(.3195, .543353),\n",
    "  ];\n",
    "  \n",
    "  let m = 1;\n",
    "  let b = 0;\n",
    "  \n",
    "  sketch.setup = function() {\n",
    "    sketch.createCanvas(800, 350);\n",
    "    sketch.background(51);\n",
    "  };\n",
    "  \n",
    "  const gradientDescent = () => {\n",
    "    var learning_rate = 0.05;\n",
    "    \n",
    "    for(var i = 0; i < data.length; i++) {\n",
    "      var x = data[i].x;\n",
    "      var y = data[i].y;\n",
    "      \n",
    "      var guess = m * x + b;\n",
    "      var error = y - guess;\n",
    "      \n",
    "      m = m + (error * x) * learning_rate;\n",
    "      b = b + (error) * learning_rate;\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  const drawLine = () => {\n",
    "    var x1 = 0;\n",
    "    var y1 = m * x1 + b;\n",
    "    var x2 = 1;\n",
    "    var y2 = m * x2 + b;\n",
    "    \n",
    "    x1 = sketch.map(x1, 0, 1, 0, sketch.width);\n",
    "    y1 = sketch.map(y1, 0, 1, sketch.height, 0);\n",
    "    x2 = sketch.map(x2, 0, 1, 0, sketch.width);\n",
    "    y2 = sketch.map(y2, 0, 1, sketch.height, 0);\n",
    "    \n",
    "    sketch.stroke(101, 74, 78);\n",
    "    sketch.strokeWeight(2);\n",
    "    sketch.line(x1, y1, x2, y2);\n",
    "  }\n",
    " \n",
    "  sketch.mousePressed = () => {\n",
    "    var x = sketch.map(sketch.mouseX, 0, sketch.width, 0 , 1);\n",
    "    var y = sketch.map(sketch.mouseY, 0, sketch.height, 1, 0);\n",
    "    var point = sketch.createVector(x, y);\n",
    "    data.push(point);\n",
    "  }\n",
    "  \n",
    "  sketch.draw = () => {\n",
    "    sketch.background(230, 221, 222)\n",
    "    for (var i = 0; i < data.length; i++) {\n",
    "      var x = sketch.map(data[i].x, 0, 1, 0, sketch.width);\n",
    "      var y = sketch.map(data[i].y, 0, 1, sketch.height, 0);\n",
    "      sketch.fill(32, 178, 170);\n",
    "      sketch.stroke(32, 178, 170);\n",
    "      sketch.ellipse(x, y, 8, 8);\n",
    "    }\n",
    "    \n",
    "    if(data.length > 1) {\n",
    "      gradientDescent();\n",
    "      drawLine();\n",
    "    }\n",
    "  }\n",
    "})\n",
    "\n",
    "import {p5} from \"@tmcw/p5\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02662aa-706a-4469-b809-201701d9de8f",
   "metadata": {},
   "source": [
    "## Gradient Descent Variants  \n",
    "\n",
    "* Batch Gradient Descent\n",
    "* Stochastic Gradient Descent\n",
    "* Mini Batch Gradient Descent\n",
    "\n",
    "\n",
    "If we use all the points in the training set to compute loss then it is **batch gradient descent**. If we use single data point and update our parameters it **stochastic gradient descent**. Anything between 1 and N is **mini batch gradient descent**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256e2c9-225c-4274-9a40-18c438698402",
   "metadata": {},
   "source": [
    "## Implementing Linear Regression using batch gradient descent \n",
    "\n",
    "Now its time to implement our linear regression model using batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3568901-5096-4013-989a-898dcd355a39",
   "metadata": {},
   "source": [
    "### Generate Synthetic data\n",
    "Using the above equation we will generate some synthetic data with w = 2 and b = 1 and some random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f28a5a-e3e3-49f9-8a14-a603df02c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.randn(100,1)\n",
    "y = 1 + 2*x + np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7481cd-5436-4dfa-8bd4-cdb9846b1ad5",
   "metadata": {},
   "source": [
    "Our goal will be to accurately predict the value of w (i.e. 2) and b (i.e. 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ae646-351c-409a-a3c3-5ab0cc247762",
   "metadata": {},
   "source": [
    "### Initialize parameters (w and b) and hyperparameter (learning_rate)\n",
    "\n",
    "So we will initialize the learnable parameters w and b with some random values and try to find right value by minimizing the loss function. The value of the hyperparameter i.e. learning rate is fixed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901a918e-6932-4917-b0c6-00011f719d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76835eb-766f-4799-87da-6ea5bfc9ecec",
   "metadata": {},
   "source": [
    "### Gradient Descent algorithm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66244a84-5b87-4cf5-87b9-434026349264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, w, b, lr):\n",
    "    \n",
    "    #compute the loss\n",
    "    yhat =  b + w*x\n",
    "    error = (y-yhat)\n",
    "    loss  = (error*2).mean()\n",
    "    \n",
    "    #compute the gradients\n",
    "    b_grad = -2*error.mean()\n",
    "    w_grad = -2*(x*error).mean()\n",
    "    \n",
    "    #update the parameters\n",
    "    b = b - lr * b_grad\n",
    "    w = w - lr * w_grad\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc6a871-c4b9-4ae0-873c-5f3bdc891ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00716235] [1.85616176]\n"
     ]
    }
   ],
   "source": [
    "# implementing multiple epochs \n",
    "\n",
    "for epoch in range(5000):\n",
    "    w,b = gradient_descent(x, y, w, b, lr)\n",
    "print(b,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9682eff5-2e94-44ba-ad4d-fc96bfc2ed1d",
   "metadata": {},
   "source": [
    "Let's compare our output with scikit-learn's Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5db4aa3e-dc9f-46f1-84f3-6ea7500ebbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00742783] [1.85674284]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x, y)\n",
    "print(lr.intercept_, lr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7b9e9-3b52-4e0d-817e-ff73e8504376",
   "metadata": {},
   "source": [
    "Our result matches upto few decimal places that means we have correctly implemented our batch gradient descent algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
