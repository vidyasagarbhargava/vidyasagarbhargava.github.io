{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4a4f7f57-bb93-47c3-8d89-a480153df90a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Gradient Descent from scratch for linear regression\"\n",
    "\n",
    "description: Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient of the function at the current point, because this is the direction of steepest descent.\n",
    "\n",
    "author: \"Vidyasagar Bhargava\"\n",
    "date: \"04/10/2017\"\n",
    "categories:\n",
    "  - gradient descent\n",
    "  - machine learning \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23f17b-b504-43ca-8e83-4c634a62d9f4",
   "metadata": {},
   "source": [
    "## Steps for gradient descent for linear regression  \n",
    "\n",
    "Step 1 : Defining the linear regression problem   \n",
    "Step 2 : Initialize the parameters and hyperparameters  \n",
    "Step 3 : Create gradient descent function   \n",
    "Step 4 : Iterate gradient descent function and update parameters to minimize loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3568901-5096-4013-989a-898dcd355a39",
   "metadata": {},
   "source": [
    "### Defining the linear regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f28a5a-e3e3-49f9-8a14-a603df02c98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.randn(10,1)\n",
    "y = 2*x + np.random.randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ae646-351c-409a-a3c3-5ab0cc247762",
   "metadata": {},
   "source": [
    "### Initializing parameters `w` and `b` and hyperparameter i.e. `learning_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901a918e-6932-4917-b0c6-00011f719d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0\n",
    "b = 0\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76835eb-766f-4799-87da-6ea5bfc9ecec",
   "metadata": {},
   "source": [
    "### Create gradient descent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01608f68-ff29-4cde-be7e-639560a02a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,w,b,learning_rate):\n",
    "    dldw = 0.0 \n",
    "    dldb = 0.0\n",
    "    N = x.shape[0]\n",
    "    \n",
    "    for xi, yi in zip(x,y):\n",
    "        dldw += -2*xi*(yi-(w*xi+b))\n",
    "        dldb += -2*(yi-(w*xi+b))\n",
    "    #make an update to the parameters\n",
    "    w = w - learning_rate*(1/N)*dldw\n",
    "    b = b - learning_rate*(1/N)*dldb\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5cbf5-012c-4583-9553-97cf074e6f7e",
   "metadata": {},
   "source": [
    "###  Iterate gradient descent function and update parameters to minimize loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cee1c58a-2d59-42e4-97a1-ad8c1ce186af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss is [3.31211004], parameters w:[0.03394664], b:[-0.00241723]\n",
      "1 loss is [3.2011369], parameters w:[0.06731399], b:[-0.00483577]\n",
      "2 loss is [3.09389844], parameters w:[0.100112], b:[-0.00725474]\n",
      "3 loss is [2.99026833], parameters w:[0.13235046], b:[-0.0096733]\n",
      "4 loss is [2.8901245], parameters w:[0.16403896], b:[-0.01209065]\n",
      "5 loss is [2.79334907], parameters w:[0.19518695], b:[-0.01450599]\n",
      "6 loss is [2.69982816], parameters w:[0.2258037], b:[-0.01691859]\n",
      "7 loss is [2.60945176], parameters w:[0.25589833], b:[-0.01932772]\n",
      "8 loss is [2.52211359], parameters w:[0.2854798], b:[-0.02173268]\n",
      "9 loss is [2.43771102], parameters w:[0.31455692], b:[-0.02413281]\n",
      "10 loss is [2.35614487], parameters w:[0.34313833], b:[-0.02652747]\n",
      "11 loss is [2.27731934], parameters w:[0.37123253], b:[-0.02891603]\n",
      "12 loss is [2.20114189], parameters w:[0.39884789], b:[-0.03129792]\n",
      "13 loss is [2.12752313], parameters w:[0.42599262], b:[-0.03367256]\n",
      "14 loss is [2.05637669], parameters w:[0.45267477], b:[-0.03603941]\n",
      "15 loss is [1.98761913], parameters w:[0.4789023], b:[-0.03839795]\n",
      "16 loss is [1.92116987], parameters w:[0.50468298], b:[-0.04074769]\n",
      "17 loss is [1.85695103], parameters w:[0.53002448], b:[-0.04308813]\n",
      "18 loss is [1.79488739], parameters w:[0.55493432], b:[-0.04541883]\n",
      "19 loss is [1.73490628], parameters w:[0.5794199], b:[-0.04773935]\n",
      "20 loss is [1.67693749], parameters w:[0.60348849], b:[-0.05004928]\n",
      "21 loss is [1.6209132], parameters w:[0.62714724], b:[-0.0523482]\n",
      "22 loss is [1.56676786], parameters w:[0.65040315], b:[-0.05463576]\n",
      "23 loss is [1.51443818], parameters w:[0.67326314], b:[-0.05691158]\n",
      "24 loss is [1.463863], parameters w:[0.69573398], b:[-0.05917532]\n",
      "25 loss is [1.41498321], parameters w:[0.71782234], b:[-0.06142665]\n",
      "26 loss is [1.36774173], parameters w:[0.73953477], b:[-0.06366526]\n",
      "27 loss is [1.32208339], parameters w:[0.76087769], b:[-0.06589085]\n",
      "28 loss is [1.27795492], parameters w:[0.78185744], b:[-0.06810315]\n",
      "29 loss is [1.23530481], parameters w:[0.80248024], b:[-0.07030189]\n",
      "30 loss is [1.19408333], parameters w:[0.82275218], b:[-0.07248682]\n",
      "31 loss is [1.1542424], parameters w:[0.84267927], b:[-0.0746577]\n",
      "32 loss is [1.11573559], parameters w:[0.86226742], b:[-0.07681431]\n",
      "33 loss is [1.07851804], parameters w:[0.88152242], b:[-0.07895644]\n",
      "34 loss is [1.04254639], parameters w:[0.90044997], b:[-0.08108389]\n",
      "35 loss is [1.00777875], parameters w:[0.91905568], b:[-0.08319647]\n",
      "36 loss is [0.97417466], parameters w:[0.93734503], b:[-0.08529402]\n",
      "37 loss is [0.94169501], parameters w:[0.95532345], b:[-0.08737636]\n",
      "38 loss is [0.91030202], parameters w:[0.97299626], b:[-0.08944336]\n",
      "39 loss is [0.87995918], parameters w:[0.99036866], b:[-0.09149486]\n",
      "40 loss is [0.85063121], parameters w:[1.00744581], b:[-0.09353074]\n",
      "41 loss is [0.82228404], parameters w:[1.02423274], b:[-0.09555088]\n",
      "42 loss is [0.79488472], parameters w:[1.04073441], b:[-0.09755518]\n",
      "43 loss is [0.76840144], parameters w:[1.05695571], b:[-0.09954352]\n",
      "44 loss is [0.74280345], parameters w:[1.07290142], b:[-0.10151582]\n",
      "45 loss is [0.71806103], parameters w:[1.08857624], b:[-0.103472]\n",
      "46 loss is [0.69414548], parameters w:[1.10398481], b:[-0.10541198]\n",
      "47 loss is [0.67102905], parameters w:[1.11913167], b:[-0.1073357]\n",
      "48 loss is [0.64868494], parameters w:[1.1340213], b:[-0.1092431]\n",
      "49 loss is [0.62708725], parameters w:[1.14865808], b:[-0.11113413]\n",
      "50 loss is [0.60621094], parameters w:[1.16304633], b:[-0.11300875]\n",
      "51 loss is [0.58603182], parameters w:[1.17719029], b:[-0.11486691]\n",
      "52 loss is [0.56652652], parameters w:[1.19109414], b:[-0.11670861]\n",
      "53 loss is [0.54767247], parameters w:[1.20476198], b:[-0.1185338]\n",
      "54 loss is [0.52944783], parameters w:[1.21819782], b:[-0.12034249]\n",
      "55 loss is [0.5118315], parameters w:[1.23140564], b:[-0.12213465]\n",
      "56 loss is [0.49480313], parameters w:[1.24438931], b:[-0.12391028]\n",
      "57 loss is [0.47834299], parameters w:[1.25715267], b:[-0.1256694]\n",
      "58 loss is [0.46243208], parameters w:[1.26969948], b:[-0.127412]\n",
      "59 loss is [0.44705198], parameters w:[1.28203342], b:[-0.1291381]\n",
      "60 loss is [0.43218494], parameters w:[1.29415812], b:[-0.13084771]\n",
      "61 loss is [0.41781377], parameters w:[1.30607717], b:[-0.13254087]\n",
      "62 loss is [0.40392188], parameters w:[1.31779405], b:[-0.1342176]\n",
      "63 loss is [0.39049322], parameters w:[1.32931223], b:[-0.13587793]\n",
      "64 loss is [0.3775123], parameters w:[1.34063508], b:[-0.13752191]\n",
      "65 loss is [0.36496413], parameters w:[1.35176593], b:[-0.13914956]\n",
      "66 loss is [0.35283424], parameters w:[1.36270807], b:[-0.14076094]\n",
      "67 loss is [0.34110864], parameters w:[1.3734647], b:[-0.1423561]\n",
      "68 loss is [0.3297738], parameters w:[1.38403899], b:[-0.14393509]\n",
      "69 loss is [0.31881666], parameters w:[1.39443404], b:[-0.14549796]\n",
      "70 loss is [0.3082246], parameters w:[1.4046529], b:[-0.14704478]\n",
      "71 loss is [0.2979854], parameters w:[1.41469859], b:[-0.14857561]\n",
      "72 loss is [0.28808727], parameters w:[1.42457404], b:[-0.15009052]\n",
      "73 loss is [0.27851882], parameters w:[1.43428216], b:[-0.15158957]\n",
      "74 loss is [0.26926903], parameters w:[1.4438258], b:[-0.15307285]\n",
      "75 loss is [0.26032724], parameters w:[1.45320775], b:[-0.15454041]\n",
      "76 loss is [0.25168318], parameters w:[1.46243078], b:[-0.15599235]\n",
      "77 loss is [0.2433269], parameters w:[1.47149758], b:[-0.15742874]\n",
      "78 loss is [0.23524878], parameters w:[1.48041082], b:[-0.15884966]\n",
      "79 loss is [0.22743954], parameters w:[1.48917311], b:[-0.1602552]\n",
      "80 loss is [0.2198902], parameters w:[1.49778701], b:[-0.16164544]\n",
      "81 loss is [0.21259208], parameters w:[1.50625506], b:[-0.16302048]\n",
      "82 loss is [0.2055368], parameters w:[1.51457974], b:[-0.16438041]\n",
      "83 loss is [0.19871625], parameters w:[1.52276348], b:[-0.16572531]\n",
      "84 loss is [0.1921226], parameters w:[1.53080869], b:[-0.16705528]\n",
      "85 loss is [0.18574828], parameters w:[1.53871771], b:[-0.16837042]\n",
      "86 loss is [0.17958597], parameters w:[1.54649288], b:[-0.16967083]\n",
      "87 loss is [0.17362859], parameters w:[1.55413645], b:[-0.1709566]\n",
      "88 loss is [0.16786932], parameters w:[1.56165068], b:[-0.17222784]\n",
      "89 loss is [0.16230154], parameters w:[1.56903775], b:[-0.17348464]\n",
      "90 loss is [0.15691888], parameters w:[1.57629984], b:[-0.17472711]\n",
      "91 loss is [0.15171514], parameters w:[1.58343906], b:[-0.17595535]\n",
      "92 loss is [0.14668439], parameters w:[1.59045751], b:[-0.17716947]\n",
      "93 loss is [0.14182083], parameters w:[1.59735724], b:[-0.17836957]\n",
      "94 loss is [0.13711891], parameters w:[1.60414026], b:[-0.17955576]\n",
      "95 loss is [0.13257324], parameters w:[1.61080857], b:[-0.18072815]\n",
      "96 loss is [0.1281786], parameters w:[1.6173641], b:[-0.18188684]\n",
      "97 loss is [0.12392998], parameters w:[1.62380878], b:[-0.18303195]\n",
      "98 loss is [0.11982249], parameters w:[1.63014448], b:[-0.18416359]\n",
      "99 loss is [0.11585145], parameters w:[1.63637307], b:[-0.18528185]\n",
      "100 loss is [0.1120123], parameters w:[1.64249635], b:[-0.18638687]\n",
      "101 loss is [0.10830065], parameters w:[1.64851612], b:[-0.18747873]\n",
      "102 loss is [0.10471227], parameters w:[1.65443414], b:[-0.18855757]\n",
      "103 loss is [0.10124303], parameters w:[1.66025213], b:[-0.18962348]\n",
      "104 loss is [0.09788899], parameters w:[1.66597179], b:[-0.19067659]\n",
      "105 loss is [0.09464629], parameters w:[1.6715948], b:[-0.191717]\n",
      "106 loss is [0.09151125], parameters w:[1.67712278], b:[-0.19274483]\n",
      "107 loss is [0.08848026], parameters w:[1.68255737], b:[-0.19376018]\n",
      "108 loss is [0.08554988], parameters w:[1.68790013], b:[-0.19476318]\n",
      "109 loss is [0.08271675], parameters w:[1.69315263], b:[-0.19575393]\n",
      "110 loss is [0.07997763], parameters w:[1.6983164], b:[-0.19673255]\n",
      "111 loss is [0.07732941], parameters w:[1.70339295], b:[-0.19769914]\n",
      "112 loss is [0.07476905], parameters w:[1.70838375], b:[-0.19865384]\n",
      "113 loss is [0.07229363], parameters w:[1.71329027], b:[-0.19959673]\n",
      "114 loss is [0.06990033], parameters w:[1.71811392], b:[-0.20052795]\n",
      "115 loss is [0.06758642], parameters w:[1.72285612], b:[-0.2014476]\n",
      "116 loss is [0.06534926], parameters w:[1.72751825], b:[-0.20235579]\n",
      "117 loss is [0.06318629], parameters w:[1.73210167], b:[-0.20325263]\n",
      "118 loss is [0.06109506], parameters w:[1.7366077], b:[-0.20413825]\n",
      "119 loss is [0.05907317], parameters w:[1.74103767], b:[-0.20501274]\n",
      "120 loss is [0.05711831], parameters w:[1.74539286], b:[-0.20587622]\n",
      "121 loss is [0.05522828], parameters w:[1.74967455], b:[-0.2067288]\n",
      "122 loss is [0.0534009], parameters w:[1.75388396], b:[-0.20757059]\n",
      "123 loss is [0.05163409], parameters w:[1.75802234], b:[-0.20840171]\n",
      "124 loss is [0.04992585], parameters w:[1.76209089], b:[-0.20922225]\n",
      "125 loss is [0.04827423], parameters w:[1.76609078], b:[-0.21003233]\n",
      "126 loss is [0.04667735], parameters w:[1.77002318], b:[-0.21083207]\n",
      "127 loss is [0.04513339], parameters w:[1.77388924], b:[-0.21162155]\n",
      "128 loss is [0.04364058], parameters w:[1.77769008], b:[-0.21240091]\n",
      "129 loss is [0.04219724], parameters w:[1.78142681], b:[-0.21317024]\n",
      "130 loss is [0.04080172], parameters w:[1.7851005], b:[-0.21392964]\n",
      "131 loss is [0.03945244], parameters w:[1.78871223], b:[-0.21467923]\n",
      "132 loss is [0.03814785], parameters w:[1.79226305], b:[-0.21541911]\n",
      "133 loss is [0.03688647], parameters w:[1.79575399], b:[-0.21614939]\n",
      "134 loss is [0.03566688], parameters w:[1.79918607], b:[-0.21687017]\n",
      "135 loss is [0.03448768], parameters w:[1.80256027], b:[-0.21758155]\n",
      "136 loss is [0.03334753], parameters w:[1.80587759], b:[-0.21828364]\n",
      "137 loss is [0.03224513], parameters w:[1.80913897], b:[-0.21897654]\n",
      "138 loss is [0.03117924], parameters w:[1.81234538], b:[-0.21966035]\n",
      "139 loss is [0.03014864], parameters w:[1.81549773], b:[-0.22033518]\n",
      "140 loss is [0.02915216], parameters w:[1.81859696], b:[-0.22100112]\n",
      "141 loss is [0.02818867], parameters w:[1.82164394], b:[-0.22165827]\n",
      "142 loss is [0.02725708], parameters w:[1.82463958], b:[-0.22230674]\n",
      "143 loss is [0.02635632], parameters w:[1.82758473], b:[-0.22294662]\n",
      "144 loss is [0.02548538], parameters w:[1.83048025], b:[-0.22357801]\n",
      "145 loss is [0.02464326], parameters w:[1.83332699], b:[-0.22420101]\n",
      "146 loss is [0.02382901], parameters w:[1.83612576], b:[-0.22481571]\n",
      "147 loss is [0.02304171], parameters w:[1.83887738], b:[-0.22542221]\n",
      "148 loss is [0.02228046], parameters w:[1.84158265], b:[-0.2260206]\n",
      "149 loss is [0.0215444], parameters w:[1.84424234], b:[-0.22661099]\n",
      "150 loss is [0.02083269], parameters w:[1.84685723], b:[-0.22719345]\n",
      "151 loss is [0.02014453], parameters w:[1.84942809], b:[-0.2277681]\n",
      "152 loss is [0.01947913], parameters w:[1.85195564], b:[-0.22833501]\n",
      "153 loss is [0.01883575], parameters w:[1.85444063], b:[-0.22889427]\n",
      "154 loss is [0.01821365], parameters w:[1.85688377], b:[-0.22944599]\n",
      "155 loss is [0.01761212], parameters w:[1.85928578], b:[-0.22999025]\n",
      "156 loss is [0.01703049], parameters w:[1.86164734], b:[-0.23052713]\n",
      "157 loss is [0.01646809], parameters w:[1.86396914], b:[-0.23105673]\n",
      "158 loss is [0.0159243], parameters w:[1.86625186], b:[-0.23157914]\n",
      "159 loss is [0.01539848], parameters w:[1.86849614], b:[-0.23209443]\n",
      "160 loss is [0.01489005], parameters w:[1.87070265], b:[-0.23260271]\n",
      "161 loss is [0.01439843], parameters w:[1.87287202], b:[-0.23310404]\n",
      "162 loss is [0.01392307], parameters w:[1.87500488], b:[-0.23359852]\n",
      "163 loss is [0.01346342], parameters w:[1.87710184], b:[-0.23408623]\n",
      "164 loss is [0.01301897], parameters w:[1.87916352], b:[-0.23456725]\n",
      "165 loss is [0.01258921], parameters w:[1.8811905], b:[-0.23504167]\n",
      "166 loss is [0.01217365], parameters w:[1.88318337], b:[-0.23550957]\n",
      "167 loss is [0.01177183], parameters w:[1.88514272], b:[-0.23597102]\n",
      "168 loss is [0.01138329], parameters w:[1.8870691], b:[-0.23642611]\n",
      "169 loss is [0.01100759], parameters w:[1.88896307], b:[-0.23687491]\n",
      "170 loss is [0.01064431], parameters w:[1.89082518], b:[-0.23731751]\n",
      "171 loss is [0.01029303], parameters w:[1.89265597], b:[-0.23775398]\n",
      "172 loss is [0.00995336], parameters w:[1.89445596], b:[-0.2381844]\n",
      "173 loss is [0.00962491], parameters w:[1.89622568], b:[-0.23860884]\n",
      "174 loss is [0.00930732], parameters w:[1.89796564], b:[-0.23902738]\n",
      "175 loss is [0.00900021], parameters w:[1.89967634], b:[-0.2394401]\n",
      "176 loss is [0.00870326], parameters w:[1.90135827], b:[-0.23984706]\n",
      "177 loss is [0.00841611], parameters w:[1.90301192], b:[-0.24024835]\n",
      "178 loss is [0.00813845], parameters w:[1.90463777], b:[-0.24064403]\n",
      "179 loss is [0.00786996], parameters w:[1.90623628], b:[-0.24103417]\n",
      "180 loss is [0.00761034], parameters w:[1.90780791], b:[-0.24141885]\n",
      "181 loss is [0.00735929], parameters w:[1.90935313], b:[-0.24179813]\n",
      "182 loss is [0.00711653], parameters w:[1.91087237], b:[-0.24217209]\n",
      "183 loss is [0.00688179], parameters w:[1.91236608], b:[-0.24254079]\n",
      "184 loss is [0.00665481], parameters w:[1.91383468], b:[-0.2429043]\n",
      "185 loss is [0.00643531], parameters w:[1.9152786], b:[-0.24326269]\n",
      "186 loss is [0.00622307], parameters w:[1.91669825], b:[-0.24361602]\n",
      "187 loss is [0.00601783], parameters w:[1.91809404], b:[-0.24396436]\n",
      "188 loss is [0.00581937], parameters w:[1.91946638], b:[-0.24430778]\n",
      "189 loss is [0.00562747], parameters w:[1.92081566], b:[-0.24464633]\n",
      "190 loss is [0.0054419], parameters w:[1.92214228], b:[-0.24498009]\n",
      "191 loss is [0.00526245], parameters w:[1.9234466], b:[-0.24530912]\n",
      "192 loss is [0.00508893], parameters w:[1.92472901], b:[-0.24563347]\n",
      "193 loss is [0.00492113], parameters w:[1.92598988], b:[-0.24595321]\n",
      "194 loss is [0.00475888], parameters w:[1.92722957], b:[-0.2462684]\n",
      "195 loss is [0.00460198], parameters w:[1.92844843], b:[-0.2465791]\n",
      "196 loss is [0.00445026], parameters w:[1.92964683], b:[-0.24688536]\n",
      "197 loss is [0.00430354], parameters w:[1.93082509], b:[-0.24718726]\n",
      "198 loss is [0.00416167], parameters w:[1.93198357], b:[-0.24748484]\n",
      "199 loss is [0.00402448], parameters w:[1.9331226], b:[-0.24777816]\n",
      "200 loss is [0.00389181], parameters w:[1.9342425], b:[-0.24806728]\n",
      "201 loss is [0.00376353], parameters w:[1.93534359], b:[-0.24835226]\n",
      "202 loss is [0.00363948], parameters w:[1.9364262], b:[-0.24863315]\n",
      "203 loss is [0.00351952], parameters w:[1.93749063], b:[-0.24891001]\n",
      "204 loss is [0.00340351], parameters w:[1.93853719], b:[-0.24918288]\n",
      "205 loss is [0.00329134], parameters w:[1.93956618], b:[-0.24945183]\n",
      "206 loss is [0.00318286], parameters w:[1.9405779], b:[-0.2497169]\n",
      "207 loss is [0.00307797], parameters w:[1.94157264], b:[-0.24997815]\n",
      "208 loss is [0.00297653], parameters w:[1.94255068], b:[-0.25023564]\n",
      "209 loss is [0.00287844], parameters w:[1.9435123], b:[-0.2504894]\n",
      "210 loss is [0.00278359], parameters w:[1.94445779], b:[-0.25073949]\n",
      "211 loss is [0.00269186], parameters w:[1.94538741], b:[-0.25098597]\n",
      "212 loss is [0.00260316], parameters w:[1.94630143], b:[-0.25122888]\n",
      "213 loss is [0.00251739], parameters w:[1.94720011], b:[-0.25146826]\n",
      "214 loss is [0.00243444], parameters w:[1.94808371], b:[-0.25170417]\n",
      "215 loss is [0.00235423], parameters w:[1.94895249], b:[-0.25193666]\n",
      "216 loss is [0.00227667], parameters w:[1.9498067], b:[-0.25216576]\n",
      "217 loss is [0.00220166], parameters w:[1.95064657], b:[-0.25239154]\n",
      "218 loss is [0.00212913], parameters w:[1.95147235], b:[-0.25261402]\n",
      "219 loss is [0.00205899], parameters w:[1.95228428], b:[-0.25283327]\n",
      "220 loss is [0.00199116], parameters w:[1.95308259], b:[-0.25304931]\n",
      "221 loss is [0.00192556], parameters w:[1.95386751], b:[-0.25326221]\n",
      "222 loss is [0.00186213], parameters w:[1.95463927], b:[-0.25347199]\n",
      "223 loss is [0.0018008], parameters w:[1.95539809], b:[-0.25367871]\n",
      "224 loss is [0.00174148], parameters w:[1.95614417], b:[-0.2538824]\n",
      "225 loss is [0.00168412], parameters w:[1.95687775], b:[-0.25408311]\n",
      "226 loss is [0.00162865], parameters w:[1.95759903], b:[-0.25428088]\n",
      "227 loss is [0.00157501], parameters w:[1.95830821], b:[-0.25447575]\n",
      "228 loss is [0.00152313], parameters w:[1.9590055], b:[-0.25466776]\n",
      "229 loss is [0.00147297], parameters w:[1.9596911], b:[-0.25485694]\n",
      "230 loss is [0.00142446], parameters w:[1.96036521], b:[-0.25504335]\n",
      "231 loss is [0.00137755], parameters w:[1.96102801], b:[-0.25522702]\n",
      "232 loss is [0.00133218], parameters w:[1.96167971], b:[-0.25540798]\n",
      "233 loss is [0.00128831], parameters w:[1.96232048], b:[-0.25558627]\n",
      "234 loss is [0.00124589], parameters w:[1.96295051], b:[-0.25576194]\n",
      "235 loss is [0.00120486], parameters w:[1.96356998], b:[-0.25593501]\n",
      "236 loss is [0.00116519], parameters w:[1.96417907], b:[-0.25610553]\n",
      "237 loss is [0.00112682], parameters w:[1.96477795], b:[-0.25627353]\n",
      "238 loss is [0.00108972], parameters w:[1.96536679], b:[-0.25643905]\n",
      "239 loss is [0.00105384], parameters w:[1.96594577], b:[-0.25660211]\n",
      "240 loss is [0.00101914], parameters w:[1.96651504], b:[-0.25676277]\n",
      "241 loss is [0.00098559], parameters w:[1.96707478], b:[-0.25692104]\n",
      "242 loss is [0.00095314], parameters w:[1.96762513], b:[-0.25707696]\n",
      "243 loss is [0.00092176], parameters w:[1.96816627], b:[-0.25723057]\n",
      "244 loss is [0.00089141], parameters w:[1.96869834], b:[-0.2573819]\n",
      "245 loss is [0.00086207], parameters w:[1.9692215], b:[-0.25753099]\n",
      "246 loss is [0.00083369], parameters w:[1.96973589], b:[-0.25767785]\n",
      "247 loss is [0.00080624], parameters w:[1.97024167], b:[-0.25782253]\n",
      "248 loss is [0.0007797], parameters w:[1.97073897], b:[-0.25796506]\n",
      "249 loss is [0.00075404], parameters w:[1.97122795], b:[-0.25810546]\n",
      "250 loss is [0.00072921], parameters w:[1.97170873], b:[-0.25824377]\n",
      "251 loss is [0.00070521], parameters w:[1.97218147], b:[-0.25838002]\n",
      "252 loss is [0.000682], parameters w:[1.97264628], b:[-0.25851424]\n",
      "253 loss is [0.00065955], parameters w:[1.97310331], b:[-0.25864645]\n",
      "254 loss is [0.00063784], parameters w:[1.97355269], b:[-0.25877668]\n",
      "255 loss is [0.00061685], parameters w:[1.97399455], b:[-0.25890497]\n",
      "256 loss is [0.00059655], parameters w:[1.974429], b:[-0.25903134]\n",
      "257 loss is [0.00057691], parameters w:[1.97485619], b:[-0.25915581]\n",
      "258 loss is [0.00055793], parameters w:[1.97527621], b:[-0.25927842]\n",
      "259 loss is [0.00053957], parameters w:[1.97568921], b:[-0.25939919]\n",
      "260 loss is [0.00052181], parameters w:[1.9760953], b:[-0.25951816]\n",
      "261 loss is [0.00050464], parameters w:[1.97649458], b:[-0.25963533]\n",
      "262 loss is [0.00048803], parameters w:[1.97688718], b:[-0.25975075]\n",
      "263 loss is [0.00047197], parameters w:[1.97727321], b:[-0.25986443]\n",
      "264 loss is [0.00045644], parameters w:[1.97765278], b:[-0.2599764]\n",
      "265 loss is [0.00044142], parameters w:[1.978026], b:[-0.26008669]\n",
      "266 loss is [0.00042689], parameters w:[1.97839297], b:[-0.26019532]\n",
      "267 loss is [0.00041285], parameters w:[1.9787538], b:[-0.26030232]\n",
      "268 loss is [0.00039926], parameters w:[1.97910859], b:[-0.2604077]\n",
      "269 loss is [0.00038613], parameters w:[1.97945744], b:[-0.26051149]\n",
      "270 loss is [0.00037342], parameters w:[1.97980045], b:[-0.26061372]\n",
      "271 loss is [0.00036113], parameters w:[1.98013773], b:[-0.2607144]\n",
      "272 loss is [0.00034925], parameters w:[1.98046936], b:[-0.26081357]\n",
      "273 loss is [0.00033776], parameters w:[1.98079545], b:[-0.26091123]\n",
      "274 loss is [0.00032665], parameters w:[1.98111607], b:[-0.26100742]\n",
      "275 loss is [0.0003159], parameters w:[1.98143134], b:[-0.26110216]\n",
      "276 loss is [0.00030551], parameters w:[1.98174133], b:[-0.26119546]\n",
      "277 loss is [0.00029546], parameters w:[1.98204613], b:[-0.26128734]\n",
      "278 loss is [0.00028574], parameters w:[1.98234584], b:[-0.26137784]\n",
      "279 loss is [0.00027634], parameters w:[1.98264053], b:[-0.26146697]\n",
      "280 loss is [0.00026725], parameters w:[1.98293029], b:[-0.26155474]\n",
      "281 loss is [0.00025846], parameters w:[1.98321521], b:[-0.26164118]\n",
      "282 loss is [0.00024995], parameters w:[1.98349536], b:[-0.26172631]\n",
      "283 loss is [0.00024173], parameters w:[1.98377083], b:[-0.26181015]\n",
      "284 loss is [0.00023378], parameters w:[1.98404169], b:[-0.26189271]\n",
      "285 loss is [0.00022609], parameters w:[1.98430802], b:[-0.26197402]\n",
      "286 loss is [0.00021865], parameters w:[1.98456989], b:[-0.26205409]\n",
      "287 loss is [0.00021146], parameters w:[1.98482739], b:[-0.26213294]\n",
      "288 loss is [0.00020451], parameters w:[1.98508058], b:[-0.26221059]\n",
      "289 loss is [0.00019778], parameters w:[1.98532954], b:[-0.26228706]\n",
      "290 loss is [0.00019127], parameters w:[1.98557434], b:[-0.26236237]\n",
      "291 loss is [0.00018498], parameters w:[1.98581504], b:[-0.26243652]\n",
      "292 loss is [0.0001789], parameters w:[1.98605172], b:[-0.26250955]\n",
      "293 loss is [0.00017302], parameters w:[1.98628444], b:[-0.26258146]\n",
      "294 loss is [0.00016733], parameters w:[1.98651327], b:[-0.26265227]\n",
      "295 loss is [0.00016182], parameters w:[1.98673828], b:[-0.26272201]\n",
      "296 loss is [0.0001565], parameters w:[1.98695953], b:[-0.26279067]\n",
      "297 loss is [0.00015135], parameters w:[1.98717707], b:[-0.26285829]\n",
      "298 loss is [0.00014638], parameters w:[1.98739098], b:[-0.26292487]\n",
      "299 loss is [0.00014156], parameters w:[1.98760132], b:[-0.26299043]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    w, b = gradient_descent(x,y,w,b,learning_rate) \n",
    "    yhat =  w*x + b\n",
    "    loss =  np.divide(np.sum((y-yhat)**2, axis=0), x.shape[0])\n",
    "    print(f'{epoch} loss is {loss}, parameters w:{w}, b:{b}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
