{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Introduction to Apple's Machine learning Framework- MLX\"\n",
    "description: \"Apple's machine learning research team recently released a Machine Learning framework called MLX, a NumPy-like array framework designed for efficient and flexible machine learning on Apple silicon.\"\n",
    "author: \"Vidyasagar Bhargava\"\n",
    "date: \"12/24/2023\"\n",
    "categories:\n",
    "  - machine learning framework\n",
    "  - deep learning \n",
    "jupyter: python3\n",
    "title-block-banner: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation \n",
    "MLX is available on PyPI. You need an Apple silicon based computer.\n",
    "\n",
    "```\n",
    "pip install mlx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features of MLX\n",
    "\n",
    "#### 1. Familiar APIs\n",
    "\n",
    "MLX has a Python API that closely follows NumPy. MLX also has a fully featured C++ API, which closely mirrors the Python API. MLX has higher-level packages like mlx.nn and mlx.optimizers with APIs that closely follow PyTorch to simplify building more complex models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 2, 3, 4], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "a = mx.array([1,2,3,4])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    }
   ],
   "source": [
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "b = mx.array([1.0, 2.0, 3.0, 4.0])\n",
    "print(b.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lazy computation: \n",
    "\n",
    "Computations in MLX are lazy. That means outputs of MLX operations are not computed untill they are needed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([2, 4, 6, 8], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c = a + b   # c not yet evaluated\n",
    "mx.eval(c)  # evaluates c\n",
    "c = a + b\n",
    "print(c)    # also evaluates c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Composable function transformations & Dynamic graph construction : \n",
    "\n",
    "MLX supports composable function transformations for automatic differentiation, automatic vectorization, and computation graph optimization.  \n",
    "\n",
    "Computation graphs in MLX are constructed dynamically. Changing the shapes of function arguments does not trigger slow compilations, and debugging is simple and intuitive.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0, dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLX has standard function transformations like grad() and vmap()\n",
    "\n",
    "x = mx.array(0.0)\n",
    "mx.sin(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1, dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.grad(mx.sin)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-0, dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.grad(mx.grad(mx.sin))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Unified memory Architecture \n",
    "\n",
    "A notable difference from MLX and other frameworks is the unified memory model. Arrays in MLX live in shared memory. Operations on MLX arrays can be performed on any of the supported device types without transferring data.\n",
    "\n",
    "Let's see an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mx.random.normal((100,))\n",
    "b = mx.random.normal((100,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both `a` and `b` lives in unified memory.  \n",
    "\n",
    "In MLX, you don't need to move arrays between different memory locations for different devices (like CPU or GPU). Instead of moving data, you specify the device (like CPU or GPU) when you perform an operation on the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 6, 8], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.add(a, b, stream=mx.cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 6, 8], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.add(a, b, stream=mx.gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you perform operations that don't depend on each other (like adding 'a' and 'b' in example), MLX can run them in parallel.\n",
    "So, the CPU and GPU can both work on the same task simultaneously because there are no dependencies between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = mx.add(a, b, stream=mx.cpu)\n",
    "d = mx.add(a, c, stream=mx.gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are dependencies (meaning one operation depends on the result of another), MLX takes care of managing them.\n",
    "For instance, if you add 'a' and 'b' on the CPU and then perform another addition on the GPU that depends on the result from the CPU, MLX ensures that the GPU operation waits for the CPU operation to finish before it starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a, b, d1, d2):\n",
    "  x = mx.matmul(a, b, stream=d1)\n",
    "  for _ in range(500):\n",
    "      b = mx.exp(b, stream=d2)\n",
    "  return x, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mx.random.uniform(shape=(4096, 512))\n",
    "b = mx.random.uniform(shape=(512, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first matmul operation is good fit for the GPU since it is more compute dense. The second sequence of operations are better fit for the CPU, since they are very small and would be probably overhead bound on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Multi-device \n",
    "\n",
    "Operations can run on any of the supported devices (currently the CPU and the GPU).\n",
    "The framework is intended to be user-friendly, but still efficient to train and deploy models. The design of the framework itself is also conceptually simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stream(Device(gpu, 0), 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx.default_stream(mx.default_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression implementation\n",
    "\n",
    "Let's implement simple linear regression example as starting point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "\n",
    "num_features = 100\n",
    "num_examples = 1_000\n",
    "num_iters = 10_000  # iterations of SGD\n",
    "lr = 0.01  # learning rate for SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize parameters (w and b) and hyperparameter (learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters\n",
    "w_star = mx.random.normal((num_features,))\n",
    "\n",
    "# Input examples (design matrix)\n",
    "X = mx.random.normal((num_examples, num_features))\n",
    "\n",
    "# Noisy labels\n",
    "eps = 1e-2 * mx.random.normal((num_examples,))\n",
    "y = X @ w_star + eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(w):\n",
    "    return 0.5 * mx.mean(mx.square(X @ w - y))\n",
    "\n",
    "grad_fn = mx.grad(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1e-2 * mx.random.normal((num_features,))\n",
    "\n",
    "for _ in range(num_iters):\n",
    "    grad = grad_fn(w)\n",
    "    w = w - lr * grad\n",
    "    mx.eval(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.00004, |w-w*| = 0.00354, \n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(w)\n",
    "error_norm = mx.sum(mx.square(w - w_star)).item() ** 0.5\n",
    "\n",
    "print(\n",
    "    f\"Loss {loss.item():.5f}, |w-w*| = {error_norm:.5f}, \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Let's implement logistic regression now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.02796, Accuracy 1.00000 Throughput 2555.69304 (it/s)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import mlx.core as mx\n",
    "\n",
    "num_features = 100\n",
    "num_examples = 1_000\n",
    "num_iters = 10_000\n",
    "lr = 0.1\n",
    "\n",
    "# True parameters\n",
    "w_star = mx.random.normal((num_features,))\n",
    "\n",
    "# Input examples\n",
    "X = mx.random.normal((num_examples, num_features))\n",
    "\n",
    "# Labels\n",
    "y = (X @ w_star) > 0\n",
    "\n",
    "\n",
    "# Initialize random parameters\n",
    "w = 1e-2 * mx.random.normal((num_features,))\n",
    "\n",
    "\n",
    "def loss_fn(w):\n",
    "    logits = X @ w\n",
    "    return mx.mean(mx.logaddexp(0.0, logits) - y * logits)\n",
    "\n",
    "\n",
    "grad_fn = mx.grad(loss_fn)\n",
    "\n",
    "tic = time.time()\n",
    "for _ in range(num_iters):\n",
    "    grad = grad_fn(w)\n",
    "    w = w - lr * grad\n",
    "    mx.eval(w)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "loss = loss_fn(w)\n",
    "final_preds = (X @ w) > 0\n",
    "acc = mx.mean(final_preds == y)\n",
    "\n",
    "throughput = num_iters / (toc - tic)\n",
    "print(\n",
    "    f\"Loss {loss.item():.5f}, Accuracy {acc.item():.5f} \"\n",
    "    f\"Throughput {throughput:.5f} (it/s)\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "mlx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
