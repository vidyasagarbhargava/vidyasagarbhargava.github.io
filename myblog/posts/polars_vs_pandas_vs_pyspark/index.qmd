---
title: "Polars vs Pandas vs PySpark"
description: Comparing syntax of top 3 libraries of used in data science world for data preprocessing.
author: "Vidyasagar Bhargava"
date: "8/31/2024"
categories:
  - pandas
  - polars
  - pyspark
---

## Syntax of Polars, Pandas and PySpark

| **Operation**                      | **Polars**                                                              | **Pandas**                                                       | **PySpark**                                                                       |
|------------------|-------------------|------------------|------------------|
| **1. Creating a Series**           | `pl.Series("name", [1, 2, 3])`                                          | `pd.Series([1, 2, 3], name="name")`                              | `spark.createDataFrame([(1,), (2,), (3,)], ["name"])`                             |
| **2. Creating a DataFrame**        | `pl.DataFrame({"col1": [1, 2], "col2": [3, 4]})`                        | `pd.DataFrame({"col1": [1, 2], "col2": [3, 4]})`                 | `spark.createDataFrame([(1, 3), (2, 4)], ["col1", "col2"])`                       |
| **3. Selecting Columns**           | `df.select(["col1", "col2"])`                                           | `df[["col1", "col2"]]`                                           | `df.select("col1", "col2")`                                                       |
| **4. Filtering Rows (Condition)**  | `df.filter(pl.col("col1") > 1)`                                         | `df[df["col1"] > 1]`                                             | `df.filter(df.col1 > 1)`                                                          |
| **5. Multiple Filter Conditions**  | `df.filter((pl.col("col1") > 1) & (pl.col("col2") < 4))`                | `df[(df["col1"] > 1) & (df["col2"] < 4)]`                        | `df.filter((df.col1 > 1) & (df.col2 < 4))`                                        |
| **6. Select and Filter Together**  | `df.select(["col1"]).filter(pl.col("col1") > 1)`                        | `df.loc[df["col1"] > 1, ["col1"]]`                               | `df.select("col1").filter(df.col1 > 1)`                                           |
| **7. Creating New Column**         | `df.with_column((pl.col("col1") + 1).alias("new_col"))`                 | `df["new_col"] = df["col1"] + 1`                                 | `df.withColumn("new_col", df.col1 + 1)`                                           |
| **8. Group By and Aggregation**    | `df.groupby("col1").agg([pl.col("col2").sum(), pl.col("col2").mean()])` | `df.groupby("col1")["col2"].agg(["sum", "mean"])`                | `df.groupBy("col1").agg(F.sum("col2"), F.mean("col2"))`                           |
| **9. Sorting DataFrame**           | `df.sort("col1")` <br> `df.sort(["col1", "col2"])`                      | `df.sort_values("col1")` <br> `df.sort_values(["col1", "col2"])` | `df.orderBy("col1")` <br> `df.orderBy(["col1", "col2"])`                          |
| **10. Dropping Column**            | `df.drop("col1")`                                                       | `df.drop("col1", axis=1)`                                        | `df.drop("col1")`                                                                 |
| **11. Finding Null Counts**        | `df.null_count()`                                                       | `df.isnull().sum()`                                              | `df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns])` |
| **12. Handling Null Values**       | `df.fill_null(0)`                                                       | `df.fillna(0)`                                                   | `df.fillna(0)`                                                                    |
| **13. Column Renaming**            | `df.rename({"col1": "new_col1"})`                                       | `df.rename(columns={"col1": "new_col1"})`                        | `df.withColumnRenamed("col1", "new_col1")`                                        |
| **14. Dropping Duplicates**        | `df.unique()`                                                           | `df.drop_duplicates()`                                           | `df.dropDuplicates()`                                                             |
| **15. Changing Data Types**        | `df.with_column(pl.col("col1").cast(pl.Int64))`                         | `df["col1"] = df["col1"].astype(int)`                            | `df.withColumn("col1", df.col1.cast("int"))`                                      |
| **16. Merging/Joining DataFrames** | `df.join(df2, on="col1")`                                               | `df.merge(df2, on="col1")`                                       | `df.join(df2, df.col1 == df2.col1)`                                               |
| **17. Pivot Table**                | `df.pivot(index="col1", columns="col2", values="col3")`                 | `df.pivot_table(index="col1", columns="col2", values="col3")`    | `df.groupBy("col1").pivot("col2").sum("col3")`                                    |
| **18. Concatenating DataFrames**   | `pl.concat([df1, df2])`                                                 | `pd.concat([df1, df2])`                                          | `df1.union(df2)`                                                                  |

We can see that polars syntax is like best of pandas and pyspark world. I will keep on updating this as I find more useful and common operation for data pre-processing.