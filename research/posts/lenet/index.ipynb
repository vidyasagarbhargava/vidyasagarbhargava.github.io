{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"LeNet-5\"\n",
    "\n",
    "description: LeNet-5 is introduced by **Yann LeCun, Leon Bottou, Yoshua Bengio** and **Patrick Haffner** in the year 1998 in the paper  **[Gradient-Based Learning Applied to Document Recognition](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf).** LeNet is a classic convolutional neural network employing the use of convolutions, pooling and fully connected layers. It was used for the handwritten digit recognition task with the MNIST dataset.  \n",
    " \n",
    "author: \"Vidyasagar Bhargava\"\n",
    "date: \"05/08/2022\"\n",
    "categories:\n",
    "  - deep learning\n",
    "  - convolutional neural network\n",
    "title-block-banner: true\n",
    "image: \"https://blog.paperspace.com/content/images/2021/10/image-17.png\"\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "# Architecture\n",
    "To understand LeNet-5 in detail lets go through each component of the architecture.  \n",
    "\n",
    "![](https://i.imgur.com/vv3VcAt.png)   \n",
    "\n",
    "\n",
    "The input for LeNet-5 architecture is grey scale images which are 32x32 size. Since these are grey scale images hence channel is 1.   \n",
    "\n",
    "LeNet-5 has 5 layers with 3 convolutional layers with a combination of average pooling and two fully connected layers. The first convolutional layers has filter size of 5x5 with 6 such filters. Now these will decrease the width and height of image while increasing the depth (channels) of image. The output will be 28x28x6.\n",
    "  \n",
    "\n",
    "Here is simplified version of architecture.\n",
    "![](https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-18-12-52-17.png) \n",
    "\n",
    "::: {.callout-caution collapse=\"true\"}\n",
    "## How we get output as 28x28x6? \n",
    "\n",
    "The spatial size of output is calculated using **`([W-F+2P]/S)+1`**  \n",
    "* W is the input volume size  \n",
    "* F is the size of the filter  \n",
    "* P is the number of padding applied    \n",
    "* S is the number of strides  \n",
    "\n",
    "W = 32, F = 5, P =0, S = 1 the output depth will be equal to the number of filters applied i.e. = 6.  \n",
    "Applying formula ([32-5+2*0]/1)+1= 28. So the output volume is 28x28x6.\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "After this pooling is applied to decrease the feature map by half i.e. 14x14x6. Again same filter size 5x5 with 16 filters is now applied to the output followed by a pooling layer. This reduces the output feature map to 5x5x16.\n",
    "\n",
    "After this, a convolutional layer of size 5x5 with 120 filters is applied to flatten the feature map to 120 values.Then comes the first fully connected layer, with 84 neurons.\n",
    "Finally, we have the output layer which has 10 output neurons, since the MNIST data have 10 classes for each of the represented 10 numerical digits.\n",
    "\n",
    "| Layer           | #Filters/Neurons | Filter Size | stride |  size of Feature Map | Activation Function |\n",
    "|-----------------|:----|------:|:------:|:------:|:------:|:------:|\n",
    "| input           | 1   |   -   |  - | 32x32x1 |       |\n",
    "| conv 1          | 6   | 5x5   |  1 | 28x28x6 | Relu  | \n",
    "| Average Pooling | 6   | 2x2   |  2 | 14x14x6 |       | \n",
    "| conv 2          | 16  | 5x5   |  1 | 10x10x16| Relu  |\n",
    "| Average Pooling | 16  | 2x2   |  2 | 5x5x16  |       |\n",
    "| conv 3          | 120 | 5x5   |  1 | 120     | Relu  |\n",
    "| FC              | -   |  -    |  - | 84      | Relu  |\n",
    "| FC              | -   |  -    |  - | 10      | softmax |\n",
    "\n",
    "\n",
    ": LeNet-5 Architecture Detail {.striped .hover}\n",
    "\n",
    "::: {.callout-note}\n",
    "In original paper `sigmoid` is used as activation Function. Here we are replacing it recently most popular one i.e. `Relu`.\n",
    ":::\n",
    "\n",
    "\n",
    "# Implementation  \n",
    "Now we have understood the architecture and let's implement it. \n",
    "\n",
    "* Dataset Understanding   \n",
    "* LeNet from Scratch  \n",
    "* Setting Hyperparameters  \n",
    "* Model Training  \n",
    "* Model Evaluation  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Understanding\n",
    "Here we will be using famous MNIST dataset which contains hand written digits. These are greyscale with size of 28x28 composed of 60,000 training images and 10,000 testing images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the relevant libraries\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the relevant variables\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "Using the torchvision library we will load the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "The MNIST data can't be used as it is for the LeNet5 architecture. The LeNet5 architecture accepts the input to be 32x32 and the MNIST images are 28x28. We can fix this by resizing the images, normalizing them using the pre-calculated mean and standard deviation (available online), and finally storing them as tensors.\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the convolutional neural network\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5(num_classes).to(device)\n",
    "\n",
    "# Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "# Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [400/938], Loss: 0.1018\n",
      "Epoch [1/10], Step [800/938], Loss: 0.0178\n",
      "Epoch [2/10], Step [400/938], Loss: 0.0299\n",
      "Epoch [2/10], Step [800/938], Loss: 0.0044\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0335\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0365\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0217\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0110\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0063\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0620\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0178\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0588\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0092\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0120\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0048\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0455\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0067\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0589\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0015\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0030\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = cost(outputs, labels)\n",
    "        \t\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \t\t\n",
    "        if (i+1) % 400 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "        \t\t           .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the loss is decreasing with every epoch which shows that our model is indeed learning.\n",
    "\n",
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.97 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "  \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this model, we get around 98.97% accuracy which is quite good.\n",
    "\n",
    "# Conclusions\n",
    "\n",
    "* Understood the architecture of LeNet5\n",
    "* Then we built LeNet5 from scratch along with defining hyperparameters for the model.  \n",
    "* Finally, we trained and tested our model on the MNIST dataset, and the model seemed to perform well on the test dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
