{
  "hash": "61d7068e743c5d81104478b6caab042d",
  "result": {
    "markdown": "---\ntitle: \"Nearest Neighbour Classifier\"\ndescription: \"Nearest neighbor classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples.\"\nauthor: \"Vidyasagar Bhargava\"\ndate: \"06/08/2022\"\ncategories:\n  - machine learning\n  - supervised learning\n  - algorithm\n  - classifier\nformat:\n    html:\n        code-fold: false\njupyter: python3\nexecute: \n  enabled: true\n---\n\n![](https://i.imgur.com/bpU9JuI.gif)\n\n# Introduction\nNearest neighbor classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples.\n\nNearest Neighbors works well for classification task where the relationships among the features and target classes are numerous and extremely difficult to understand but the items of similar class tend to be homogeneous.\n\nNearest Neighbor classifier struggles most when there is no clear distinction exists among the groups.\n\n\n# k-NN algorithm\nk-Nearest Neighbor algorithm is an example of nearest neighbor classifier.\n\n## Strength\n* Simple and effective\n* Makes no assumption about data\n* Fast Training Process\n\n\n## Weakness\n* Doesnâ€™t produce model, limiting the ability to understand how features are related to class\n* Requires selection of k\n* Slow classification phase\n* Categorical features and missing data require pre processing\n\nThe k-Nearest Neighbor algorithm uses nearest k number of neighbors for labeling of an unlabeled example. The unlabeled test example is assigned the class of majority of the k-Nearest Neighbors.\n\nFor finding the distance k-NN algorithm uses **Euclidean distance**.\n\n# Choosing an appropriate k\nChoosing the value of k determines how well the model will generalize to future data. Choosing a large k reduces the impact or variance caused by noisy data but can bias the learner so that it runs the risk of ignoring small, but important pattern.\n\nIn Practice choosing k depends on the difficulty of the concept to be learned and the number of records in training data.\n\n* Start with k value equal to the square root of the number of training examples.\n* Using cross validation to determine the best k value.\n* Weighted voting is one of interesting way to solve this problem. By giving higher weight to close neighbors.b 444\n\n# k-NN from scratch\n* Compute distances between x and all examples in the training set\n* Sort by distance and return indexes of the first k neighbors\n* Extract the labels of the k nearest neighbor training samples\n* Return the most common class label\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom collections import Counter\ndef euclidean_distance(x1, x2):\n        return np.sqrt(np.sum((x1 - x2)**2))\n```\n:::\n\n\nWe used euclidean distance for calculating the nearest neighbors.\n\nNow we will define our KNN Class\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nclass KNN:\n    def __init__(self, k=3):\n        self.k = k\n\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n\n    def predict(self, X):\n        y_pred = [self._predict(x) for x in X]\n        return np.array(y_pred)\n\n    def _predict(self, x):\n        # Compute distances between x and all examples in the training set\n        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n        # Sort by distance and return indexes of the first k neighbors\n        k_idx = np.argsort(distances)[:self.k]\n        # Extract the labels of the k nearest neighbor training samples\n        k_neighbor_labels = [self.y_train[i] for i in k_idx]  \n        # return the most common class label\n        most_common = Counter(k_neighbor_labels).most_common(1)\n        return most_common[0][0]\n```\n:::\n\n\nWe are going to use iris dataset to test our KNN model that we created !!!\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\n\nX, y = iris.data, iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n\n\nclf = KNN()\nclf.fit(X_train, y_train)\n\n\ndef accuracy(y_true, y_pred):\n    accuracy = np.sum(y_true == y_pred) / len(y_true)\n    return accuracy\n\n\npredictions = clf.predict(X_test)\nprint(\"custom KNN classification accuracy\", accuracy(y_test, predictions))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncustom KNN classification accuracy 1.0\n```\n:::\n:::\n\n\n# k-NN Classifier\n\n## Defining the dataset\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# First Feature\nweather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\n'Rainy','Sunny','Overcast','Overcast','Rainy']\n# Second Feature\ntemp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\n\n# Label or target varible\nplay=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']\n```\n:::\n\n\nWe have two features weather and temperature and one label play.\n\n## Encoding data columns\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn import preprocessing\n\n#creating labelEncoder\nle = preprocessing.LabelEncoder()\n\n# Converting string labels into numbers.\nweather_encoded=le.fit_transform(weather)\n```\n:::\n\n\nSimilarly, you can encode temperature and label into numeric columns.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# converting string labels into numbers\ntemp_encoded=le.fit_transform(temp)\nlabel=le.fit_transform(play)\n```\n:::\n\n\n## Combining Features\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n#combinig weather and temp into single listof tuples\nfeatures=list(zip(weather_encoded,temp_encoded))\n```\n:::\n\n\n## Generating Models\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=3)\n\n# Train the model using the training sets\nmodel.fit(features,label)\n\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nKNeighborsClassifier(n_neighbors=3)\n```\n:::\n:::\n\n\n## Predict Output\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\npredicted= model.predict([[0,2]]) # 0:Overcast, 2:Mild\nprint(predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]\n```\n:::\n:::\n\n\n# k-NN Regressor\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_boston\nfrom sklearn.neighbors import KNeighborsRegressor\nX,y = load_boston(return_X_y=True)\nmod = KNeighborsRegressor()\nmod.fit(X,y)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/vidyasagar.bhargava/miniconda3/envs/databricks/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning:\n\nFunction load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n\n    The Boston housing prices dataset has an ethical problem. You can refer to\n    the documentation of this function for further details.\n\n    The scikit-learn maintainers therefore strongly discourage the use of this\n    dataset unless the purpose of the code is to study and educate about\n    ethical issues in data science and machine learning.\n\n    In this special case, you can fetch the dataset from the original\n    source::\n\n        import pandas as pd\n        import numpy as np\n\n\n        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n        target = raw_df.values[1::2, 2]\n\n    Alternative datasets include the California housing dataset (i.e.\n    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n    dataset. You can load the datasets as follows::\n\n        from sklearn.datasets import fetch_california_housing\n        housing = fetch_california_housing()\n\n    for the California housing dataset and::\n\n        from sklearn.datasets import fetch_openml\n        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\n    for the Ames housing dataset.\n    \n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nKNeighborsRegressor()\n```\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nmod.predict(X)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\narray([21.78, 22.9 , 25.36, 26.06, 27.1 , 27.1 , 20.88, 19.1 , 18.4 ,\n       19.48, 19.28, 22.  , 24.34, 20.52, 24.66, 21.3 , 30.48, 20.4 ,\n       15.7 , 23.54, 16.82, 17.64, 18.3 , 17.08, 16.66, 15.1 , 16.78,\n       14.94, 19.94, 18.34, 14.1 , 16.82, 15.12, 14.1 , 15.12, 26.92,\n       22.14, 27.4 , 28.44, 31.88, 31.88, 25.36, 25.36, 24.22, 20.68,\n       20.44, 20.44, 18.1 , 18.1 , 24.  , 21.54, 24.  , 27.16, 27.16,\n       25.7 , 39.82, 27.08, 38.28, 24.8 , 25.64, 21.78, 33.6 , 21.78,\n       24.06, 31.74, 25.3 , 26.98, 22.18, 20.42, 20.42, 27.76, 29.5 ,\n       27.76, 27.76, 22.92, 21.64, 25.82, 21.64, 21.38, 22.02, 24.8 ,\n       21.88, 25.22, 25.64, 25.98, 25.98, 23.28, 25.98, 24.02, 25.58,\n       25.58, 25.06, 26.34, 26.04, 30.1 , 24.84, 23.62, 24.32, 28.52,\n       24.96, 22.1 , 22.2 , 15.34, 19.74, 19.74, 19.66, 19.56, 21.34,\n       19.66, 19.56, 22.08, 20.1 , 19.6 , 17.54, 20.1 , 17.7 , 20.2 ,\n       20.1 , 20.66, 19.8 , 22.76, 20.6 , 19.66, 18.52, 19.66, 20.6 ,\n       18.52, 16.62, 18.04, 16.88, 18.4 , 18.4 , 18.78, 18.56, 20.24,\n       17.44, 17.8 , 18.4 , 15.88, 17.06, 15.24, 14.76, 15.62, 15.62,\n       15.62, 18.26, 18.26, 15.62, 17.82, 17.44, 37.22, 20.66, 19.28,\n       20.24, 20.24, 15.34, 15.34, 37.78, 25.52, 32.08, 20.66, 42.56,\n       44.54, 44.54, 30.34, 20.24, 37.22, 19.52, 19.98, 20.24, 19.98,\n       18.74, 21.9 , 24.4 , 23.74, 25.82, 22.34, 24.2 , 23.84, 38.56,\n       33.24, 38.56, 33.24, 31.6 , 33.24, 38.56, 37.84, 32.72, 33.14,\n       32.72, 33.14, 32.72, 33.72, 31.8 , 31.8 , 34.9 , 26.78, 25.24,\n       26.78, 29.38, 29.5 , 25.2 , 25.3 , 41.28, 41.28, 23.76, 23.78,\n       20.74, 22.9 , 21.1 , 21.1 , 21.1 , 23.9 , 28.84, 22.6 , 27.28,\n       22.96, 21.9 , 21.1 , 23.38, 25.42, 17.32, 31.8 , 24.46, 37.82,\n       36.46, 36.14, 35.96, 29.5 , 29.44, 34.  , 41.28, 38.28, 38.16,\n       28.12, 29.3 , 34.12, 34.12, 21.44, 21.92, 21.44, 21.4 , 22.1 ,\n       21.74, 20.  , 19.68, 22.16, 20.  , 21.38, 31.22, 31.22, 26.28,\n       29.56, 31.22, 27.08, 24.86, 38.28, 42.44, 38.9 , 36.48, 38.82,\n       41.88, 41.88, 37.9 , 41.88, 34.6 , 38.82, 36.16, 32.42, 31.74,\n       32.58, 28.82, 31.74, 26.88, 31.96, 31.96, 31.96, 31.96, 31.96,\n       30.58, 31.74, 32.58, 36.62, 42.8 , 24.84, 21.88, 38.64, 21.88,\n       24.44, 22.62, 34.9 , 34.9 , 31.88, 24.54, 23.28, 24.44, 23.22,\n       22.94, 25.28, 29.12, 25.42, 25.78, 28.02, 30.58, 31.22, 27.02,\n       31.96, 27.02, 23.72, 21.6 , 29.  , 21.32, 21.02, 20.94, 21.44,\n       21.6 , 18.54, 19.52, 20.5 , 21.3 , 23.32, 23.76, 23.32, 22.9 ,\n       22.06, 23.76, 26.14, 22.06, 19.7 , 21.22, 19.92, 21.86, 22.98,\n       23.6 , 21.16, 20.78, 25.74, 24.3 , 20.72, 22.64, 24.32, 24.44,\n       19.8 , 29.36, 26.58, 19.  , 18.84, 26.48, 33.32, 25.78, 25.78,\n       28.  , 30.46, 42.8 , 20.96, 24.8 , 15.88, 19.06, 20.94, 21.42,\n       33.8 , 25.6 , 30.94, 25.6 , 27.22, 27.22, 16.98, 14.58, 39.16,\n       39.16, 26.46, 36.6 , 27.22, 11.36, 10.54, 10.82, 12.36, 12.5 ,\n        9.6 , 10.22,  6.86, 10.26, 11.62, 11.62, 14.16, 11.  ,  8.94,\n        9.74, 13.18, 12.5 , 13.52, 21.66, 11.88, 15.58, 11.74, 13.54,\n       13.98, 12.46,  8.5 , 14.82,  9.54, 11.14, 15.88, 10.5 , 14.28,\n        6.86, 13.18, 18.56, 14.52, 17.62, 10.34, 12.74, 11.88, 17.18,\n       10.92, 11.88, 11.2 , 14.58, 11.96, 10.44, 16.98, 16.98, 14.16,\n       13.58, 12.94, 11.74, 12.56, 10.26, 13.52, 11.58, 14.  , 12.6 ,\n       13.52, 13.3 , 12.9 , 12.16, 12.1 , 10.56, 12.12, 11.96, 10.6 ,\n       14.56, 13.84, 13.34, 13.44, 12.44, 16.98, 13.34, 14.48, 16.06,\n       13.58, 15.54, 17.04, 16.62, 12.54, 12.2 , 13.58, 12.94, 14.84,\n       20.24, 14.34, 19.82, 20.24, 20.38, 22.28, 17.56, 12.74, 18.56,\n       23.7 , 21.26, 20.24, 18.6 , 22.72, 23.28, 15.54, 16.06, 14.18,\n       14.96, 15.88, 16.36, 22.28, 22.72, 23.44, 20.86, 22.8 , 21.34,\n       21.42, 21.34, 17.04, 11.54, 12.28, 14.86, 18.3 , 22.08, 21.82,\n       22.02, 18.7 , 18.7 , 20.64, 18.7 , 19.96, 21.18, 23.12, 20.88,\n       21.9 , 21.42])\n```\n:::\n:::\n\n\n\n",
    "supporting": [
      "Untitled_files"
    ],
    "filters": [],
    "includes": {}
  }
}