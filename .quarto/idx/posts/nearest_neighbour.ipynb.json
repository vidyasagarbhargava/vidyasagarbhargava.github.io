{"title":"Nearest Neighbour Classifier","markdown":{"yaml":{"title":"Nearest Neighbour Classifier","description":"Nearest neighbor classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples.","author":"Vidyasagar Bhargava","date":"06/08/2022","categories":["machine learning","supervised learning","algorithm","classifier"],"format":{"html":{"code-fold":false}},"jupyter":"python3","execute":{"enabled":true},"title-block-banner":true},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n![](https://i.imgur.com/bpU9JuI.gif)\n\nNearest neighbor classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples.\n\nNearest Neighbors works well for classification task where the relationships among the features and target classes are numerous and extremely difficult to understand but the items of similar class tend to be homogeneous.\n\nNearest Neighbor classifier struggles most when there is no clear distinction exists among the groups.\n\n\n\n# k-NN algorithm\nk-Nearest Neighbor algorithm is an example of nearest neighbor classifier.\n\n## Strength\n* Simple and effective\n* Makes no assumption about data\n* Fast Training Process\n\n\n## Weakness\n* Doesnâ€™t produce model, limiting the ability to understand how features are related to class\n* Requires selection of k\n* Slow classification phase\n* Categorical features and missing data require pre processing\n\nThe k-Nearest Neighbor algorithm uses nearest k number of neighbors for labeling of an unlabeled example. The unlabeled test example is assigned the class of majority of the k-Nearest Neighbors.\n\nFor finding the distance k-NN algorithm uses **Euclidean distance**.\n\n# Choosing an appropriate k\nChoosing the value of k determines how well the model will generalize to future data. Choosing a large k reduces the impact or variance caused by noisy data but can bias the learner so that it runs the risk of ignoring small, but important pattern.\n\nIn Practice choosing k depends on the difficulty of the concept to be learned and the number of records in training data.\n\n* Start with k value equal to the square root of the number of training examples.\n* Using cross validation to determine the best k value.\n* Weighted voting is one of interesting way to solve this problem. By giving higher weight to close neighbors.b 444\n\n# k-NN from scratch\n* Compute distances between x and all examples in the training set\n* Sort by distance and return indexes of the first k neighbors\n* Extract the labels of the k nearest neighbor training samples\n* Return the most common class label\n\nWe used euclidean distance for calculating the nearest neighbors.\n\nNow we will define our KNN Class\n\nWe are going to use iris dataset to test our KNN model that we created !!!\n\n# k-NN Classifier\n\n## Defining the dataset\n\nWe have two features weather and temperature and one label play.\n\n## Encoding data columns\n\nSimilarly, you can encode temperature and label into numeric columns.\n\n## Combining Features\n\n## Generating Models\n\n## Predict Output\n\n# k-NN Regressor\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":true,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"nearest_neighbour.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"0.9.518","mainfont":"system-ui","theme":["journal","../theme.scss"],"backgroundcolor":"#e6ddde","title":"Nearest Neighbour Classifier","description":"Nearest neighbor classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples.","author":"Vidyasagar Bhargava","date":"06/08/2022","categories":["machine learning","supervised learning","algorithm","classifier"],"jupyter":"python3","title-block-banner":true},"extensions":{"book":{"multiFile":true}}}}}