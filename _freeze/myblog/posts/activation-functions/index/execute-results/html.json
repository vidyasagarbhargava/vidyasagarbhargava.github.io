{
  "hash": "39f256fab826d11768c0cd6997d5c741",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Choosing Activation Functions\"\n\ndescription: An activation function decides whether a neuron should be activated or not which helps neural network to use important information while suppressing the irrelevant data points.\n\nauthor: \"Vidyasagar Bhargava\"\ndate: \"18/08/2022\"\ncategories:\n  - activation function\n  - deeplearning\n---\n\n![Dubey Singh Chaudhary(2022). Activation Functions in Deep Learning : A Comprehensive Survey and Benchmark](images/paste-1.png)\n\n## Introduction\n\nA neural network has three types of layers : **input layer** that take raw input , **hidden layer** that take input from another layer and pass output to another layer, and finally **output layer** that make a prediction.\n\nInput layer has no computation performed so there is no activation function required. All hidden layers typically use the same activation function. The output layer will typically use different activation function from the hidden layer depending on the type of prediction required by the model.\n\nAn activation function should also be differentiable which means their first order derivative can be calculated for a given input value. This is required since neural network are trained using backpropagation algorithm which requires derivative of loss function in order to updates the weight of model.\n\n## Need of Activation Functions in Neural Networks {#need-of-activation-function-in-neural-networks}\n\nThe objective of activation function in neural network is to add non-linearity so that it can learn complex patterns. Activation function introduces an additional step at each layer during the forward propagation, but its computation is worth. Here it is why-\n\nLet's suppose we have a neural network without activation functions. In that case every neuron will be performing linear transformation on the inputs using the weights and biases. It’s because it doesn’t matter how many hidden layers we attach in the neural network; all layers will behave in the same way because the composition of two linear functions is a linear function itself.\n\nAlthough the neural network becomes simpler, hence learning any complex task is impossible, and our model would be just a linear regression model.\n\n## Types of Activation Functions\n\nMost of activation functions are non-linear however we also use linear activation functions in neural networks. For example, we use linear activation function in the output layer of neural network model that solves a regression problem.\n\n::: callout-note\n## Linear vs Non Linear Functions\n\nA linear function (called **f**) takes the input, **z** and returns the output, **cz** which is the multiplication of the input by the constant, **c**. Mathematically, this can be expressed as **f(z) = cz**. When c=1, the function returns the input as it is and no change is made to the input. The graph of a linear function is a *single straight line*.\n\nAny function that is not linear can be classified as a non-linear function. The graph of a non-linear function is not a single straight line. It can be a complex pattern or a combination of two or more linear components.\n:::\n\nBelow are the most common activation function used in hidden layers :\n\n-   Binary Step Function\n-   Linear Activation Function\n-   Non - Linear Activation Functions\n    -   Sigmoid\n    -   Tanh\n    -   ReLU\n        -   Leaky ReLU\n        -   Parametric ReLU\n    -   ELU\n    -   GELU\n    -   Swish\n    -   SELU\n    -   Softmax\n\n### Binary Step Function\n\nIn binary step function a threshold value decides that a neuron should be activated or not. Here the input fed to activation function is compared with threshold and if its greater than threshold neuron is activated otherwise it is deactivated which means output is not passed to the next hidden layer.\n\nMathematically a binary step function can be represented as\n\n$$\nf(x) = \\begin{cases} \n      0 & for \\ x\\lt 0 \\\\\n      1 & for \\ x\\geq 0 \n   \\end{cases}\n$$\n\ncode for binary step function\n\n::: {#fd718b49 .cell execution_count=1}\n``` {.python .cell-code}\ndef binary_step_function(x):\n    return np.where(x < 0, 0, 1)\n```\n:::\n\n\nplot for binary step function\n\n::: {#61bb1906 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np \nimport matplotlib.pyplot as plt\n\n# Generate a range of values for x\nx_values = np.linspace(-5, 5, 1000)\n\n# Apply the binary step function to each value of x\ny_values = binary_step_function(x_values)\n\n# Plot the binary step function\nplt.plot(x_values, y_values, label='Binary Step Function')\nplt.title('Binary Step Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=589 height=449}\n:::\n:::\n\n\n::: callout-warning\nThe gradient of step function is zero which causes hindrance in the backpropagation process as well as it can't be use for multi-class classification problems,\n:::\n\n### Linear Activation Function\n\nThis is also known as *identity* or *no activation* function where the activation is proportional to input.\n\nMathematically it can be represented as:\n\n$$\nf(x)= x\n$$\n\ncode for linear activation function\n\n::: {#f55790e7 .cell execution_count=3}\n``` {.python .cell-code}\ndef linear_activation(x):\n    return x\n```\n:::\n\n\nplot for linear activation function\n\n::: {#754a75ea .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np \nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 10)\ny_values = linear_activation(x_values)\n\n# Plot the linear activation function\nplt.plot(x_values, y_values, label='Linear Activation Function')\nplt.title('Linear Activation Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=587 height=449}\n:::\n:::\n\n\n::: callout-warning\nBackpropagation can't be used with linear activation function as the derivative of function is constant and has no relation to the input x. All layers of the neural network will collapse into one if a linear activation function is used.\n:::\n\n### Non-Linear Activation Functions\n\n#### Sigmoid\n\nSigmoid activation function is also called the *logistic function*. It is a non-linear function which converts its input into a probability value between 0 and 1. Large negative values are converted towards 0 while large positive values are converted towards 1.\n\nMathematically it can be represented as:\n\n$$\nf(x) = \\frac{1}{1+e^{-x}}\n$$\n\ncode for sigmoid activation function\n\n::: {#b43b6205 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\n\ndef sigmoid(x):\n    return 1/(1+ np.exp(-x))\n```\n:::\n\n\nplot for sigmoid activation function\n\n::: {#c93e7535 .cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-10, 10, 100) \nz = sigmoid(x)\n\nplt.plot(x, z) \nplt.title('Sigmoid Function')\nplt.xlabel('Input (x)') \nplt.ylabel(\"Output\") \nplt.grid(color = 'gray', linestyle = '--', linewidth = 0.5)  \nplt.show() \n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=589 height=449}\n:::\n:::\n\n\nSigmoid is right choice where we have to predict the probability as an output. The function is differentiable and provide smooth gradient which mean no jumps in output values\n\n::: callout-warning\nSigmoid function suffers from vanishing gradient problem which makes learning difficult.\n:::\n\n#### Tanh\n\nTanh or hyperbolic tangent is very similar to sigmoid activation function and even has same S-shape with difference in output range of -1 to 1. In Tanh larger the input (more positive) , the closer the output will be to 1.0 , whereas the smaller the input (more negative) , the closer the output will be to -1.0.\n\nMathematically it can be represented as:\n\n$$\nf(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\n$$\n\ncan also be written as :\n\n$$\nf(x) = \\frac{e^{2x}-1}{e^{2x}+1}\n$$\n\ncode for Tanh activation function\n\n::: {#75ccfa53 .cell execution_count=7}\n``` {.python .cell-code}\nimport numpy\n\ndef tanh(x):\n    return (np.exp(2*x) - 1) / (np.exp(2*x) + 1)\n```\n:::\n\n\nplot for Tanh activation function\n\n::: {#7725d389 .cell execution_count=8}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 1000)\ny_values = tanh(x_values)\n# Plot the tanh activation function\nplt.plot(x_values, y_values, label='tanh Activation Function')\nplt.title('tanh Activation Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=608 height=449}\n:::\n:::\n\n\n::: callout-warning\nIt also faces the vanishing gradient issue similar to sigmoid activation function.\n:::\n\n#### ReLU\n\nReLU stands for rectified linear unit. ReLU gives an impression of linear activation function but it has derivative function and allows for backpropagation while simultaneously making it computationally efficient. ReLU function doesn't activate all the neuron at same time. The neuron will be deactivated only when the output of the linear transformation is less than 0.\n\nReLU is a simple and robust choice.\n\nMathematically it can be represented as:\n\n$$\nf(x) = max(0,x)\n$$\n\ncode for ReLU activation function\n\n::: {#ee5cf0b6 .cell execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\n\ndef relu(x):\n    return np.maximum(0, x)\n```\n:::\n\n\nplot for ReLU activation function\n\n::: {#fc86c4c7 .cell execution_count=10}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 1000)\ny_values = relu(x_values)\n\n# Plot the ReLU activation function\nplt.plot(x_values, y_values, label='ReLU Activation Function')\nplt.title('Rectified Linear Unit (ReLU) Activation Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=576 height=449}\n:::\n:::\n\n\nReLU accelerates the convergence of gradient descent towards the global minimum of the loss function due to its linear, non-saturating property.\n\n::: callout-note\nThe ReLU is not differentiable at a singular point x = 0, but we can still use what are known as sub-derivatives in backpropagation algorithm. The usual derivative of a ReLU is actually a sub-derivative to be precise. We use what is called sub-gradient descent approach to optimize such functions.\n:::\n\n**Dying ReLU Problem**\n\nThe derivative of ReLU activation is given as :\n\n$$\nf'(x) = \\begin{cases} \n      1 & for \\ x\\geq 0 \\\\\n      0 & for \\ x\\lt 0 \n   \\end{cases}\n$$\n\nHere the negative values makes the gradient value zero. Due to this reason, during the backpropagation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated. \n\n#### Leaky ReLU\n\nLeaky ReLU is improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area.\n\nMathematically it can be represented as:\n\n$$\nf(x) = max(0.01x, x)\n$$\n\ncode for Leaky ReLU activation function\n\n::: {#d06d2fb4 .cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np\n\ndef leaky_relu(x):\n    return np.maximum(0.01*x, x)\n```\n:::\n\n\nplot for Leaky ReLU activation function\n\n::: {#2120218d .cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 1000)\ny_values = leaky_relu(x_values)\n\n# Plot the ReLU activation function\nplt.plot(x_values, y_values, label='Leaky ReLU Activation Function')\nplt.title('Leaky Rectified Linear Unit (Leaky ReLU) Activation Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=576 height=449}\n:::\n:::\n\n\nThe advantages of Leaky ReLU are same as of ReLU, in addition to the fact that it does enable backpropagation, even for negative input values. However it suffers from inconsistent predictions for negative input values.\n\n#### Parametric ReLU\n\nParametric ReLU or PReLU is another variant of ReLU that aims to solve the problem of dying ReLU and Leaky ReLU (inconsistent predictions for negative input values). So the authors of the paper behind PReLU thought why not let the **a in ax for x\\<0 (in LeakyReLU)** get learned.\n\nAnd here is the catch: if all the channels share the same **a** that gets learned, it is called channel-shared PReLU. But if each channel learn their own **a**, it is called channel-wise PReLU.\n\nSo what if ReLU or LeakyReLU was better for that problem? That is upto the model to learn:\n\n-   if a is/are learned as 0 -\\> PReLU becomes ReLu\n\n-   if a is/are learned as small number -\\> PReLU becomes LeakyReLU\n\nMathematically it can be represented as:\n\n$$\nf(x) = max(ax, x)\n$$Where *\"a\"* is the slope parameter for negative values.\n\ncode for Parametric ReLU activation function\n\n::: {#d7e96296 .cell execution_count=13}\n``` {.python .cell-code}\nimport numpy as np\n\nclass PReLU:\n    def __init__(self, alpha=0.01):\n        self.alpha = alpha\n\n    def __call__(self, x):\n        return np.maximum(self.alpha * x, x)\n\n```\n:::\n\n\nplot for Parametric ReLU activation function\n\n::: {#55e75ee7 .cell execution_count=14}\n``` {.python .cell-code}\nx_values = np.linspace(-5, 5, 100)\n\n# Create a PReLU instance with an initial slope (alpha)\nprelu = PReLU(alpha=0.1)\n\n# Calculate corresponding output values using PReLU\nprelu_values = prelu(x_values)\n\n# Plot the PReLU function\nplt.plot(x_values, prelu_values, label='PReLU Function (alpha=0.1)')\nplt.title('Parametric ReLU (PReLU) Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=576 height=449}\n:::\n:::\n\n\n> In leaky ReLU alpha is hyper parameter where as in Parametric ReLU it is a parameter.\n\n#### ELU\n\nExponential Linear Unit, or ELU for short, is also a variant of ReLU that modifies the slope of the negative part of the function. ELU uses a log curve to define the negative values unlike the leaky ReLU and Parametric ReLU functions with a straight line. \n\nMathematically it can be represented as:\n\n$$\nf(x) = \\begin{cases} \n      x & for \\ x\\geq 0 \\\\\n      \\alpha(e^x-1) & for \\ x\\lt 0 \n   \\end{cases}\n$$\n\ncode for ELU activation function\n\n::: {#cea0a276 .cell execution_count=15}\n``` {.python .cell-code}\nimport numpy as np\n\ndef elu(x, alpha=1.0):\n    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n```\n:::\n\n\nplot for ELU activation function\n\n::: {#28d45888 .cell execution_count=16}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 100)\nelu_values = elu(x_values)\n\n# Plot the ELU function\nplt.plot(x_values, elu_values, label='ELU Function (alpha=1.0)')\nplt.title('Exponential Linear Unit (ELU) Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=587 height=449}\n:::\n:::\n\n\nELU becomes smooth slowly until its output equal to -α whereas ReLU sharply smoothes. It also avoids dead ReLU problem by introducing log curve for negative values of input.\n\nHowever the computational time increases because of the exponential operation.\n\n#### GELU\n\nGaussian Error Linear Unit or GELU activation function is compatible with BERT, ROBERTa, ALBERT, and other top NLP models. This activation function is motivated by combining properties from dropout, zoneout, and ReLUs. \n\nMathematically it can be represented as:\n\n$$\nf(x) = x\\Phi(x)\n$$\n\n$$\nf(x) = 0.5x \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715x^3\\right)\\right)\\right)\n$$\n\nwhere $\\Phi(x)$ is the cumulative distribution function of Gaussian distribution.\n\ncode for GELU activation function\n\n::: {#fa8085f0 .cell execution_count=17}\n``` {.python .cell-code}\nimport numpy as np\n\ndef gelu(x):\n    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n```\n:::\n\n\nplot for GELU activation function\n\n::: {#b6f9785c .cell execution_count=18}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 100)\ngelu_values = gelu(x_values)\n\n# Plot the GELU function\nplt.plot(x_values, gelu_values, label='GELU Function')\nplt.title('Gaussian Error Linear Unit (GELU) Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=576 height=449}\n:::\n:::\n\n\nGELU non linearity is better than ReLU and ELU activations and finds performance improvements across all tasks in domains of computer vision, natural language processing, and speech recognition.\n\n::: callout-note\nGELU activation function was implemented in GPT-2 models\n:::\n\n#### Swish\n\nIn 2018 the paper [[Searching for activation functions]{.underline}](https://arxiv.org/pdf/1710.05941.pdf?ref=blog.paperspace.com) by researchers at Google Brain team proposes a novel activation function called Swish, which was discovered using a Neural Architecture Search (NAS) approach and showed significant improvement in performance compared to standard activation functions like ReLU or Leaky ReLU.\n\nSwish consistently matches or outperforms ReLU activation function on deep networks applied to various challenging domains such as image classification machine translation etc. \n\nMathematically it can be represented as:\n\n$$\nf(x) = \\frac{x}{1 + e^{-\\beta x}}\n$$\n\nwhere $\\beta$ is either a constant or trainable parameter depending on the model.\n\nit can also written in terms of sigmoid activation function\n\n$$\nf(x) = x*sigmoid(\\beta x)\n$$\n\nat $\\beta=1$ the function becomes equivalent to sigmoid linear unit or SiLU.\n\n$$\nf(x) = \\frac{x}{1+ e^{-x}}\n$$\n\ncode for Swish activation function\n\n::: {#2b2d1c1d .cell execution_count=19}\n``` {.python .cell-code}\nimport numpy as np\n\ndef swish(x,beta = 1.0):\n    return x*(1/(1+np.exp(-beta*x)))\n```\n:::\n\n\nplot for Swish activation function\n\n::: {#b03f499e .cell execution_count=20}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 100)\nswish_values = swish(x_values)\n\n# Plot the Swish function\nplt.plot(x_values, swish_values, label='Swish Function (beta=1.0)')\nplt.title('Swish Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-21-output-1.png){width=576 height=449}\n:::\n:::\n\n\n#### SELU\n\nScaled Exponential Linear Unit or SELU was defined in self-normalizing networks and takes care of internal normalization which means each layer preserves the mean and variance from the previous layers. SELU enables this normalization by adjusting the mean and variance. \n\nMathematically it can be represented as:\n\n$$\n\\begin{equation} f(\\alpha, x) = \\lambda \\begin{cases} \\alpha(e^x - 1), & \\text{if}\\ x \\lt 0 \\\\ \nx, & \\text{otherwise} \\\\ \\end{cases} \\end{equation}\n$$\n\nSELU has values of α and λ predefined.\n\ncode for SELU activation function\n\n::: {#dfc4dd6b .cell execution_count=21}\n``` {.python .cell-code}\nimport numpy as np\n\ndef selu(x, alpha=1.67326, lambda_=1.0507):\n    return lambda_ * np.where(x > 0, x, alpha * (np.exp(x) - 1))\n```\n:::\n\n\nplot for SELU activation function\n\n::: {#c19c5be0 .cell execution_count=22}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx_values = np.linspace(-5, 5, 100)\nselu_values = selu(x_values)\n\n# Plot the SELU function\nplt.plot(x_values, selu_values, label='SELU Function (alpha=1.67326, lambda=1.0507)')\nplt.title('Scaled Exponential Linear Unit (SELU) Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.grid(color='gray', linestyle='--', linewidth=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-1.png){width=587 height=449}\n:::\n:::\n\n\nThe main advantage of SELU over ReLU is internal normalization is faster than external normalization, which means the network converges faster.\n\n> SELU is a relatively newer activation function and needs more papers on architectures such as CNNs and RNNs, where it is comparatively explored.\n\n#### Softmax\n\nSoftmax is generalization of sigmoid activation function which can be used for multi-class classification. The softmax function squashes the outputs of each unit to be between 0 and 1, just like a sigmoid function. But it also divides each output such that the total sum of the outputs is equal to 1.\n\nAssume that you have three classes, meaning that there would be three neurons in the output layer. Now, suppose that your output from the neurons is \\[1.8, 0.9, 0.68\\]. Applying the softmax function over these values to give a probabilistic view will result in the following outcome: \\[0.58, 0.23, 0.19\\]. \n\n$$\nf(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K\n$$\n\ncode for Softmax activation function\n\n::: {#b765d98e .cell execution_count=23}\n``` {.python .cell-code}\nimport numpy as np\n\ndef softmax(z):\n    exp_z = np.exp(z)\n    return exp_z / np.sum(exp_z)\n```\n:::\n\n\n## Choosing the right activation function\n\nBelow are some rule of thumb for choosing the right activation function :\n\n-   Activation function in output layer depends on type of prediction problem.\n\n    -   **Binary Classification** - Sigmoid\n    -   **Multi-class Classification** - Softmax\n    -   **Regression** - Linear\n    -   **Multilabel Classification**—Sigmoid\n\n-   Activation function in hidden layer\n\n    -   Start with using ReLU function and then move over to other activation functions if ReLU doesn’t provide optimum results.\n    -   Don't use sigmoid and Tanh activation functions in hidden layer as they can cause vanishing gradient problem.\n    -   Swish function is used in neural networks having a depth greater than 40 layers.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}