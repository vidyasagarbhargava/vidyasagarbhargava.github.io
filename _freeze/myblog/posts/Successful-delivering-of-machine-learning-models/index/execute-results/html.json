{
  "hash": "ab33d0f51c0c252b23b46f28108e4144",
  "result": {
    "markdown": "---\ntitle: \"Successful delivering of Machine Learning Projects\"\ndescription: \"Principles and process for democratizing ML projects\"\nauthor: \"Vidyasagar Bhargava\"\ndate: \"07/13/2019\"\ncategories:\n  - machine learning\njupyter: python3\nexecute: \n  enabled: true\ntitle-block-banner: true\nimage: https://i.imgur.com/1skYErY.png\n---\n\n### 1. Easy access to required data and a comprehensive data strategy\n\nThere is saying in computer science world \"garbage in, garbage out\" which means nonsense input data produces nonsense output. Therefore your machine learning model is only as good as the data it's trained on.If there is problem with data, machine learning scientists will end up spending their time in doing data cleanup and management.So we need a strong data strategy to make efficient use of ML scientist's time and talent. \n\n**What makes a strong data strategy ?**\n\n* Data should be viewed as organizational asset rather than property of individual department that created or collected that data.\n* Data should be available easily, securely and in compliance with legal and regulatory requirements.\n* Data is put to work through analytics and machine learning to make better decisions, create efficiencies and drive new innovations.\n\n\n**Data related questions to be asked before the start of ML project**\n\n* What data is available to me today?\n* What data is not quite available, but with some effort could become available?\n* What data I don't have today, but I might have in next few months or year? And what steps can be taken to begin gathering that data?\n* Is there any potential bias in data or data sources? \n\n\n\n### 2. Selecting machine learning use cases and setting success metrics\n\nWe should aim to use machine learning where it is actually needed and not where it might be interesting. *Some times simple analytics or rules get you 10-40% of business impact*.Things to keep in mind include data readiness, business impact and machine learning applicability. \n\n* A high impact use case without data or machine learing applicability ❌\n* A use case with lots of data and high machine learning applicability but low business impact ❌\n\nBefore working on a project the team needs estimate its potential impact as well (Opportunity Sizing). So once we define business problem which can be solved with machine learning and done with opportunity sizing the next step is to outlining clear metrics to measure success.\n\n\nThe data science projects needs to have clear goal which is typically a target value for a clearly defined metric. In real world data science projects there are not just one but multiple metrics that model will evaluated against. Some of these evaluation metric won't even be related to how your prediction performs against the ground truth. Other such metrics are like :\n\n* Overall memory usage\n* latency of the prediction process\n* complexity of predictive model\n\n*Real world problems are indeed dominated by business and tech infrastructure concerns.*\n\n\n### 3. Technical experts and domain experts should work together\nWe need to make sure that domain experts and technical experts or stakeholders work side by side. If relevant stakeholders are the part of entire process, everyone is most likely to accept, adopt and implement the solution. If a data scientist is working in silos then its very much unlikely that their models get implemented.\n\n### 4. Exploratory data analysis\n\nBefore building the model,we need to interrogate the data to see if there is any predictive power in the feature set. Read more about EDA [here](https://vidyasagarbhargava.github.io/posts/eda.html).\n\n### 5. A quick MVP\n\nIts good practice to build a minimum viable product which is build quickly and cheaply to validate the hypothesis before we commit extensive time \nand resource.\n\n### 6. Experiment Metrics \n\nWe should look for more than one metric to look when an experiment concludes.\n\n### 7. Regular Check-ins \n\nRather than meeting at the start and end of a model build, it is better to check-in frequently (e.g. once or twice a week) to discuss latest findings and align on if any course corrections are necessary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}